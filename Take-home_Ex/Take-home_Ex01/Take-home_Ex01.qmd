---
title: "Take-home_Ex01"
author: "NeoYX"
date: '7 May 2023'
#date-modified: "`r Sys.Date()`"
editor: visual
execute: 
  freeze: auto
  warning: false
  #echo: false
  #message: false
  html:
    code-fold: True
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---

# 1. Task and Dataset

This exerise aims to reveal the demographic and financial characteristics of the city of Engagement, using appropriate **static and interactive statistical graphics** methods. It also requires a user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.

The dataset consists of a sample **survey** of 1000 representative residents that collects data related to their household demographic and spending patterns, among other things. There are primarily two datasets used in this exercise

-   'FinancialJournal.csv": Contains 1513635 number of daily transaction records (different categories of income and expenses) over a period of twelve months from March 2022 to February 2023.

-   'Particpants.csv" : Contains demographics information like household size, age, education level, interest groups, joviality index and whether each household has kids.

In this exercise, each dataset will be cleansed separately and then joined by 'participantID' as primary key to form the final dataset used for further analysis.

# 

# 2. Data Preparation

## 2.1 Install and load the required libraries

The code chunk below uses `pacman::p_load()` to check if packages are installed. If they are, they will be launched into R. The packages installed are

-   `plotly`: Used for creating interactive web-based graphs.

-   `knitr`: Used for dynamic report generation

-   `patchwork`: Used to combine plots

-   `tidyverse`: A collection of core packages designed for data science, used extensively for data preparation and wrangling.

-   `ggthemes`: Provide additional themes for `ggplot2`

-   `ggstatsplot`: Used for creating graphics with details from statistical tests.

-   `ggdist`: Used for visualising distribution and uncertainty

-   `rstatix`: Allows us to perform basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.

-   `gt` : starting from a tibble table, customise a table and export in various formats. Most importantly, it works with patch. We will save the tabular results from shapiro test as gt object and export using gtsave() into .png file later.

-   `ggridges`: a ggplot2 extension specially designed for plotting ridgeline plots.

```{r}
pacman::p_load(plotly, knitr, patchwork, tidyverse, ggthemes,hrbrthemes, ggiraph, ggstatsplot, ggdist, ggridges, colorspace, png, gifski, rstatix, gt)
```

## 2.2 Import the dataset

The datasets are imported using `tidyverse`'s `readr::read_csv()` function.

'FinancialJournal.csv" is stored as `finance` variable.

```{r}
finance <- read_csv('data/FinancialJournal.csv')
```

```{r}
#| echo: false
head(finance)
```

Check for empty values in the `finance` table using the `is.na()` function.

```{r}
any(is.na(finance))
```

'Particpants.csv" is stored as `ptcp` variable.

```{r}
ptcp <- read_csv('data/Participants.csv')
```

```{r}
#| echo: false
head(ptcp)
```

Checking for empty values in `ptcp` table using the `is.na()`\` function.

```{r}
any(is.na(ptcp))
```

## 2.3 Data Issues and wrangling

I will discuss the issues in the datasets and proposed cleaning methods.

### 2.3.1 **finance** dataset issues:

-   **participantId** should be converted from `<dbl>` format to `<chr>` format. It should be a categorical and not numerical data type.

-   **timestamp** should be converted from `<dttm>` format to `<date>` format as I will not be analysing time in this exercise.

-   Negative values of **amount** that belong to the expenses categories should be converted to positive values. The amount will also be rounded to two decimal places.

The code chunk below does the following:

-   use the **`as.character()`** function to convert **participantId** to `<chr>` format

-   create a new column **month_year** by extracting the year and month from the **timestamp** column using the **`format()`** function with the **`%Y-%m`** format specifier.

-   use the **`abs()`** function to convert negative values **amount** to positive and round the values to 2 decimal places using the **`round()`** function.

```{r}

# Convert participantId to character
finance <- finance %>% mutate(participantId = as.character(participantId))

# Extract month and year from timestamp
finance <- finance %>% 
  mutate(month_year = format(timestamp, "%m-%Y"))

# Transform negative amounts to positive and round to 2 decimal places
finance <- finance %>% 
  mutate(amount = abs(amount),
         amount = round(amount, 2))

```

A check for duplicates using the `duplicated()` function reveals that there are 1,113 records of duplicates.

-   The **`duplicated()`** function to identify the duplicate rows. It returns a logical vector indicating whether each row is a duplicate of a previous row in the data frame. We can then use this logical vector to subset the data frame and show the duplicate rows. The logical vector is stored in a filter **duplicated_rows** which is used to subset the `finance` data.

```{r}
# Show duplicate rows
duplicated_rows <- finance[duplicated(finance),]
glimpse(duplicated_rows)
```

-   **`unique()`** function is used to remove the duplicate rows form `finance` data

```{r}
# Remove duplicate rows
finance <- unique(finance)
```

-   Perform a final check to verify that there are no more duplicate using `any()` function

```{r}
any(duplicated(finance))
```

The last thing to do is to create a new column **date** that is in `<date>` format using the `as.Date` function.

-   the paste0() function is used to concatenate "01-" with each value in the month_year column. This is because as.Date() requires a complete date in the format "dd-mm-yyyy"

    ```{r}
    finance$date <- as.Date(paste0("01-", finance$month_year), format = "%d-%m-%Y")
    ```

**Other issues**

When the `finance` dataset is group-by the **date** variable , it is noticed that the number of distinct participantID who took part in the survey was 1,011 in March 2022 and suddenly reduced to a constant value of 880 from April 2022 onwards. It seems to suggest that there are 131 residents who moved out of the city at the end of March 2022.

In the code chunk below:

-   dataset is group-by **date** and the distinct count of **participantID** is generated using `n_distinct` function

-   the **missing** dataframe is displayed below using `knitr::kable()` function

```{r}
missing_summary <- finance %>%
  group_by(date) %>% 
  summarise(n_distinct=n_distinct(participantId)) %>% 
  rename(`Number of unique participantId` = n_distinct)

knitr::kable(missing_summary, "simple")
```

Since 11 out of 12 months of records are missing for these 131 residents, we will delete their records from the `finance` dataset.

The code chunk below does the following:

-   extract the **participantIds** of residents whose records exists in March 22 but not in all April 22 using the `anti-join` function

-   pass the unique **`participantId`** column name as an argument to **`pull()`** and use the **`as.vector()`** function to convert the resulting tibble column to a vector

-   resulting dataframe will only contain **`participantId`**s that are in '2022-03-01' but not in '2022-04-01' onwards. There are 131 of them.

```{r}

missing_id <- finance %>%
  filter(date == as.Date('2022-03-01')) %>% # filter for '2022-03-01' date
  anti_join(finance %>%
             filter(date == as.Date('2022-04-01')), # filter for '2022-04-01' date
             by = 'participantId') %>% # anti-join by 'participantId'
  select(participantId) %>% 
  distinct(participantId)

# extract participantId column as convert this column to vector.
missing_id_vector <- as.vector(pull(missing_id, participantId))

missing_id_vector 

```

Next, we will remove all records of the 131 potentially non-residents from the `finance` dataset .

In the code chunk below:

-   the **`%in%`** operator is to check if each **`id`** value is contained in the missing_id_vector

-   the negation operator **`!`** ensures the resulting filtered data frame will not contain the rows where the **`id`** values are in missing_id_vector

```{r}
finance1 <- finance[!finance$participantId %in% missing_id_vector, ]
finance1
```

We will double check that the records of 131 non-residents have been removed from `finance1` dataframe.

In the code below

-   **`distinct()`** function to extract the distinct **`participantId`** values from **`finance1`**

-   the **`n_distinct()`** function will count the number of distinct **`participantId`** values in the resulting tibble

```{r}
finance1 %>% 
  distinct(participantId) %>% 
  n_distinct()
```

### 2.3.2 ptcp dataset issues:

-   **participantId** should be converted from `<dbl>` format to `<chr>` format

-   **householdSize** should be converted from `<dbl>`{style="caret-color: white;"} format to `<fct>`{style="caret-color: white;"} format. It does not make sense to have 2.5 persons.

-   **age** should be converted from `<dbl>`{style="caret-color: white;"} format to `<int>`{style="caret-color: white;"} format.

-   **educationLevel** should be converted from `<chr>` to `<fct>` . It should also be ordered according to 'Low', 'HighSchoolOrCollege', 'Bachelors' and 'Graduate'.

The code chunk below does the following:

-   `as.character` and `as.factor` functions are used to convert **participantId** to `<chr>` , **householdSize** to `<fct>` and **age** to `<int>`.

-   `factor(educationLevel, levels=c("Low", "HighSchoolOrCollege", "Bachelors", "Graduate")))` not only converts **educationLevel** to factor, but also order the values inside.

```{r}
# convert to factor
ptcp <- ptcp %>% mutate(participantId = as.character(participantId))
ptcp <- ptcp %>% mutate(householdSize = as.factor(householdSize))

# Convert educationLevel to factor and order accordingly
ptcp <- ptcp %>% mutate(educationLevel = factor(educationLevel, levels=c("Low", "HighSchoolOrCollege", "Bachelors", "Graduate")))

# convert age to int
ptcp <- ptcp %>% mutate(age = as.integer(age))
```

The columns format are all in order now.

```{r}
glimpse(ptcp)
```

Use `distinct()` and `n_distinct()` to check on the number of unique participantIds in `ptcp` table.

```{r}
ptcp %>% 
  distinct(participantId) %>% 
  n_distinct()
```

Currently, the `ptcp` table still contain the demographic records of the 131 residents who moved out. Let us remove their records by using similar method used in removing the same records in `financial` table.

```{r}
ptcp1 <- ptcp[!ptcp$participantId %in% missing_id_vector, ]

ptcp1 %>% 
  distinct(participantId) %>% 
  n_distinct()
```

Both `finance1` and `ptcp1` tables now contains information about the same number of participantIds.

### 2.3.3 Convert `finance1` table to wide format and perform left outer join with `ptcp1` table.

We will now convert the `finance1` dataframe from a long to a wide format. The code chunk below does the following:

-   group the data by **participantId** , **date** and **category** using the **`group_by`** function

-   use the `sum` function to calculate the total monthly amount for each **category** per **participantId** per **month**

-   the `pivot_wider` function will convert the **category** column to wide format with total monthly values in the **amount** column.

    ```{r}
    finance1_wide<- finance1 %>%
      group_by(participantId, date, category) %>%
      summarise(total_amount = sum(amount)) %>%
      pivot_wider(names_from = category, values_from = total_amount)
    ```

    ```{r}
    #| echo: false
    finance1_wide
    ```

    ::: callout-note
    About finance1_wide table

    `finance_wide` is a table that has one row for each unique combination of **participantId** and **month** and one column for each unique category from the former `finance1` table.
    :::

    The code chunk below performs a left outer join with finance1_wide table (left) and ptcp1 table (right) with join key **participantId**.

    ```{r}
    # left outer join
    finance1_wide_ptcp1 <- left_join(finance1_wide, ptcp1, by = "participantId")
    ```

    The first 12 rows of the cleansed finance1_wide_ptcp1 is displayed using `knitr::kable()`{style="caret-color: white;"} function. It contains 10,560 rows and 14 columns.

    ```{r}
    knitr::kable(head(finance1_wide_ptcp1,12), "simple") 
    ```

# 3 Visualisation

## 3.1 Wage and categories of expenses

In this section, I will explore the dataset from high level and then zoom into interesting patterns (if I can find any =))

### 3.1.1 Normality assumptions of annual wage

Before zooming into wages in March, we will first perform a test to confirm whether wage follows the normal distribution.

H0: The wage does not follow a normal distribution.

H1: The wage follows a normal distribution.

```{r}
#| code-fold: True
set.seed(123) # for reproducibility
sample_5000<- finance1_wide_ptcp1[sample(1:nrow(finance1_wide_ptcp1), 5000), ] # perform sampling as limit of shapiro test is 5000


qq <- ggplot(sample_5000,
       aes(sample=Wage)) +  #<<< use a new argument call sample: el scores
  
  stat_qq() +
  stat_qq_line() +
  ggtitle("QQ plot with Shapiro-Wilk test results")  # add plot title

sw_t <- shapiro_test(sample_5000$Wage) %>% 
  as_tibble() %>% 
  mutate(variable = "Wage")%>% gt()  #<<< make into a gt format (will give a nice table)  shapiro.test is not used here as it gives output in another format.

tmp <- tempfile(fileext = '.png') # create  temp table
gtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder
table_png <- png::readPNG(tmp, native = TRUE)

qq+table_png

```

From the Shapiro test , p-value \< 0.05 and we have enough statistical evidence to reject the null hypothesis and conclude that Wage does not follow the normal distribution.

### 3.1.2 Interactive Line charts of wages by month

Preparing the data, creating a **highlevel** dataframe containing the rows date (month) and aggregated columns of expenses and wage only.

```{r}
#| code-fold: True
highlevel <- finance1_wide_ptcp1 %>%
  group_by(date) %>%
  summarize(Education = round(sum(Education, na.rm = TRUE)),
                Food = round(sum(Food, na.rm = TRUE)),
                Recreation = round(sum(Recreation, na.rm = TRUE)),
                Shelter = round(sum(Shelter, na.rm = TRUE)),
                Wage = round(sum(Wage, na.rm = TRUE)),
                RentAdjustment = round(sum(RentAdjustment, na.rm = TRUE)),
                ExpenseP = sum(Education, Food, Recreation, Shelter),  #<<<
                Income = sum(Wage, RentAdjustment),                    #<<<
                Saving = Income - ExpenseP,                            #<<<
                Expense = ExpenseP * -1                                #<<<
                )
head(highlevel,5)
```

An interactive line chart showing us an overview of total income across months.

```{r}
#| code-fold: True
library(scales)

q1<-highlevel %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = Wage, color = "Red", linetype = "Wage"), size = 1) +

  
  # annotating the plot
  geom_text(aes(x=as.Date("2022-04-01"),
                y=6000000,
                label="High wages \nobserved in \nMarch"), 
            hjust=1, vjust=1, color='black', size=2.5) +
  geom_text(aes(x=as.Date("2022-12-01"), y=3800000, label="Wage"),
            hjust=1, vjust=1,color='red', size=2.5) +


  # scale control
  labs(x = "Month", y = "Amount") +
  scale_x_date(date_breaks = '1 month',date_labels = "%b %Y") +
  scale_y_continuous(limits = c(0, 6500000), breaks=seq(0, 6500000, 1000000),
                     labels= comma) +
  
  theme_light(base_size = 12) +
  theme(axis.title = element_text(size = 10 , face = "bold"),
        axis.text = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.line = element_line(size = 1),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_line(colour='black'),
        panel.border = element_blank(),
        legend.position = "none",
        legend.title = element_blank()) +


  labs(title= 'Cumulative Income across Months',
       x='Month',
       y='Amount')

ggplotly(q1,tooltip = c('labels','x','y'))
```

### 3.1.3 Boxplot, One-way Anova and Error Plot of wage across months

Now we willl examine the distribution of wages across the months by education levels using boxplots.

::: panel-tabset
#### Boxplots

The boxplot shows the distribution of wages of the residents across all the months. The red dot represents the median wage of that month. Wage is much higher in March and possible reasons could be due to Harvest / Bonus month. Are the medians of the month wage significantly different from one another? (See next tab)

```{r}
#| code-fold: True
#| fig-width: 12
#| fig-asp: 0.618
library(lubridate)

df_wage_edu_month <- finance1_wide_ptcp1 %>%
  mutate(month = month(date))

df_wage_edu_month$month <- factor(month(finance1_wide_ptcp1$date), 
                                  levels = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2), 
                                  labels = c("Mar 22", "Apr 22", "May 22", "Jun 22", "Jul 22", "Aug 22", "Sep 22", "Oct 22", "Nov 22", "Dec 22", "Jan 23", "Feb 23"))


ggplot(df_wage_edu_month,
       aes(x = month, y = Wage)) +
  geom_boxplot(aes(fill = educationLevel)) +
  stat_summary(fun.y = "median", geom = "point", color = "red", size = 2) +
  labs(x = "Month", y = "Wage", fill = "Education Level", title='Wage Across education level by month') +
  scale_fill_brewer(palette="RdBu") + 
  theme_minimal() +
  theme(legend.key.size = unit(0.5,'cm'),
        legend.position="bottom",
        plot.title = element_text(size = 12,
                                  face='bold'),
        axis.title = element_text(size = 11 , face = "bold"),
        axis.text = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1))

```

#### One way Anova plot

The plot below shows us the pairs of months where wage are significantly different. Median Wage of March is significantly different from all the wages of the other months. Check the next tab to see the error plots.

```{r}
#| code-fold: True
#| fig-width: 12
#| fig-asp: 0.618
ggbetweenstats(data = df_wage_edu_month, x = month, y = Wage,
               xlab = "Month", ylab = "Wage",
               type = "np", pairwise.comparisons = TRUE, pairwise.display = "s",
               sort = "descending",
               sort.fun = median,
               mean.ci = T, p.adjust.method = "fdr",  conf.level = 0.95,
               title = "Comparison of Median Wage across Months") +
 # scale_y_continuous(limits = c(0, 300000)) +
   theme(axis.title.y=element_text(angle = 0,
                                  vjust=0.9))
```

#### Error plots

As we are analyzing survey data, each of our sample mean could vary from the actual population mean. Thus we have to visualize the margin of error. The higher the CI, the higher the margin of error.

95% and 99% confidence intervals are constructed for the median wage for each month.

Note: A confidence level of 95% means the true result will be within the error bar range 95 times out of 100 sampling tries.

```{r}
#| code-fold: True
#| fig-width: 12
#| fig-asp: 0.618
df_wage_edu_month %>%
  ggplot(aes(x = month, y = Wage)) +
  
  #Using stat_pointinterval to plot the points and intervals
  stat_pointinterval(.width = c(0.95,0.99),
  .point = median,
  .interval = qi,
  aes(interval_color=stat(level)),
  show.legend = FALSE) +
  
  #Defining the color of the intervals 
  scale_color_manual(
    values = c("blue", "darkblue"),
    aesthetics = "interval_color") +
  
  #Title, subtitle, and caption
  labs(
    title = "Visualising confidence intervals of median wage",
    subtitle = "Median Point + Multiple-interval plot, 95% and 99%",
    x = "Months", y = "Wage") +
  
  theme_ipsum() +
  
  theme(axis.title = element_text(size = 10 , face = "bold"),
        axis.text = element_text(size = 10),
        axis.title.y=element_text(angle = 0, 
                                  vjust=0.9, 
                                  size = 10, 
                                  face='bold'),
        axis.title.x=element_text(size = 10,
                                   face='bold'),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1),
        plot.title = element_text(size = 12,
                                  face='bold'),
        panel.border = element_blank(),
        panel.grid.major = element_blank())
```
:::

### 3.1.4 Interactive Line charts of expenditures by month

**Design considerations:**

Instead of combining Education, Recreation, Food and Shelter expense in one chart, I have plotted them on one chart each with different Y axis range. This will be enable us to visualise variability of amount across categories clearly.

```{r}
#| code-fold: True
#| fig-width: 10
#| fig-asp: 0.618

s <- highlevel %>%
  plot_ly(x = ~date, y = ~Shelter, type = 'scatter', mode = 'lines', name='Shelter') %>%
  layout(
         xaxis = list(title = "Date"), 
         yaxis = list(title = "Shelter"),
         plot_bgcolor = "#e5ecf6")


e <- highlevel %>%
  plot_ly(x = ~date, y = ~Education, type = 'scatter', mode = 'lines', name='Education') %>%
  layout(xaxis = list(title = "Date"), yaxis = list(title = "Education"))

f <- highlevel %>%
  plot_ly(x = ~date, y = ~Food, type = 'scatter', mode = 'lines', name='Food') %>%
  layout(xaxis = list(title = "Date"), yaxis = list(title = "Food"))

r <- highlevel %>%
  plot_ly(x = ~date, y = ~Recreation, type = 'scatter', mode = 'lines', name='Recreation') %>%
  layout(xaxis = list(title = "Date"), yaxis = list(title = "Recreation"))

#subplot(s, e, f, r,titleX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>% layout(title = "Custom Hover Text")

subplot(s, e, f, r, shareX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>%
  layout(title = "<b>Annual expenses by month<b>",
         plot_bgcolor='#e5ecf6', 
         xaxis = list( 
           zerolinecolor = '#ffff', 
           zerolinewidth = 2, 
           gridcolor = 'ffff'), 
         yaxis = list( 
           zerolinecolor = '#ffff', 
           zerolinewidth = 2, 
           gridcolor = 'ffff'))
```

From all the plots above, March seems to be an exciting month where there is several anomalies observed. There are unusual spikes in wage, recreational and shelter spending.

Next, we will plot a coordinated dotplot to studying the distribution of expenses. Since we have the daily trasnaction data of participant, which we can aggregate to find the annual spending for each type of expense.

First prepare the **annual** dataframe that contains participantId, their demographics data (e.g. educationLevel, haveKids etc.. ) and annual expense amount (e.g. Education, Shelter etc..) . The first 5 rows of the **annual** df is displayed using knitr::kable() function. It contains 880 rows and 12 columns.

```{r}
#| code-fold: True
annual <- finance1_wide_ptcp1 %>% 
  group_by(participantId, householdSize, haveKids, educationLevel, interestGroup, joviality) %>% 
  summarize(Education = sum(Education, na.rm = TRUE),
            Food = sum(Food, na.rm = TRUE),
            Recreation = sum(Recreation, na.rm = TRUE),
            Shelter = sum(Shelter, na.rm = TRUE),
            Wage = sum(Wage, na.rm = TRUE),
            RentAdjustment = sum(RentAdjustment, na.rm = TRUE)) 

knitr::kable(head(annual,3), "simple")
```

::: callout-note
Note that the **`na.rm = TRUE`** argument is used in the **`sum`** function to handle missing values in the columns during aggregation.
:::

### 3.1.5 Coordinated interactive dotplot of amount spent across categories

Hovering the cursor over the dots will show whether a participant have kids (True) or do not have kids (False). A participant will incur educational expenses when they have kids. Also observed that those with kids are among those who spend slightly more in shelter.

```{r}
#| code-fold: True
#| fig-width: 10
#| fig-asp: 0.618


sdot <- ggplot(data=annual,
            aes(x=Shelter)) +
  geom_dotplot_interactive(aes(tooltip = haveKids,  #<<<
                               data_id = haveKids),  #<<<
                           stackgroups = TRUE,
                           #binwidth = 2500,
                           method = "histodot",
                           dotsize= 0.2) +
  scale_y_continuous(NULL,
                     breaks= NULL) +  #null to suppress axis labels
 
  theme_bw()  +
  labs(title= 'Amount-spent distribution across category') +
  theme(plot.title = element_text(size = 13,
                                  face='bold'))

edot <- ggplot(data=annual,
            aes(x=Education)) +
  geom_dotplot_interactive(aes(tooltip = haveKids,
                               data_id = haveKids), 
                           stackgroups = TRUE,
                           #binwidth = 2500,
                           method = "histodot",
                           dotsize= 0.2) +
  scale_y_continuous(NULL,
                     breaks= NULL) +
 
  theme_bw()

fdot <- ggplot(data=annual,
            aes(x=Food)) +
  geom_dotplot_interactive(aes(tooltip = haveKids,
                               data_id = haveKids), 
                           stackgroups = TRUE,
                           #binwidth = 2500,
                           method = "histodot",
                           dotsize= 0.2) +
  scale_y_continuous(NULL,
                     breaks= NULL) +
 
  theme_bw()

rdot <- ggplot(data=annual,
            aes(x=Recreation)) +
  geom_dotplot_interactive(aes(tooltip = haveKids,
                               data_id = haveKids), 
                           stackgroups = TRUE,
                           #binwidth = 2500,
                           method = "histodot",
                           dotsize= 0.2) +
  scale_y_continuous(NULL,
                     breaks= NULL) +
 
  theme_bw()

girafe(code = print(sdot + edot + fdot + rdot), 
       width_svg = 10,
       height_svg = 10 *0.618,
       options = list(                          #<<<
         opts_hover(css='fill: blue;'),      #<<<
         opts_hover_inv(css = 'opacity: 0.2;')  #<<<
         )
       )
```

## 3.2 Demographics - Finance Analysis

### 3.2.1 Distribution of Annual wage across Education Levels

#### 3.2.1.1 Ridgeline plot

We will use the **annual** dataframe to visualise the distribution of annual wage across education levels.

```{r}
#| code-fold: True
#| fig-width: 8
#| fig-asp: 0.618
ggplot(annual, 
       aes(x = Wage, 
           y = educationLevel,
           fill = stat(x))) +
  geom_density_ridges_gradient(
    scale = 3,
    rel_min_height = 0.01) +
  scale_fill_viridis_c(name = "Wage",
                       option = "D") +
  scale_x_continuous(
    name = "Annual Wage",
    expand = c(0, 0)
  ) +
  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +
  theme_ridges()
```

#### 3.2.1.4 One- way Anova plot

Since Wage does not follow normal distribution, I will choose a non parametric test to compare whether there is significant difference in the median of wage between education levels.

```{r}
#| code-fold: True
#| fig-width: 10
#| fig-asp: 0.618
ggbetweenstats(data = annual, x = educationLevel, y = Wage,
               xlab = "Education level", ylab = "Annual Wage",
               type = "np", pairwise.comparisons = TRUE, pairwise.display = "s",
               sort = "descending",
               sort.fun = median,
               mean.ci = T, p.adjust.method = "fdr",  conf.level = 0.95,
               title = "Comparison of Median Annual Wage across Education Levels") +
  scale_y_continuous(limits = c(0, 300000)) +
   theme(axis.title.y=element_text(angle = 0,
                                  vjust=0.9))
```

For 4 categories of education levels, we can have a total of 4C2 = k(k-1)/2 (=6) possible combinations of pairs.

From the results, all six pairwise comparison p-values are less than 0.05 and thus we can reject the null hypothesis and conclude that the median wages across all different educational levels are all different from one another.

#### 3.2.1.5 Error plots

The error bar for mean Median Wage is the longest for Graduate education level and this could be due to outliers in this category.

```{r}
#| code-fold: True
annual %>%
  ggplot(aes(x = educationLevel, y = Wage)) +
  
  #Using stat_pointinterval to plot the points and intervals
  stat_pointinterval(.width = c(0.95,0.99),
  .point = median,
  .interval = qi,
  aes(interval_color=stat(level)),
  show.legend = FALSE) +
  
  #Defining the color of the intervals 
  scale_color_manual(
    values = c("blue", "darkblue"),
    aesthetics = "interval_color") +
  
  #Title, subtitle, and caption
  labs(
    title = "Visualising confidence intervals of median math score",
    subtitle = "Median Point + Multiple-interval plot, 95% and 99%",
    x = "Education  level", y = "Wage") +
  
  theme_ipsum() +
  
  theme(axis.title.y=element_text(angle = 0, 
                                  vjust=0.9, 
                                  size = 10, 
                                  face='bold'),
        axis.title.x=element_text(size = 10,
                                   face='bold'),
        plot.title = element_text(size = 12,
                                  face='bold'),
        panel.border = element_blank(),
        panel.grid.major = element_blank())
```

### 3.2.2 Distribution of Joviality Index across Education levels

#### 3.2.2.1 Ridgeline plot

```{r}
#| code-fold: True
#| fig-width: 8
#| fig-asp: 0.618
ggplot(annual, 
       aes(x = joviality, 
           y = educationLevel,
           fill = stat(x))) +
  geom_density_ridges_gradient(
    scale = 3,
    rel_min_height = 0.01) +
  scale_fill_viridis_c(name = "Joviality",
                       option = "D") +
  scale_x_continuous(
    name = "Joviality",
    expand = c(0, 0)
  ) +
  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +
  theme_ridges()
```

#### 3.2.2.2 Normality check on Joviality Index

Perform a test to confirm whether joviality index follows the normal distribution.

```{r}
#| code-fold: True
qq <- ggplot(annual,
       aes(sample=joviality)) + 
  
  stat_qq() +
  stat_qq_line() +
  ggtitle("QQ plot with Shapiro-Wilk test results")  

sw_t <- shapiro_test(annual$joviality) %>% 
  as_tibble() %>% 
  mutate(variable = "Joviality Index")%>% gt()  

tmp <- tempfile(fileext = '.png') # create  temp table
gtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder
table_png <- png::readPNG(tmp, native = TRUE)

qq+table_png
```

We have enough statistical evidence to reject the null hypothesis that joviality index follows normal distribution. Use a non parametric test below to test for difference in median of joviality index across education levels.

#### 3.2.2.2 One way Anova plot

From the results, only 2 pairwise comparison p-values \<0.05 and thus we can reject the null hypothesis for (High School & Graduate) and (High School & Bachelors) and conclude that the median Joviality index for these two pairs of education levels are different.

```{r}
#| code-fold: True
#| fig-width: 10
#| fig-aspect: fig-width*0.618
ggbetweenstats(data = annual, x = educationLevel, y = joviality,
               xlab = "Education level", ylab = "Joviality Index",
               type = "np", pairwise.comparisons = TRUE, pairwise.display = "s",
               sort = "descending",
               sort.fun = median,
               mean.ci = T, p.adjust.method = "fdr",  conf.level = 0.95,
               title = "Comparison of Median Joviality index across Education Levels") +
  scale_y_continuous(limits = c(0, 2)) +
   theme(axis.title.y=element_text(angle = 0,
                                  vjust=0.9))
```

#### 3.2.2.3 Error Plot

```{r}
#| code-fold: True
annual %>%
  ggplot(aes(x = educationLevel, y = joviality)) +
  
  #Using stat_pointinterval to plot the points and intervals
  stat_pointinterval(.width = c(0.95,0.99),
  .point = median,
  .interval = qi,
  aes(interval_color=stat(level)),
  show.legend = FALSE) +
  
  #Defining the color of the intervals 
  scale_color_manual(
    values = c("blue", "darkblue"),
    aesthetics = "interval_color") +
  
  #Title, subtitle, and caption
  labs(
    title = "Visualising confidence intervals of median joviality index",
    subtitle = "Median Point + Multiple-interval plot, 95% and 99%",
    x = "Education  level", y = "Joviality Index") +
  
  theme_ipsum() +
  
  theme(axis.title.y=element_text(angle = 0, 
                                  vjust=0.9, 
                                  size = 10, 
                                  face='bold'),
        axis.title.x=element_text(size = 10,
                                   face='bold'),
        plot.title = element_text(size = 12,
                                  face='bold'),
        panel.border = element_blank(),
        panel.grid.major = element_blank())
```

## 3.3 Correlation between Annual Shelter cost and Annual Wage

In this section, we will investigate if people who earn more will also spend more on shelter. We will also subset the data across education levels.

We will use the non-parametric Spearman correlation analysis instead of Pearson correlation since the wage data is not normally distributed.

```{r}
#| code-fold: True
#| fig-width: 12
#| fig-asp: 0.618
low_correl <- ggscatterstats(data = annual |> filter(educationLevel == "Low"), 
                           x = Wage, y = Shelter,
                           type = "nonparametric") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_x_continuous(limits = c(0, 70000), 
                     breaks=seq(0, 70000, 10000), 
                     labels= comma) + 
  scale_y_continuous(limits = c(0, 20000), 
                     breaks=seq(0, 20000, 5000), 
                     labels= comma) +
  
  labs(title = "Low Education Status", 
       x = "Annual Wage", y = "Annual Shelter fee") 



high_correl <- ggscatterstats(data = annual |> filter(educationLevel == "HighSchoolOrCollege"), 
                           x = Wage, y = Shelter,
                           type = "nonparametric") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_x_continuous(limits = c(0, 70000), 
                     breaks=seq(0, 70000, 10000), 
                     labels= comma) + 
  scale_y_continuous(limits = c(0, 20000), 
                     breaks=seq(0, 20000, 5000), 
                     labels= comma) +
  
  labs(title = "High Sch Education Status", 
       x = "Annual Wage", y = "Annual Shelter fee") 



bac_correl <- ggscatterstats(data = annual |> filter(educationLevel == "Bachelors"), 
                           x = Wage, y = Shelter,
                           type = "nonparametric") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_x_continuous(limits = c(0, 70000), 
                     breaks=seq(0, 70000, 10000), 
                     labels= comma) + 
  scale_y_continuous(limits = c(0, 20000), 
                     breaks=seq(0, 20000, 5000), 
                     labels= comma) +
  
  labs(title = "Degree Education Status", 
       x = "Annual Wage", y = "Annual Shelter fee") 


grad_correl <- ggscatterstats(data = annual |> filter(educationLevel == "Graduate"), 
                           x = Wage, y = Shelter,
                           type = "nonparametric") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_x_continuous(limits = c(0, 70000),
                     breaks=seq(0, 70000, 10000), 
                     labels= comma) + 
  scale_y_continuous(limits = c(0, 20000),
                     breaks=seq(0, 20000, 5000), 
                     labels= comma) +
  
  labs(title = "Graduate Education Status", 
       x = "Annual Wage", y = "Annual Shelter fee") 


# combining plots using patchwork
p_correl <- (low_correl + high_correl) / (bac_correl + grad_correl) #+ plot_spacer() + plot_spacer()
p_correl + plot_annotation(title = "Correlation between Shelter spending and Annual Wage", 
                           theme = theme(plot.title = element_text(size = 18),
                                         plot.subtitle = element_text(size = 12))) + plot_layout(heights = c(2,2))

```

H0: There is no \[monotonic\] association between the annual shelter fees and wage.

H1: There is association between the annual shelter fees and wage.

From the plots above, there are no strong correlation values above 0.7. only the 'low' and 'degree' education group showed p-values less than 0.05 which shows that there is a significant association between shelter fee and wage. However, the correlation is weak between shelter spending and wage.

::: callout-note
Spearman correlation

The Spearman correlation is not a linear correlation of the data, but a linear correlation of a transformed version of the data \-- specifically, the correlation of the rank-transformed data. Do not be mislead by the slope direction.
:::

### 

# 4 References

Heat Map with ggplot2. (2023). Retrieved May 9, 2023, from <https://r-charts.com/correlation/heat-map-ggplot2/#:~:text=Heat%20map%20with%20geom_tile,-A%20heap%20map&text=You%20can%20customize%20the%20border,%2C%20lwd%20and%20linetype%20%2C%20respectively.&text=In%20addition%2C%20you%20can%20add,argument%20of%20the%20aes%20function>.

Downward slope but Spearman correlation coefficient is positive (and vice versa). (2022, April 5). In Stack Exchange. Retrieved May 9, 2023, from <https://stats.stackexchange.com/questions/545516/downward-slope-but-spearman-correlation-coefficient-is-positive-and-vice-versa>

Stack Overflow. (2016, December 31). Getting separate axis labels on R plotly subplots. Retrieved May 11, 2023, from <https://stackoverflow.com/questions/41324934/getting-separate-axis-labels-on-r-plotly-subplots#:~:text=To%20get%20x%2Daxis%20labels,%3D%20FALSE%20(the%20default)>.
