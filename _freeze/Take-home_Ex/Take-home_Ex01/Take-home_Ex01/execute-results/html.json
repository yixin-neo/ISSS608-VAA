{
  "hash": "cba62b826a9de52ba29153151572b613",
  "result": {
    "markdown": "---\ntitle: \"Take-home_Ex01\"\nauthor: \"NeoYX\"\ndate: '7 May 2023'\n#date-modified: \"2023-05-11\"\neditor: visual\nexecute: \n  freeze: auto\n  warning: false\n  #echo: false\n  #message: false\n  html:\n    code-fold: True\n    code-overflow: scroll\n    code-summary: \"Show the code\"\n    code-line-numbers: true\n---\n\n\n# 1. Task and Dataset\n\nThis exerise aims to reveal the demographic and financial characteristics of the city of Engagement, using appropriate **static and interactive statistical graphics** methods. It also requires a user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.\n\nThe dataset consists of a sample **survey** of 1000 representative residents that collects data related to their household demographic and spending patterns, among other things. There are primarily two datasets used in this exercise\n\n-   'FinancialJournal.csv\": Contains 1513635 number of daily transaction records (different categories of income and expenses) over a period of twelve months from March 2022 to February 2023.\n\n-   'Particpants.csv\" : Contains demographics information like household size, age, education level, interest groups, joviality index and whether each household has kids.\n\nIn this exercise, each dataset will be cleansed separately and then joined by 'participantID' as primary key to form the final dataset used for further analysis.\n\n# \n\n# 2. Data Preparation\n\n## 2.1 Install and load the required libraries\n\nThe code chunk below uses `pacman::p_load()` to check if packages are installed. If they are, they will be launched into R. The packages installed are\n\n-   `plotly`: Used for creating interactive web-based graphs.\n\n-   `knitr`: Used for dynamic report generation\n\n-   `patchwork`: Used to combine plots\n\n-   `tidyverse`: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\n\n-   `ggthemes`: Provide additional themes for `ggplot2`\n\n-   `ggstatsplot`: Used for creating graphics with details from statistical tests.\n\n-   `ggdist`: Used for visualising distribution and uncertainty\n\n-   `rstatix`: Allows us to perform basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\n\n-   `gt` : starting from a tibble table, customise a table and export in various formats. Most importantly, it works with patch. We will save the tabular results from shapiro test as gt object and export using gtsave() into .png file later.\n\n-   \n\n-   \n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(plotly, knitr, patchwork, tidyverse, ggthemes,hrbrthemes, ggiraph, ggstatsplot, ggdist, png, gifski, rstatix, gt)\n```\n:::\n\n\n## 2.2 Import the dataset\n\nThe datasets are imported using `tidyverse`'s `readr::read_csv()` function.\n\n'FinancialJournal.csv\" is stored as `finance` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinance <- read_csv('data/FinancialJournal.csv')\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  participantId timestamp           category  amount\n          <dbl> <dttm>              <chr>      <dbl>\n1             0 2022-03-01 00:00:00 Wage      2473. \n2             0 2022-03-01 00:00:00 Shelter   -555. \n3             0 2022-03-01 00:00:00 Education  -38.0\n4             1 2022-03-01 00:00:00 Wage      2047. \n5             1 2022-03-01 00:00:00 Shelter   -555. \n6             1 2022-03-01 00:00:00 Education  -38.0\n```\n:::\n:::\n\n\nCheck for empty values in the `finance` table using the `is.na()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(is.na(finance))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n'Particpants.csv\" is stored as `ptcp` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nptcp <- read_csv('data/Participants.csv')\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  participantId householdSize haveKids   age educationLevel      interestGroup\n          <dbl>         <dbl> <lgl>    <dbl> <chr>               <chr>        \n1             0             3 TRUE        36 HighSchoolOrCollege H            \n2             1             3 TRUE        25 HighSchoolOrCollege B            \n3             2             3 TRUE        35 HighSchoolOrCollege A            \n4             3             3 TRUE        21 HighSchoolOrCollege I            \n5             4             3 TRUE        43 Bachelors           H            \n6             5             3 TRUE        32 HighSchoolOrCollege D            \n# ℹ 1 more variable: joviality <dbl>\n```\n:::\n:::\n\n\nChecking for empty values in `ptcp` table using the `is.na()`\\` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(is.na(ptcp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n## 2.3 Data Issues and wrangling\n\nI will discuss the issues in the datasets and proposed cleaning methods.\n\n### 2.3.1 **finance** dataset issues:\n\n-   **participantId** should be converted from `<dbl>` format to `<chr>` format. It should be a categorical and not numerical data type.\n\n-   **timestamp** should be converted from `<dttm>` format to `<date>` format as I will not be analysing time in this exercise.\n\n-   Negative values of **amount** that belong to the expenses categories should be converted to positive values. The amount will also be rounded to two decimal places.\n\nThe code chunk below does the following:\n\n-   use the **`as.character()`** function to convert **participantId** to `<chr>` format\n\n-   create a new column **month_year** by extracting the year and month from the **timestamp** column using the **`format()`** function with the **`%Y-%m`** format specifier.\n\n-   use the **`abs()`** function to convert negative values **amount** to positive and round the values to 2 decimal places using the **`round()`** function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert participantId to character\nfinance <- finance %>% mutate(participantId = as.character(participantId))\n\n# Extract month and year from timestamp\nfinance <- finance %>% \n  mutate(month_year = format(timestamp, \"%m-%Y\"))\n\n# Transform negative amounts to positive and round to 2 decimal places\nfinance <- finance %>% \n  mutate(amount = abs(amount),\n         amount = round(amount, 2))\n```\n:::\n\n\nA check for duplicates using the `duplicated()` function reveals that there are 1,113 records of duplicates.\n\n-   The **`duplicated()`** function to identify the duplicate rows. It returns a logical vector indicating whether each row is a duplicate of a previous row in the data frame. We can then use this logical vector to subset the data frame and show the duplicate rows. The logical vector is stored in a filter **duplicated_rows** which is used to subset the `finance` data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show duplicate rows\nduplicated_rows <- finance[duplicated(finance),]\nglimpse(duplicated_rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,113\nColumns: 5\n$ participantId <chr> \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"…\n$ timestamp     <dttm> 2022-03-01, 2022-03-01, 2022-03-01, 2022-03-01, 2022-03…\n$ category      <chr> \"Shelter\", \"Education\", \"Shelter\", \"Education\", \"Shelter…\n$ amount        <dbl> 554.99, 38.01, 554.99, 38.01, 556.55, 12.81, 554.99, 38.…\n$ month_year    <chr> \"03-2022\", \"03-2022\", \"03-2022\", \"03-2022\", \"03-2022\", \"…\n```\n:::\n:::\n\n\n-   **`unique()`** function is used to remove the duplicate rows form `finance` data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove duplicate rows\nfinance <- unique(finance)\n```\n:::\n\n\n-   Perform a final check to verify that there are no more duplicate using `any()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(duplicated(finance))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nThe last thing to do is to create a new column **date** that is in `<date>` format using the `as.Date` function.\n\n-   the paste0() function is used to concatenate \"01-\" with each value in the month_year column. This is because as.Date() requires a complete date in the format \"dd-mm-yyyy\"\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    finance$date <- as.Date(paste0(\"01-\", finance$month_year), format = \"%d-%m-%Y\")\n    ```\n    :::\n\n\n**Other issues**\n\nWhen the `finance` dataset is group-by the **date** variable , it is noticed that the number of distinct participantID who took part in the survey was 1,011 in March 2022 and suddenly reduced to a constant value of 880 from April 2022 onwards. It seems to suggest that there are 131 residents who moved out of the city at the end of March 2022.\n\nIn the code chunk below:\n\n-   dataset is group-by **date** and the distinct count of **participantID** is generated using `n_distinct` function\n\n-   the **missing** dataframe is displayed below using `knitr::kable()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmissing_summary <- finance %>%\n  group_by(date) %>% \n  summarise(n_distinct=n_distinct(participantId)) %>% \n  rename(`Number of unique participantId` = n_distinct)\n\nknitr::kable(missing_summary, \"simple\")\n```\n\n::: {.cell-output-display}\ndate          Number of unique participantId\n-----------  -------------------------------\n2022-03-01                              1011\n2022-04-01                               880\n2022-05-01                               880\n2022-06-01                               880\n2022-07-01                               880\n2022-08-01                               880\n2022-09-01                               880\n2022-10-01                               880\n2022-11-01                               880\n2022-12-01                               880\n2023-01-01                               880\n2023-02-01                               880\n:::\n:::\n\n\nSince 11 out of 12 months of records are missing for these 131 residents, we will delete their records from the `finance` dataset.\n\nThe code chunk below does the following:\n\n-   extract the **participantIds** of residents whose records exists in March 22 but not in all April 22 using the `anti-join` function\n\n-   pass the unique **`participantId`** column name as an argument to **`pull()`** and use the **`as.vector()`** function to convert the resulting tibble column to a vector\n\n-   resulting dataframe will only contain **`participantId`**s that are in '2022-03-01' but not in '2022-04-01' onwards. There are 131 of them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmissing_id <- finance %>%\n  filter(date == as.Date('2022-03-01')) %>% # filter for '2022-03-01' date\n  anti_join(finance %>%\n             filter(date == as.Date('2022-04-01')), # filter for '2022-04-01' date\n             by = 'participantId') %>% # anti-join by 'participantId'\n  select(participantId) %>% \n  distinct(participantId)\n\n# extract participantId column as convert this column to vector.\nmissing_id_vector <- as.vector(pull(missing_id, participantId))\n\nmissing_id_vector \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"44\"  \"127\" \"142\" \"154\" \"161\" \"256\" \"262\" \"267\" \"279\" \"285\" \"288\" \"298\"\n [13] \"301\" \"346\" \"352\" \"356\" \"380\" \"382\" \"383\" \"384\" \"392\" \"406\" \"407\" \"509\"\n [25] \"510\" \"512\" \"514\" \"523\" \"526\" \"539\" \"541\" \"553\" \"558\" \"567\" \"568\" \"572\"\n [37] \"574\" \"575\" \"577\" \"580\" \"589\" \"595\" \"599\" \"602\" \"603\" \"604\" \"605\" \"611\"\n [49] \"615\" \"617\" \"621\" \"628\" \"629\" \"634\" \"639\" \"641\" \"643\" \"647\" \"653\" \"655\"\n [61] \"657\" \"658\" \"663\" \"668\" \"670\" \"756\" \"757\" \"760\" \"761\" \"762\" \"768\" \"771\"\n [73] \"773\" \"774\" \"780\" \"785\" \"789\" \"790\" \"791\" \"792\" \"793\" \"794\" \"799\" \"802\"\n [85] \"806\" \"808\" \"816\" \"817\" \"818\" \"824\" \"825\" \"827\" \"828\" \"831\" \"832\" \"834\"\n [97] \"839\" \"842\" \"846\" \"847\" \"853\" \"855\" \"856\" \"858\" \"859\" \"860\" \"862\" \"864\"\n[109] \"867\" \"872\" \"875\" \"876\" \"883\" \"884\" \"885\" \"886\" \"887\" \"892\" \"896\" \"897\"\n[121] \"900\" \"901\" \"902\" \"907\" \"909\" \"911\" \"919\" \"920\" \"923\" \"924\" \"925\"\n```\n:::\n:::\n\n\nNext, we will remove all records of the 131 potentially non-residents from the `finance` dataset .\n\nIn the code chunk below:\n\n-   the **`%in%`** operator is to check if each **`id`** value is contained in the missing_id_vector\n\n-   the negation operator **`!`** ensures the resulting filtered data frame will not contain the rows where the **`id`** values are in missing_id_vector\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinance1 <- finance[!finance$participantId %in% missing_id_vector, ]\nfinance1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,509,897 × 6\n   participantId timestamp           category  amount month_year date      \n   <chr>         <dttm>              <chr>      <dbl> <chr>      <date>    \n 1 0             2022-03-01 00:00:00 Wage      2473.  03-2022    2022-03-01\n 2 0             2022-03-01 00:00:00 Shelter    555.  03-2022    2022-03-01\n 3 0             2022-03-01 00:00:00 Education   38.0 03-2022    2022-03-01\n 4 1             2022-03-01 00:00:00 Wage      2047.  03-2022    2022-03-01\n 5 1             2022-03-01 00:00:00 Shelter    555.  03-2022    2022-03-01\n 6 1             2022-03-01 00:00:00 Education   38.0 03-2022    2022-03-01\n 7 2             2022-03-01 00:00:00 Wage      2437.  03-2022    2022-03-01\n 8 2             2022-03-01 00:00:00 Shelter    557.  03-2022    2022-03-01\n 9 2             2022-03-01 00:00:00 Education   12.8 03-2022    2022-03-01\n10 3             2022-03-01 00:00:00 Wage      2367.  03-2022    2022-03-01\n# ℹ 1,509,887 more rows\n```\n:::\n:::\n\n\nWe will double check that the records of 131 non-residents have been removed from `finance1` dataframe.\n\nIn the code below\n\n-   **`distinct()`** function to extract the distinct **`participantId`** values from **`finance1`**\n\n-   the **`n_distinct()`** function will count the number of distinct **`participantId`** values in the resulting tibble\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinance1 %>% \n  distinct(participantId) %>% \n  n_distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 880\n```\n:::\n:::\n\n\n### 2.3.2 ptcp dataset issues:\n\n-   **participantId** should be converted from `<dbl>` format to `<chr>` format\n\n-   **householdSize** should be converted from `<dbl>`{style=\"caret-color: white;\"} format to `<fct>`{style=\"caret-color: white;\"} format. It does not make sense to have 2.5 persons.\n\n-   **age** should be converted from `<dbl>`{style=\"caret-color: white;\"} format to `<int>`{style=\"caret-color: white;\"} format.\n\n-   **educationLevel** should be converted from `<chr>` to `<fct>` . It should also be ordered according to 'Low', 'HighSchoolOrCollege', 'Bachelors' and 'Graduate'.\n\nThe code chunk below does the following:\n\n-   `as.character` and `as.factor` functions are used to convert **participantId** to `<chr>` , **householdSize** to `<fct>` and **age** to `<int>`.\n\n-   `factor(educationLevel, levels=c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")))` not only converts **educationLevel** to factor, but also order the values inside.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert to factor\nptcp <- ptcp %>% mutate(participantId = as.character(participantId))\nptcp <- ptcp %>% mutate(householdSize = as.factor(householdSize))\n\n# Convert educationLevel to factor and order accordingly\nptcp <- ptcp %>% mutate(educationLevel = factor(educationLevel, levels=c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")))\n\n# convert age to int\nptcp <- ptcp %>% mutate(age = as.integer(age))\n```\n:::\n\n\nThe columns format are all in order now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(ptcp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,011\nColumns: 7\n$ participantId  <chr> \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",…\n$ householdSize  <fct> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ haveKids       <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ age            <int> 36, 25, 35, 21, 43, 32, 26, 27, 20, 35, 48, 27, 34, 18,…\n$ educationLevel <fct> HighSchoolOrCollege, HighSchoolOrCollege, HighSchoolOrC…\n$ interestGroup  <chr> \"H\", \"B\", \"A\", \"I\", \"H\", \"D\", \"I\", \"A\", \"G\", \"D\", \"D\", …\n$ joviality      <dbl> 0.001626703, 0.328086500, 0.393469590, 0.138063446, 0.8…\n```\n:::\n:::\n\n\nUse `distinct()` and `n_distinct()` to check on the number of unique participantIds in `ptcp` table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nptcp %>% \n  distinct(participantId) %>% \n  n_distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1011\n```\n:::\n:::\n\n\nCurrently, the `ptcp` table still contain the demographic records of the 131 residents who moved out. Let us remove their records by using similar method used in removing the same records in `financial` table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nptcp1 <- ptcp[!ptcp$participantId %in% missing_id_vector, ]\n\nptcp1 %>% \n  distinct(participantId) %>% \n  n_distinct()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 880\n```\n:::\n:::\n\n\nBoth `finance1` and `ptcp1` tables now contains information about the same number of participantIds.\n\n### 2.3.3 Convert `finance1` table to wide format and perform left outer join with `ptcp1` table.\n\nWe will now convert the `finance1` dataframe from a long to a wide format. The code chunk below does the following:\n\n-   group the data by **participantId** , **date** and **category** using the **`group_by`** function\n\n-   use the `sum` function to calculate the total monthly amount for each **category** per **participantId** per **month**\n\n-   the `pivot_wider` function will convert the **category** column to wide format with total monthly values in the **amount** column.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    finance1_wide<- finance1 %>%\n      group_by(participantId, date, category) %>%\n      summarise(total_amount = sum(amount)) %>%\n      pivot_wider(names_from = category, values_from = total_amount)\n    ```\n    :::\n\n    ::: {.cell}\n    ::: {.cell-output .cell-output-stdout}\n    ```\n    # A tibble: 10,560 × 8\n    # Groups:   participantId, date [10,560]\n       participantId date       Education  Food Recreation Shelter   Wage\n       <chr>         <date>         <dbl> <dbl>      <dbl>   <dbl>  <dbl>\n     1 0             2022-03-01      38.0  268.      349.     555. 11932.\n     2 0             2022-04-01      38.0  266.      219.     555.  8637.\n     3 0             2022-05-01      38.0  265.      383.     555.  9048.\n     4 0             2022-06-01      38.0  257.      466.     555.  9048.\n     5 0             2022-07-01      38.0  270.     1069.     555.  8637.\n     6 0             2022-08-01      38.0  262.      314.     555.  9459.\n     7 0             2022-09-01      38.0  256.      295.     555.  9048.\n     8 0             2022-10-01      38.0  267.       25.0    555.  8637.\n     9 0             2022-11-01      38.0  261       377.     555.  9048.\n    10 0             2022-12-01      38.0  266.      357.     555.  9048.\n    # ℹ 10,550 more rows\n    # ℹ 1 more variable: RentAdjustment <dbl>\n    ```\n    :::\n    :::\n\n\n    ::: callout-note\n    About finance1_wide table\n\n    `finance_wide` is a table that has one row for each unique combination of **participantId** and **month** and one column for each unique category from the former `finance1` table.\n    :::\n\n    The code chunk below performs a left outer join with finance1_wide table (left) and ptcp1 table (right) with join key **participantId**.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # left outer join\n    finance1_wide_ptcp1 <- left_join(finance1_wide, ptcp1, by = \"participantId\")\n    ```\n    :::\n\n\n    The first 12 rows of the cleansed finance1_wide_ptcp1 is displayed using `knitr::kable()`{style=\"caret-color: white;\"} function. It contains 10,560 rows and 14 columns.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    knitr::kable(head(finance1_wide_ptcp1,12), \"simple\") \n    ```\n    \n    ::: {.cell-output-display}\n    participantId   date          Education     Food   Recreation   Shelter       Wage   RentAdjustment  householdSize   haveKids    age  educationLevel        interestGroup    joviality\n    --------------  -----------  ----------  -------  -----------  --------  ---------  ---------------  --------------  ---------  ----  --------------------  --------------  ----------\n    0               2022-03-01        38.01   268.26       348.68    554.99   11931.95               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-04-01        38.01   265.79       219.42    554.99    8636.88               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-05-01        38.01   264.54       382.99    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-06-01        38.01   256.90       465.67    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-07-01        38.01   270.13      1069.48    554.99    8636.88               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-08-01        38.01   261.76       314.13    554.99    9459.44               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-09-01        38.01   256.04       294.64    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-10-01        38.01   266.67        25.01    554.99    8636.88               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-11-01        38.01   261.00       377.41    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2022-12-01        38.01   265.98       356.69    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2023-01-01        38.01   264.97       209.77    554.99    9048.16               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    0               2023-02-01        38.01   239.05       319.93    554.99    8225.60               NA  3               TRUE         36  HighSchoolOrCollege   H                0.0016267\n    :::\n    :::\n\n\n# 3 Visualisation\n\n## 3.1 Wage and categories of expenses\n\nIn this section, I will explore the dataset from high level and then zoom into interesting patterns (if I can find any =))\n\n### 3.1.1 Normality assumptions of annual wage\n\nBefore zooming into wages in March, we will first perform a test to confirm whether wage follows the normal distribution.\n\nH0: The wage does not follow a normal distribution.\n\nH1: The wage follows a normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123) # for reproducibility\nsample_5000<- finance1_wide_ptcp1[sample(1:nrow(finance1_wide_ptcp1), 5000), ] # perform sampling as limit of shapiro test is 5000\n\n\nqq <- ggplot(sample_5000,\n       aes(sample=Wage)) +  #<<< use a new argument call sample: el scores\n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  # add plot title\n\nsw_t <- shapiro_test(sample_5000$Wage) %>% \n  as_tibble() %>% \n  mutate(variable = \"Wage\")%>% gt()  #<<< make into a gt format (will give a nice table)  shapiro.test is not used here as it gives output in another format.\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nFrom the Shapiro test , p-value \\< 0.05 and we have enough statistical evidence to reject the null hypothesis and conclude that Wage does not follow the normal distribution.\n\n### 3.1.2 Interactive Line charts of wages by month\n\nPreparing the data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nhighlevel <- finance1_wide_ptcp1 %>%\n  group_by(date) %>%\n  summarize(Education = round(sum(Education, na.rm = TRUE)),\n                Food = round(sum(Food, na.rm = TRUE)),\n                Recreation = round(sum(Recreation, na.rm = TRUE)),\n                Shelter = round(sum(Shelter, na.rm = TRUE)),\n                Wage = round(sum(Wage, na.rm = TRUE)),\n                RentAdjustment = round(sum(RentAdjustment, na.rm = TRUE)),\n                ExpenseP = sum(Education, Food, Recreation, Shelter),  #<<<\n                Income = sum(Wage, RentAdjustment),                    #<<<\n                Saving = Income - ExpenseP,                            #<<<\n                Expense = ExpenseP * -1                                #<<<\n                )\nhead(highlevel,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 11\n  date       Education   Food Recreation Shelter    Wage RentAdjustment ExpenseP\n  <date>         <dbl>  <dbl>      <dbl>   <dbl>   <dbl>          <dbl>    <dbl>\n1 2022-03-01     11424 320126     649580  631623 6068610          53504  1612753\n2 2022-04-01     11424 304282     389688  559919 3468757           1429  1265313\n3 2022-05-01     11424 313538     336413  558451 3623068              0  1219826\n4 2022-06-01     11424 302893     314804  558451 3608883              0  1187572\n5 2022-07-01     11424 313803     329608  558451 3485799              0  1213286\n# ℹ 3 more variables: Income <dbl>, Saving <dbl>, Expense <dbl>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(scales)\n\nq1<-highlevel %>%\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = Wage, color = \"Red\", linetype = \"Wage\"), size = 1) +\n\n  \n  # annotating the plot\n  geom_text(aes(x=as.Date(\"2022-04-01\"),\n                y=6000000,\n                label=\"High wages \\nobserved in \\nMarch\"), \n            hjust=1, vjust=1, color='black', size=2.5) +\n  geom_text(aes(x=as.Date(\"2022-12-01\"), y=3800000, label=\"Wage\"),\n            hjust=1, vjust=1,color='red', size=2.5) +\n\n\n  # scale control\n  labs(x = \"Month\", y = \"Amount\") +\n  scale_x_date(date_breaks = '1 month',date_labels = \"%b %Y\") +\n  scale_y_continuous(limits = c(0, 6500000), breaks=seq(0, 6500000, 1000000),\n                     labels= comma) +\n  \n  theme_light(base_size = 12) +\n  theme(axis.title = element_text(size = 10 , face = \"bold\"),\n        axis.text = element_text(size = 10),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.line = element_line(size = 1),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_line(colour='black'),\n        panel.border = element_blank(),\n        legend.position = \"none\",\n        legend.title = element_blank()) +\n\n\n  labs(title= 'Cumulative Income across Months',\n       x='Month',\n       y='Amount')\n\nggplotly(q1,tooltip = c('labels','x','y'))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"plotly html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-f10d14aaab4c6eec5ea0\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f10d14aaab4c6eec5ea0\">{\"x\":{\"data\":[{\"x\":[19052,19083,19113,19144,19174,19205,19236,19266,19297,19327,19358,19389],\"y\":[6068610,3468757,3623068,3608883,3485799,3763808,3610504,3483353,3607156,3627932,3623061,3292644],\"text\":[\"date: 2022-03-01<br />Wage: 6068610\",\"date: 2022-04-01<br />Wage: 3468757\",\"date: 2022-05-01<br />Wage: 3623068\",\"date: 2022-06-01<br />Wage: 3608883\",\"date: 2022-07-01<br />Wage: 3485799\",\"date: 2022-08-01<br />Wage: 3763808\",\"date: 2022-09-01<br />Wage: 3610504\",\"date: 2022-10-01<br />Wage: 3483353\",\"date: 2022-11-01<br />Wage: 3607156\",\"date: 2022-12-01<br />Wage: 3627932\",\"date: 2023-01-01<br />Wage: 3623061\",\"date: 2023-02-01<br />Wage: 3292644\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"(Red,Wage)\",\"legendgroup\":\"(Red,Wage)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[19083,19083,19083,19083,19083,19083,19083,19083,19083,19083,19083,19083],\"y\":[6000000,6000000,6000000,6000000,6000000,6000000,6000000,6000000,6000000,6000000,6000000,6000000],\"text\":[\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\",\"High wages <br />observed in <br />March\"],\"hovertext\":[\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\",\"as.Date(\\\"2022-04-01\\\"): 2022-04-01<br />y: 6e+06\"],\"textfont\":{\"size\":9.4488188976378,\"color\":\"rgba(0,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[19327,19327,19327,19327,19327,19327,19327,19327,19327,19327,19327,19327],\"y\":[3800000,3800000,3800000,3800000,3800000,3800000,3800000,3800000,3800000,3800000,3800000,3800000],\"text\":[\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\",\"Wage\"],\"hovertext\":[\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\",\"as.Date(\\\"2022-12-01\\\"): 2022-12-01<br />y: 3800000\"],\"textfont\":{\"size\":9.4488188976378,\"color\":\"rgba(255,0,0,1)\"},\"type\":\"scatter\",\"mode\":\"text\",\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":46.2864259028643,\"r\":7.97011207970112,\"b\":57.1982804131844,\"l\":85.0145288501453},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":15.9402241594022},\"title\":{\"text\":\"Cumulative Income across Months\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":19.1282689912827},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[19035.15,19405.85],\"tickmode\":\"array\",\"ticktext\":[\"Mar 2022\",\"Apr 2022\",\"May 2022\",\"Jun 2022\",\"Jul 2022\",\"Aug 2022\",\"Sep 2022\",\"Oct 2022\",\"Nov 2022\",\"Dec 2022\",\"Jan 2023\",\"Feb 2023\"],\"tickvals\":[19052,19083,19113,19144,19174,19205,19236,19266,19297,19327,19358,19389],\"categoryorder\":\"array\",\"categoryarray\":[\"Mar 2022\",\"Apr 2022\",\"May 2022\",\"Jun 2022\",\"Jul 2022\",\"Aug 2022\",\"Sep 2022\",\"Oct 2022\",\"Nov 2022\",\"Dec 2022\",\"Jan 2023\",\"Feb 2023\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(179,179,179,1)\",\"ticklen\":3.98505603985056,\"tickwidth\":0.362277821804596,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":13.2835201328352},\"tickangle\":-45,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":1.32835201328352,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"<b> Month <\\/b>\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":13.2835201328352}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-325000,6825000],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"1,000,000\",\"2,000,000\",\"3,000,000\",\"4,000,000\",\"5,000,000\",\"6,000,000\"],\"tickvals\":[0,1000000,2000000,3000000,4000000,5000000,6000000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"1,000,000\",\"2,000,000\",\"3,000,000\",\"4,000,000\",\"5,000,000\",\"6,000,000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(179,179,179,1)\",\"ticklen\":3.98505603985056,\"tickwidth\":0.362277821804596,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":1.32835201328352,\"showgrid\":true,\"gridcolor\":\"rgba(222,222,222,1)\",\"gridwidth\":0.362277821804596,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"<b> Amount <\\/b>\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":13.2835201328352}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":2.06156048675734,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":12.7521793275218}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"851c3c662aeb\":{\"x\":{},\"y\":{},\"colour\":{},\"linetype\":{},\"type\":\"scatter\"},\"851c45d822fc\":{\"x\":{},\"y\":{},\"label\":{}},\"851c22bd225b\":{\"x\":{},\"y\":{},\"label\":{}}},\"cur_data\":\"851c3c662aeb\",\"visdat\":{\"851c3c662aeb\":[\"function (y) \",\"x\"],\"851c45d822fc\":[\"function (y) \",\"x\"],\"851c22bd225b\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n### 3.1.3 Boxplot, One-way Anova and Error Plot of wage across months\n\nNow lets examine the wages across the months closer using boxplots.\n\n::: panel-tabset\n#### March 22 to Feb 23\n\nThe boxplot is able to show the distribution of wages of the residents across all the months. Wage is much higher in March and possible reasons could be due to Harvest / Bonus month. Are the medians of the month wage significantly different from one another? (See next tab)\n\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(lubridate)\n\ndf_wage_edu_month <- finance1_wide_ptcp1 %>%\n  mutate(month = month(date))\n\ndf_wage_edu_month$month <- factor(month(finance1_wide_ptcp1$date), \n                                  levels = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2), \n                                  labels = c(\"Mar 22\", \"Apr 22\", \"May 22\", \"Jun 22\", \"Jul 22\", \"Aug 22\", \"Sep 22\", \"Oct 22\", \"Nov 22\", \"Dec 22\", \"Jan 23\", \"Feb 23\"))\n\n\nggplot(df_wage_edu_month,\n       aes(x = month, y = Wage)) +\n  geom_boxplot(aes(fill = educationLevel)) +\n  stat_summary(fun.y = \"median\", geom = \"point\", color = \"red\", size = 2) +\n  labs(x = \"Month\", y = \"Wage\", fill = \"Education Level\") +\n  scale_fill_brewer(palette=\"RdBu\") + \n  theme_minimal() +\n  theme(legend.key.size = unit(0.5,'cm'),\n        legend.position=\"bottom\",\n        axis.title = element_text(size = 10 , face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-28-1.png){width=1152}\n:::\n:::\n\n\n#### One way Anova plot\n\nThe plot below shows us the pairs of months where wage are significantly different. See the next tab to see the error plots.\n\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggbetweenstats(data = df_wage_edu_month, x = month, y = Wage,\n               xlab = \"Month\", ylab = \"Wage\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Wage across Months\") +\n # scale_y_continuous(limits = c(0, 300000)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-29-1.png){width=1152}\n:::\n:::\n\n\n#### Error plots\n\nAs we are analyzing survey data, each of our sample mean could vary from the actual population mean. Thus we have to visualise the margin of error. The higher the CI, the higher the margin of error.\n\n95% and 99% confidence intervals are constructed for the median wage for each month.\n\nNote: A confidence level of 95% means the true result will be within the error bar range 95 times out of 100 sampling tries.\n\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf_wage_edu_month %>%\n  ggplot(aes(x = month, y = Wage)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of median wage\",\n    subtitle = \"Median Point + Multiple-interval plot, 95% and 99%\",\n    x = \"Months\", y = \"Wage\") +\n  \n  theme_ipsum() +\n  \n  theme(axis.title.y=element_text(angle = 0, \n                                  vjust=0.9, \n                                  size = 10, \n                                  face='bold'),\n        axis.title.x=element_text(size = 10,\n                                   face='bold'),\n        axis.text.x = element_text(angle = 45,\n                                   hjust = 1),\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank())\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-30-1.png){width=1152}\n:::\n:::\n\n:::\n\n### 3.1.4 Interactive (pending coordinated) Line chart of expenditures by month\n\nDesign considerations:\n\nInstead of combining Education, Recreation, Food and Shelter expense in one chart, I have plotted them on one chart each with different Y axis range. This will be enable us to visualise variability of amount across categories clearly.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ns <- highlevel %>%\n  ggplot(aes(x = date,y = Shelter)) +\n  geom_line() + \n  # axes scale control\n  scale_y_continuous(limits = c(550000, 650000), \n                     breaks=seq(550000, 650000, 50000), \n                     labels= comma) +\n  theme_clean()\n\n\ne <- highlevel %>%\n  ggplot(aes(x = date,y = Education)) +\n  geom_line() + \n  theme_clean()\n\nf <- highlevel %>%\n  ggplot(aes(x = date,y = Food)) +\n  geom_line() + theme_clean()\n\nr <- highlevel %>%\n  ggplot(aes(x = date,y = Recreation)) +\n  geom_line() +\n  scale_y_continuous(limits = c(250000, 650000), \n                     breaks=seq(250000, 650000, 50000), \n                     labels= comma)+\n  theme_clean()\n\n(s | e) / (f | r)\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n-   Work in progress.. tidy up axis ticks, titles title\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    glimpse(highlevel)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Rows: 12\n    Columns: 11\n    $ date           <date> 2022-03-01, 2022-04-01, 2022-05-01, 2022-06-01, 2022-0…\n    $ Education      <dbl> 11424, 11424, 11424, 11424, 11424, 11424, 11424, 11424,…\n    $ Food           <dbl> 320126, 304282, 313538, 302893, 313803, 312824, 302861,…\n    $ Recreation     <dbl> 649580, 389688, 336413, 314804, 329608, 306957, 301906,…\n    $ Shelter        <dbl> 631623, 559919, 558451, 558451, 558451, 558451, 558451,…\n    $ Wage           <dbl> 6068610, 3468757, 3623068, 3608883, 3485799, 3763808, 3…\n    $ RentAdjustment <dbl> 53504, 1429, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    $ ExpenseP       <dbl> 1612753, 1265313, 1219826, 1187572, 1213286, 1189656, 1…\n    $ Income         <dbl> 6122114, 3470186, 3623068, 3608883, 3485799, 3763808, 3…\n    $ Saving         <dbl> 4509361, 2204873, 2403242, 2421311, 2272513, 2574152, 2…\n    $ Expense        <dbl> -1612753, -1265313, -1219826, -1187572, -1213286, -1189…\n    ```\n    :::\n    :::\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\ns <- highlevel %>%\n  plot_ly(x = ~date, y = ~Shelter, type = 'scatter', mode = 'lines', name='Shelter') %>%\n  layout(\n         xaxis = list(title = \"Date\"), \n         yaxis = list(title = \"Shelter\"),\n         plot_bgcolor = \"#e5ecf6\")\n\n\ne <- highlevel %>%\n  plot_ly(x = ~date, y = ~Education, type = 'scatter', mode = 'lines', name='Education') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Education\"))\n\nf <- highlevel %>%\n  plot_ly(x = ~date, y = ~Food, type = 'scatter', mode = 'lines', name='Food') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Food\"))\n\nr <- highlevel %>%\n  plot_ly(x = ~date, y = ~Recreation, type = 'scatter', mode = 'lines', name='Recreation') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Recreation\"))\n\n#subplot(s, e, f, r,titleX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>% layout(title = \"Custom Hover Text\")\n\nsubplot(s, e, f, r, titleX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>%\n  layout(title = \"<b>Custom Hover Text<b>\",\n         plot_bgcolor='#e5ecf6', \n         xaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'), \n         yaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"plotly html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-cbfb4039aebcd68a545a\" style=\"width:100%;height:401px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-cbfb4039aebcd68a545a\">{\"x\":{\"data\":[{\"x\":[\"2022-03-01\",\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\"2023-01-01\",\"2023-02-01\"],\"y\":[631623,559919,558451,558451,558451,558451,558451,558451,558451,558451,558451,558451],\"mode\":\"lines\",\"name\":\"Shelter\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[\"2022-03-01\",\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\"2023-01-01\",\"2023-02-01\"],\"y\":[11424,11424,11424,11424,11424,11424,11424,11424,11424,11424,11424,11424],\"mode\":\"lines\",\"name\":\"Education\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"frame\":null},{\"x\":[\"2022-03-01\",\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\"2023-01-01\",\"2023-02-01\"],\"y\":[320126,304282,313538,302893,313803,312824,302861,313560,302831,313086,313255,282463],\"mode\":\"lines\",\"name\":\"Food\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"xaxis\":\"x3\",\"yaxis\":\"y3\",\"frame\":null},{\"x\":[\"2022-03-01\",\"2022-04-01\",\"2022-05-01\",\"2022-06-01\",\"2022-07-01\",\"2022-08-01\",\"2022-09-01\",\"2022-10-01\",\"2022-11-01\",\"2022-12-01\",\"2023-01-01\",\"2023-02-01\"],\"y\":[649580,389688,336413,314804,329608,306957,301906,326716,288286,316615,311575,271122],\"mode\":\"lines\",\"name\":\"Recreation\",\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"xaxis\":\"x4\",\"yaxis\":\"y4\",\"frame\":null}],\"layout\":{\"xaxis\":{\"domain\":[0,0.4],\"automargin\":true,\"title\":\"Date\",\"anchor\":\"y\",\"zerolinecolor\":\"#ffff\",\"zerolinewidth\":2,\"gridcolor\":\"ffff\"},\"xaxis2\":{\"domain\":[0.6,1],\"automargin\":true,\"title\":\"Date\",\"anchor\":\"y2\"},\"xaxis3\":{\"domain\":[0,0.4],\"automargin\":true,\"title\":\"Date\",\"anchor\":\"y3\"},\"xaxis4\":{\"domain\":[0.6,1],\"automargin\":true,\"title\":\"Date\",\"anchor\":\"y4\"},\"yaxis4\":{\"domain\":[0,0.4],\"automargin\":true,\"title\":\"Recreation\",\"anchor\":\"x4\"},\"yaxis3\":{\"domain\":[0,0.4],\"automargin\":true,\"title\":\"Food\",\"anchor\":\"x3\"},\"yaxis2\":{\"domain\":[0.6,1],\"automargin\":true,\"title\":\"Education\",\"anchor\":\"x2\"},\"yaxis\":{\"domain\":[0.6,1],\"automargin\":true,\"title\":\"Shelter\",\"anchor\":\"x\",\"zerolinecolor\":\"#ffff\",\"zerolinewidth\":2,\"gridcolor\":\"ffff\"},\"annotations\":[],\"shapes\":[],\"images\":[],\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"plot_bgcolor\":\"#e5ecf6\",\"hovermode\":\"closest\",\"showlegend\":true,\"title\":\"<b>Custom Hover Text<b>\"},\"attrs\":{\"851c12796e92\":{\"x\":{},\"y\":{},\"mode\":\"lines\",\"name\":\"Shelter\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"851c849753\":{\"x\":{},\"y\":{},\"mode\":\"lines\",\"name\":\"Education\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"851c3512349\":{\"x\":{},\"y\":{},\"mode\":\"lines\",\"name\":\"Food\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"851c636d7013\":{\"x\":{},\"y\":{},\"mode\":\"lines\",\"name\":\"Recreation\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"}},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"subplot\":true,\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n#https://stackoverflow.com/questions/41324934/getting-separate-axis-labels-on-r-plotly-subplots#:~:text=To%20get%20x%2Daxis%20labels,%3D%20FALSE%20(the%20default)\n```\n:::\n\n\nFrom all the plots above, March seems to be an exciting month where there is several anomalies observed. There are unusual spikes in wage, recreational and shelter spending.\n\nNext, lets plot a coordinated dotplot to studying the distribution of expenses since we have the individual data of participant, which we can aggregated into annual expense. First prepare the annual dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nannual <- finance1_wide_ptcp1 %>% \n  group_by(participantId, householdSize, haveKids, educationLevel, interestGroup, joviality) %>% \n  summarize(Education = sum(Education, na.rm = TRUE),\n            Food = sum(Food, na.rm = TRUE),\n            Recreation = sum(Recreation, na.rm = TRUE),\n            Shelter = sum(Shelter, na.rm = TRUE),\n            Wage = sum(Wage, na.rm = TRUE),\n            RentAdjustment = sum(RentAdjustment, na.rm = TRUE)) \n```\n:::\n\n\n::: callout-note\nNote that the **`na.rm = TRUE`** argument is used in the **`sum`** function to handle missing values in the columns during aggregation.\n:::\n\nDOTPLOT CODES\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n### 3.1.3 Interactive Heatmap of annual recreation spending and joviality index\n\nSECTION WILL BE REMOVED\n\nNext, prepare the data needed to generate a heatmap of annual recreational spendings by interest group and education level.\n\n\n::: {.cell}\n\n:::\n\n\nWe are ready to plot the heatmap.\n\n\n::: {.cell}\n\n:::\n\n\nFrom the plot above, we are able to see the link between different education levels and their interests group. Hovering the mouse above the heatmaps will reveal the annual recreation spending and joviality index in the tooltip. The median joviality index is chosen because it is a point where the maximum distance between the smallest and highest spending or index is reduced. What inteeresting patterns can you see?\n\nI can see that interest group J has higher recreation spending as compared to the rest and residents from the 'low' education level are contributing to it. From the joviality heatmap on the right, I also notice the same group of people are happiest of all. It makes me wonder what interest group it is in real life.\n\nI will be performing CDA to compare the annual recreation spends in joviality index across education levels later.\n\n## 3.2 CDA\n\n### 3.2.2 One-way Anova plots to compare Wage/Recreation/Joviality across Education levels\n\n#### (1) Does a person earn more if he/she is more highly educated in the city?\n\nSince Wage does not follow normal distribution, I will choose a non parametric test to compare whether there is significant difference in the median of wage between education levels.\n\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggbetweenstats(data = annual, x = educationLevel, y = Wage,\n               xlab = \"Education level\", ylab = \"Annual Wage\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Annual Wage across Education Levels\") +\n  scale_y_continuous(limits = c(0, 300000)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\nFor 4 categories of education levels, we can have a total of 4C2 = k(k-1)/2 (=6) possible combinations of pairs.\n\nFrom the results, all six pairwise comparison p-values are less than 0.05 and thus we can reject the null hypothesis and conclude that the median wages across all different educational levels are all different from one another.\n\n#### (1.1) Visualising the uncertainty of point estimates\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nannual %>%\n  ggplot(aes(x = educationLevel, y = Wage)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot, 95% and 99%\",\n    x = \"Education  level\", y = \"Wage\") +\n  \n  theme_ipsum() +\n  \n  theme(axis.title.y=element_text(angle = 0, \n                                  vjust=0.9, \n                                  size = 10, \n                                  face='bold'),\n        axis.title.x=element_text(size = 10,\n                                   face='bold'),\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank())\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nThe error bar for mean Median Wage is the longest for Graduate education level and this could be due to outliers in this category.\n\n#### (2) Is there a significant difference in the median of recreational spending across education levels?\n\nPerform a test to confirm whether recreational spending follows the normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nqq <- ggplot(annual,\n       aes(sample=Recreation)) + \n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  \n\nsw_t <- shapiro_test(annual$Recreation) %>% \n  as_tibble() %>% \n  mutate(variable = \"Recreation Spending\")%>% gt()  \n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nWe have enough statistical evidence to reject the null hypothesis that recreational spending follows normal distribution. Use a non parametric test below to test for difference in median of recreation spending across education levels.\n\n\n::: {.cell fig.asp='0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggbetweenstats(data = annual, x = educationLevel, y = Recreation,\n               xlab = \"Education level\", ylab = \"Annual Recreation\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"ns\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Annual Recreation spending across Education Levels\") +\n  scale_y_continuous(limits = c(0, 20000)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n\nFrom the results, all six pairwise comparison p-values are greater 0.05 and thus we do not have enough statistical evidence to reject the null hypothesis that the median recreation spending across all different educational levels are all different.\n\n#### (3) Is there a significant difference in median of the joviality index across education levels?\n\nPerform a test to confirm whether joviality index follows the normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nqq <- ggplot(annual,\n       aes(sample=joviality)) + \n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  \n\nsw_t <- shapiro_test(annual$joviality) %>% \n  as_tibble() %>% \n  mutate(variable = \"Joviality Index\")%>% gt()  \n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\nWe have enough statistical evidence to reject the null hypothesis that joviality index follows normal distribution. Use a non parametric test below to test for difference in median of joviality index across education levels.\n\n\n::: {.cell fig.aspect='fig-width*0.618'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggbetweenstats(data = annual, x = educationLevel, y = joviality,\n               xlab = \"Education level\", ylab = \"Joviality Index\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"ns\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Joviality index across Education Levels\") +\n  scale_y_continuous(limits = c(0, 2)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\nFrom the results, only 2 pairwise comparison p-values \\<0.05 and thus we can reject the null hypothesis for (High School & Graduate) and (High School & Bachelors) and conclude that the median Joviality index for these two pairs of education levels are different.\n\n### 3.2.3 Correlation between Annual Shelter cost and Annual Wage\n\nIn this section, we will investigate if people who earn more will also spend more on shelter. We will also subset the data across education levels.\n\nWe will use the non-parametric Spearman correlation analysis instead of Pearson correlation since the wage data is not normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlow_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Low\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Low Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n\nhigh_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"HighSchoolOrCollege\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"High Sch Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n\nbac_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Bachelors\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Degree Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\ngrad_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Graduate\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000),\n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000),\n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Graduate Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n# combining plots using patchwork\np_correl <- (low_correl + high_correl) / (bac_correl + grad_correl) + plot_spacer()\np_correl + plot_annotation(title = \"Correlation between Shelter spending and Annual Wage\", \n                           theme = theme(plot.title = element_text(size = 18),\n                                         plot.subtitle = element_text(size = 12)))\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex01_files/figure-html/unnamed-chunk-45-1.png){width=1152}\n:::\n:::\n\n\nH0: There is no \\[monotonic\\] association between the annual shelter fees and wage.\n\nH1: There is association between the annual shelter fees and wage.\n\nFrom the plots above, there are no strong correlation values above 0.7. only the 'low' and 'degree' education group showed p-values less than 0.05 which shows that there is a significant association between shelter fee and wage. However, the correlation is weak between shelter spending and wage.\n\n::: callout-note\nSpearman correlation\n\nThe Spearman correlation is not a linear correlation of the data, but a linear correlation of a transformed version of the data \\-- specifically, the correlation of the rank-transformed data. Do not be mislead by the slope direction.\n:::\n\n### \n\n# 4 References\n\nHeat Map with ggplot2. (2023). Retrieved May 9, 2023, from <https://r-charts.com/correlation/heat-map-ggplot2/#:~:text=Heat%20map%20with%20geom_tile,-A%20heap%20map&text=You%20can%20customize%20the%20border,%2C%20lwd%20and%20linetype%20%2C%20respectively.&text=In%20addition%2C%20you%20can%20add,argument%20of%20the%20aes%20function>.\n\nDownward slope but Spearman correlation coefficient is positive (and vice versa). (2022, April 5). In Stack Exchange. Retrieved May 9, 2023, from <https://stats.stackexchange.com/questions/545516/downward-slope-but-spearman-correlation-coefficient-is-positive-and-vice-versa>\n",
    "supporting": [
      "Take-home_Ex01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\r\n<script src=\"../../site_libs/plotly-binding-4.10.1/plotly.js\"></script>\r\n<script src=\"../../site_libs/typedarray-0.1/typedarray.min.js\"></script>\r\n<script src=\"../../site_libs/jquery-3.5.1/jquery.min.js\"></script>\r\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\r\n<link href=\"../../site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}