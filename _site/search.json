[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a student at MITB learning to enhance my skills in visual analytics using R and tableau.\nI welcome related discussions at yixin.neo.2022@mitb.smu.edu.sg =)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Getting Started Resources:\nMarkdown – useful for markdowns\nExecutionoptions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#introducing-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#introducing-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "1.3 Introducing ggplot",
    "text": "1.3 Introducing ggplot\nFor more details, visit ggplot2.\nSmall cheatsheet\n\n\n\nggplot2\n\n\n\n1.3.1 R Graphics VS ggplot\nPlotting a simple bar chart\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS, ylab='Number of students', xlab='score', main='Distribution of Math scores')\n\n\n\n\n\n\n\nggplot(data = exam_data,\n       aes(x=MATHS)) + \n  geom_histogram(bins=10,\n                 boundary = 100,\n                 color='black',\n                 fill='grey',size = 0.3) +\n  ggtitle('Distribution of Math Scores')\n\n\n\n\n\n\n\ntabsets — follow this guide to create a panel tabset\nWhy use ggplot2 instead of built-in plot function?\nHadley Wickham\n\n\n\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "1.4 Grammar of Graphics",
    "text": "1.4 Grammar of Graphics\nBefore we getting started using ggplot2, it is important for us to understand the principles of Grammer of Graphics.\nGrammar of Graphics is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson (1999) Grammar of Graphics, Springer. The grammar of graphics is an answer to a question:\nWhat is a statistical graphic?\nIn the nutshell, Grammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will allow us to gain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox 1978). It also provides a strong foundation for understanding a diverse range of graphics. Furthermore, it may also help guide us on what a well-formed or correct graphic looks like, but there will still be many grammatically correct but nonsensical graphics.\n\n1.4.1 A Layered Grammar of Graphics\nggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics. “A layered grammar of graphics.”\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics: Take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line. Facets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics: Statiscal transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "title": "Hands-on Exercise 1",
    "section": "1.5 Essential Grammatical Elements in ggplot2: data",
    "text": "1.5 Essential Grammatical Elements in ggplot2: data\nLet us call the ggplot() function using the code chunk on the right.\n\nggplot(data=exam_data)\n\n\n\n\n\n\n\n\n\n\n\nA blank canvas appears.\nggplot() initializes a ggplot object.\nThe data argument defines the dataset to be used for plotting.\nIf the dataset is not already a data.frame, it will be converted to one by fortify()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-aesthetic-mappings",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-aesthetic-mappings",
    "title": "Hands-on Exercise 1",
    "section": "1.6 Essential Grammatical Elements in ggplot2: Aesthetic mappings",
    "text": "1.6 Essential Grammatical Elements in ggplot2: Aesthetic mappings\nThe aesthetic mappings take attributes of the data and and use them to influence visual characteristics, such as position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function call (in later part of this lesson, you will see that each geom layer can have its own aes specification)\nCode chunk below adds the aesthetic element into the plot.\n\nggplot(data=exam_data,\n       aes(x=MATHS))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nggplot includes the x-axis and the axis’s label\n\n\n\n1.7 Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the actual marks we put on a plot.\nExamples include:\n\ngeom_point for drawing individual points (e.g., a scatter plot)\ngeom_line for drawing lines (e.g., for a line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar charts)\ngeom_histogram for drawing binned values (e.g. a histogram)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\nFor complete list, please refer to here.\n\n\n1.7.1 Geometric Objects: geom_bar\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data,\n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n1.7.2 Geometric Objects: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_dotplot(dotsize=0.5)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe y scale is not very useful, in fact it is very misleading.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below performs the following two steps:\n\nscale_y_continuous() is used to turn off the y-axis, and\nbinwidth argument is used to change the binwidth to 2.5.\n\n\n\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_dotplot(binwidth=2.5,\n               dotsize = 0.5) +\n  scale_y_continuous(NULL,\n                     breaks= NULL)\n\n\n\n\n\n\n1.7.3 Geometric Objects: geom_histogram()\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_histogram(binwidth=10,color='white')\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30.\n\n\n\n\n1.7.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with light blue color, and\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\n1.7.5 Modifying a geometric object by changing aes()\n\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\n\nggplot(data=exam_data,\n       aes(x=MATHS,\n           fill = GENDER)) +\n         geom_histogram(bins =20,\n                        color='grey40')\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric.\n\n\n\n\n1.7.6 Geometric Objects: geom-density()\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_density(color='orange')\n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes()\nI have to first remove the ‘color’ within geom_density.\n\nggplot(data=exam_data,\n       aes(x=MATHS,\n           color=GENDER)) +\n  geom_density()\n\n\n\n\n\n\n1.7.7 Geometric Objects: geom_boxplot\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data, \n       aes(y=MATHS,\n           x=GENDER)) +\n  geom_boxplot()\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n1.7.8 Geometric Objects: geom_violin\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n1.7.9 Geometric Objects: geom_point()\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\nNYX: groupby gender\nSet color by group\nColor-hex\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= ENGLISH,\n           color = GENDER)) +\n  geom_point() +\n  scale_color_manual(values = c(\"#ca7dcc\",  #manually st the color\n                                \"#1b98e0\",\n                                \"#353436\",  #not applicable\n                                \"#02e302\"))  #not applicable\n\n\n\n\n\n\n1.7.10 geom objects can be combined\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n\n  geom_point(position = 'jitter',\n             size=0.5) +\n  geom_boxplot()\n\n\n\n\n\nAbove: NYX’s mistake in the layer order\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +\n  geom_point(position = 'jitter',\n             size=0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 1",
    "section": "1.8 Essential Grammatical Elements in ggplot2: stat",
    "text": "1.8 Essential Grammatical Elements in ggplot2: stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n1.8.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data,\n       aes( y= MATHS, x = GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n1.8.2 Working with stat - the stat_summary() method\nThe code chunk below adds mean values by using stat_summary() function and overriding the default geom.\nNYX: Add the mean to the boxplot\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = 'point',\n               fun.y='mean',\n               colour = 'red',\n               size=4)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ncolour is spelled in UK ENG\n\n\n\n\n1.8.3 Working with stat - the geom() method\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=4)          \n\n\n\n\n\n\n1.8.4 Adding a best fit curve on a scatterplot?\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve.\nIn the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is loess.\n\n\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5, method = lm)\n\n\n\n\nNYX: To add Add equation and R^2 to the plot, we can use the library ggpmisc.\n\nlibrary(ggpmisc)\n\n\n\n\n\n\n\nNote\n\n\n\nAbout the error: annotate is masked from ggplot2. To use annotate from ggplot2, we can call it out explicitly.\nEg.: # Call out the ggplot2 annotate() function explicitly p + ggplot2::annotate(“text”, x = 4, y = 20, label = “More text”)\nOther references:\nhttps://stackoverflow.com/questions/39137110/what-does-the-following-object-is-masked-from-packagexxx-mean\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n  geom_point()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on Exercise 1",
    "section": "1.9 Essential Grammatical Elements in ggplot2: Facets",
    "text": "1.9 Essential Grammatical Elements in ggplot2: Facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_wrap and facet_grid().\n\n1.9.1 Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_histogram(bins=20) +\n  facet_wrap(~CLASS) +\n  labs(y='Number of students', x='Math scores')\n\n\n\n\nData manipulation, supposed we only want to visualise classes 3A, 3B, 3C, 3D. We could subset the exam data to include only these students.\n\nsub_data <- exam_data[exam_data$CLASS %in% c('3A','3B', '3C','3D'),]\nggplot(data=sub_data,\n       aes(x=MATHS)) +\n  geom_histogram(bins=20) +\n  facet_wrap(~CLASS) +\n  labs(y='Number of students', x='Math scores')\n\n\n\n\nNYX: We can also overlay multiple histograms together.\nStatisticsGlobe\nFacetted histograms with overlaid normal curves\n\nggplot(data=sub_data,\n       aes(x=MATHS, fill = GENDER)) +\n  geom_histogram(position = \"identity\", alpha=0.2, bins=20) +\n\n  labs(y='Number of students', x='Math scores')\n\n\n\n\n\n\n1.9.2 facet_grid() function\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nThe code chunk below plots a trellis plot using facet_grid().\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_histogram(bins=20) +\n  facet_grid(~CLASS) +\n  labs(y='Number of students', x='Math scores')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on Exercise 1",
    "section": "1.10 Essential Grammatical Elements in ggplot2: Coordinates",
    "text": "1.10 Essential Grammatical Elements in ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n-   [`coord_cartesian()`](https://ggplot2.tidyverse.org/reference/coord_cartesian.html): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\n-   [`coord_flip()`](https://ggplot2.tidyverse.org/reference/coord_flip.html): a cartesian system with the x and y flipped.\n-   [`coord_fixed()`](https://ggplot2.tidyverse.org/reference/coord_fixed.html): a cartesian system with a \"fixed\" aspect ratio (e.g. 1.78 for a \"widescreen\" plot).\n-   [`coord_quickmap()`](https://ggplot2.tidyverse.org/reference/coord_map.html): a coordinate system that approximates a good aspect ratio for maps.\n\n1.10.1 Working with Coordinate\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\nThe code chunk below flips the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n1.10.2 Changing the y- and x-axis range\nThe scatterplot on the right is slightly misleading because the y-aixs and x-axis range are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5) +\n  coord_cartesian (xlim=c(0,100),\n                   ylim= c(0,100))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 1",
    "section": "1.11 Essential Grammatical Elements in ggplot2: themes",
    "text": "1.11 Essential Grammatical Elements in ggplot2: themes\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\n\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\n1.11.1 Working with theme\nThe code chunk below plot a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nTo modify components of a theme , refer to this ggplot2 webpage"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#reference",
    "title": "Hands-on Exercise 1",
    "section": "1.12 Reference",
    "text": "1.12 Reference\n\nHadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis. Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version\nLearning ggplot2 on Paper – Components\nLearning ggplot2 on Paper – Layer\nLearning ggplot2 on Paper – Scale"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "In this chapter, I will be learning several ggplot2 extensions for creating more elegant and effective statistical graphics. They are\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package.\n\n\n\n\n\n\nIn this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nCode chunk below will be used to check if these packages have been installed and also will load them onto my working R environment.\n\npacman:: p_load(ggrepel, patchwork, ggthemes, hrbrthemes, tidyverse)\n\n\n\n\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data <- read_csv('data/Exam_data.csv')\n\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE.\n\n\n\n\n\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(y=ENGLISH, x=MATHS)) + \n  geom_point() +\n  geom_label(aes(label=ID))\n\n\n\n\nggrepel  is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our examples on the right.\n\nWe simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\n\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\", max.overlaps = 15) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5, \n                 color= 'black',\n                 fill='grey90') +\n  theme_gray() +\n  theme(panel.background=element_rect(fill='grey96')) +\n  ggtitle('Distribution of Math scores')\n\n\n\n\nRefer to this link to learn more about ggplot2 Themes\n\n\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5,\n                 boundary=100,\n                 color='grey25',\n                 fill='grey90',size=0.8) +\n  theme_economist() +\n  labs(y= 'No. of \\nPupils') +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  ggtitle('Distribution of Math scores')\n\n\n\n\nIt also provides some extra geoms and scales for ‘ggplot2’. Consult this vignette to learn more.\n\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5,\n                 boundary=100,\n                 color='grey25',\n                 fill='grey90') +\n  theme_ipsum() +\n  labs(y= 'No. of \\nPupils') +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  ggtitle('Distribution of Math scores')\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5,\n                 boundary=100,\n                 color='grey25',\n                 fill='grey90') +\n  theme_ipsum(axis_title_size = 15,\n              base_size=15,\n              grid= 'Y') +\n labs(y= 'No. of \\nPupils') +\n theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  ggtitle('Distribution of Math scores')\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines.\n\n\n\n\n\n\n\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, I will create composite plot by combining multiple graphs. First, create three statistical graphics by using the code chunk below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np1 <- ggplot(data=exam_data, \n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5,\n                 boundary=100,\n                 color='grey25',\n                 fill='grey90') +\n  coord_cartesian(xlim=c(0,100)) +\n  theme_ipsum(axis_title_size = 10,\n              base_size=10,\n              grid= 'Y') +\n labs(y= 'No. of \\nPupils') +\n theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9),\n       plot.title=element_text(size =10)) +\n  ggtitle('Distribution of Math scores')\n\n\n\n\nNext\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np2 <- ggplot(data=exam_data, \n       aes(x=ENGLISH)) +\n  geom_histogram(binwidth=5,\n                 boundary=100,\n                 color='grey25',\n                 fill='grey90') +\n  coord_cartesian(xlim=c(0,100)) +\n  theme_ipsum(axis_title_size = 10,\n              base_size=10,\n              grid= 'Y') +\n labs(y= 'No. of \\nPupils') +\n theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9),\n       plot.title=element_text(size =10)) +\n  ggtitle('Distribution of English scores')\n\n\n\n\nLastly, draw a scatterplot for English score versus Maths score by as shown below\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np3 <-ggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size = 0.5 )+\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  theme_ipsum(axis_title_size = 10,\n              base_size=10) +\n labs(y= 'EL score', x= 'Math score') +\n theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9),\n       plot.title=element_text(size =10)) +\n  ggtitle('English scores vesus Math scores \\nfor Primary 3')\n\n\n\n\n\n\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, I am going to shared with you an ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n\nFigure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\nOther interesting plot layouts\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + grid:: textGrob('I can place \\nsome text here.',\n                                   hjust=0, \n                                   x=-0, \n                                   gp=grid::gpar(font=3, \n                                                 fontsize = 12))\n\n\n\n\nTo learn more about, refer to Plot Assembly.\n\n\n\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) +\n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np4 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np5 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np6 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\np6 +  inset_element(p5,\n                    left = 0.02,\n                    bottom=0.7,\n                    right= 0.5,\n                    top=1)\n\n\n\n\n\n\n\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\npatchwork <- (p4/p5) | p6\npatchwork &   theme_economist() + theme(plot.title=element_text(size =10),\n                                        axis.title.y=element_text(size = 9,\n                                                                  angle = 0,\n                                                                  vjust=0.9),\n                                         axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\nPatchwork R package goes nerd viral\nggrepel\nggthemes\nhrbrthemes\nggplot tips: Arranging plots\nggplot2 Theme Elements Demonstration\nggplot2 Theme Elements Reference Sheet"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "title": "Hands-on_Ex03",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\nUse the pacman package to check, install and launch the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman:: p_load(ggiraph, plotly, patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "title": "Hands-on_Ex03",
    "section": "3.3 Importing Data",
    "text": "3.3 Importing Data\nIn this section, Exam_data.csv provided will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      <chr> \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   <chr> \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  <chr> \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    <chr> \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH <dbl> 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   <dbl> 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE <dbl> 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on_Ex03",
    "section": "3.4 Interactive Data Visualisation - ggiraph methods",
    "text": "3.4 Interactive Data Visualisation - ggiraph methods\nggiraph  is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n3.4.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np <- ggplot(data=exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(tooltip=ID),\n                           stackgroups = TRUE,\n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks= NULL)  #null to suppress axis labels\n\ngirafe(ggobj=p,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\nNYX: steps in creating an interactive graphic 1. instead of geom_point (i.e.), use geom_point_interactive - provide at least one of the aesthetics tools (tooltip, data_id or onclick) 2. call function girafe with the ggplot object to translate graphic into a web interactive graphic.\n\n\n\n\n\n\nNote\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed. To set index as the tooltip, replace with row.names(exam_data). To set numeric values as tooltip, replace with factor(MATHS).\n\n\n\n\n\n\n\n\n\n\n3.4.2 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below. We create a new column [tooltip] in exam_data by concatenating ID and Class info.\n\nexam_data$tooltip <- c(paste0(\"Name= \",\n                              exam_data$ID,\n                              \"\\n Class= \",\n                              exam_data$CLASS))\n\np <- ggplot(data=exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(tooltip=tooltip),\n                           stackgroups = TRUE,\n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks= NULL)  #null to suppress axis labels\n\ngirafe(ggobj=p,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\n\n\n\n\nInteractivity\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n\n\n\n\n\n\n\n\n3.4.3 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css <- 'background-color:palegreen; font-style:bold; color:black;'  #<<<\n\nexam_data$tooltip <- c(paste0(\"Name= \",\n                              exam_data$ID,\n                              \"\\n Class= \",\n                              exam_data$CLASS))\n\np <- ggplot(data=exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(tooltip=tooltip),\n                           stackgroups = TRUE,\n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks= NULL)  #null to suppress axis labels\n\ngirafe(ggobj=p,\n       width_svg = 8,\n       height_svg = 8*0.618,\n       options = list(          #<<<\n         opts_tooltip(          #<<<\n           css=tooltip_css\n         )\n       ))\n\nNotice that the background colour of the tooltip is palegreen and the font colour is black and bold. For demonstration purposes, we can also make the font italic and change the font size.\n\n\n\n\n\n\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\n3.4.4 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip <- function(y, ymax, accuracy = 0.01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales:: number (ymax-y, accuracy = accuracy)\n  paste(\"mean maths scores:\" , mean, \"+/-\", sem)\n}\n\n\ngg_point <- ggplot(data=exam_data,\n                   aes(x = RACE)) +\n  stat_summary(aes(y=MATHS,\n                   tooltip = after_stat(tooltip(y, ymax))),\n               fun.data = mean_se,\n               geom = GeomInteractiveCol,\n               fill = 'lightblue') +\n  stat_summary(aes(y= MATHS),\n               fun.data = mean_se,\n               geom= 'errorbar',\n               width = 0.2,\n               size = 0.2)\n\ngirafe(ggobj=gg_point,\n       width_svg = 8,\n       height_svg = 8 * 0.618)\n\n\n\n\n\nEXPLANATION of the codes above\n\nTooltip self defined function:\n\n\nIt takes in two arguments (y and ymax) from results of stat_summary() via after_stat(). accuracy is a fixed parameter and has a value of 0.01.\nscales:: number is to convert number to text, with formatting.\nsem output: If ymax = y + se , then in tooltip, sem = ymax-y, isnt sem = se?\nIt outputs \"mean maths scores: 57.44 +/- 2.03\"\n\n\nStat_summary function creates another geom layer. The first stat summary function has two aes mappings to visual: (1)MATHS as Y (2)tooltip output\n\n\nIt first applies mean_se method to output y, ymin, ymax for each X value (usually categorical)\nAfter this, these groups of 3 values are send into tooltip function via after_stat() helper function\nGeomInteractive makes the columns interactive so tooltips is displayed when users hover over\n\n\nThe second stat summary uses y,ymin, ymax obtained from method= mean_se to plot the error bar\n\nDOUCUMENTATION\nmean_se\nstat_summary(aes(fun.data=mean_se) is default. mean_se(x, mult = 1). When input a list of values, it returns a data frame with three columns:\ny: The mean.\nymin: The mean minus the multiples of the standard error.\nymax: The mean plus the multiples of the standard error.\nThere are a few summary functions from the Hmisc package which are reformatted for use in stat_summary(). They all return aesthetics for y, ymax, and ymin.\nmean_cl_normal() Returns sample mean and 95% confidence intervals assuming normality (i.e., t-distribution based)\nmean_sdl() Returns sample mean and a confidence interval based on the standard deviation times some constant\nmean_cl_boot() Uses a bootstrap method to determine a confidence interval for the sample mean without assuming normality.\nmedian_hilow() Returns the median and an upper and lower quantiles.\n\n\n3.4.5 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np <- ggplot(data= exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(data_id=CLASS),\n                           stackgroups = TRUE,\n                           binwidth= 1,\n                           method = 'histodot') +\n  scale_y_continuous( NULL,\n                      breaks = NULL)\n\ngirafe(ggobj=p,\n       width_svg = 8,\n       height_svg = 8 *0.618)\n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\n\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n3.4.6 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np <- ggplot(data=exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(data_id = CLASS),\n                           stackgroups = TRUE,\n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks= NULL)  #null to suppress axis labels\n\ngirafe(ggobj=p,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(                          #<<<\n         opts_hover(css='fill: #202020;'),      #<<<\n         opts_hover_inv(css = 'opacity: 0.2;')  #<<<\n         )\n       )\n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\n\nNote: Different from previous example (tooltip_css is pre-defined as input to a parameter in girafe(options=list(opts_tooltip(css=tooltip_css)))), in this example the ccs customisation request are encoded directly as girafe(options=list(opts_hover(css='tooltip_css'fill:#202020;')))\n\n\n3.4.7 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np <- ggplot(data=exam_data,\n            aes(x=MATHS)) +\n  geom_dotplot_interactive(aes(tooltip = CLASS,  #<<<\n                               data_id = CLASS),  #<<<\n                           stackgroups = TRUE,\n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL,\n                     breaks= NULL)  #null to suppress axis labels\n\ngirafe(ggobj=p,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(                          #<<<\n         opts_hover(css='fill: blue;'),      #<<<\n         opts_hover_inv(css = 'opacity: 0.2;')  #<<<\n         )\n       )\n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\n\n\n\n\n\n\n\n3.4.8 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\n\n\nInfo\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\n\n3.4.9 Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip= ID),      #<<< NYX added this          \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) +    #<<< p1 same as p2 x-axis\n  scale_y_continuous(NULL,            # suppress y axis\n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) +    #<<< p1 same as p2 x-axis\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2),         #<<< coordinated multiple views\n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nNYX: hover effects can be encoded directly in girafe unlike tooltips and data_id . Added tooltip aes effects as well.\n\n\n\n\n\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on_Ex03",
    "section": "3.5 Interactive Data Visualisation - plotly methods!",
    "text": "3.5 Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n3.5.1 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly()\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data=exam_data,\n        x=~MATHS,\n        y=~ENGLISH)\n\n\n\n\n\n\n3.5.2 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data=exam_data,\n        x= ~ENGLISH,\n        y= ~MATHS,\n        color=~RACE)\n\n\n\n\n\n\n3.5.3 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\np <- ggplot(data = exam_data,\n            aes(x= MATHS,\n                y= ENGLISH)) +\n  geom_point (size =1) +\n  geom_smooth(method=lm)+\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  labs(y= 'ENGLISH') +                                    \n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +   #<<< does not work in ggplotly\n  ggtitle('English and Math scores')\n\nggplotly(p)\n\n\n\n\n\n\n3.5.4 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nd <- highlight_key(exam_data)\n\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  labs(x='math',\n       y='english')\n\nsubplot(ggplotly(p1),\n        ggplotly(p2)) %>% \n  layout(xaxis = list(title = \"math\"),\n         yaxis = list(title = \"english\"))\n\n\n\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk\n\n\n\n\n\n\n\nNote\n\n\n\nDifference between 3.4.9 and 3.5.4\n3.4.9:interactive geom functions of ggiraph & girafe(patchwork) used\n3.5.4 : highlight_key() , normal ggplot + geom_obj , subplot(ggplot(p1), ggplot(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on_Ex03",
    "section": "3.6 Interactive Data Visualisation - crosstalk methods!",
    "text": "3.6 Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n3.6.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class='compact')\n\n\n\n\n\n\n\n\n3.6.2 Linked brushing: crosstalk method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,\n                  DT::datatable(d),\n                  widths=5)\n\n\n\n\nThe highlight_key() function is used to create a unique identifier for each row in a data frame, based on its values. The resulting identifier is used to keep track of the rows that have been selected or highlighted on a plot, particularly when using the highlight() function.\nplotly_selected is a built-in plotly attribute that represents the currently selected points on the plot.\nSo, highlight(ggplotly(p), “plotly_selected”) is taking the ggplotly(p) object and highlighting the currently selected points on the plot by changing their appearance in some way, such as by changing their color or size.\ncrosstalk::bscols() is a function in R that creates a Bootstrap column layout. The first argument is the left column, and the second argument is the right column. The widths argument is used to specify the relative width of the two columns, with the default value being 6 for both.\ncrosstalk::bscols() is particularly useful when working with interactive data visualizations, as it allows for easy linking of different components of the dashboard, such as brushing and highlighting on the plot and filtering on the data table."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference",
    "title": "Hands-on_Ex03",
    "section": "3.7 Reference",
    "text": "3.7 Reference\n\n3.7.1 ggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n3.7.2 plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, I will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, I will also learn how to\n(i) reshape data by using tidyr package, and\n(ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBeforewe start making animated graphs, we should first ask ourself: Does it makes sense to go through the effort? If we are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if we are giving a presentation, a few well-placed animated graphics can help an audience connect with our topic remarkably better than static counterparts.\n\n\n\n\n\n\n\n\nWe will use p_load from pacman package to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder, plotly, gganimate, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol <- c('Country', 'Continent')\n\nglobalPop <- read_xls('data/GlobalPopulation.xls',\n                      sheet='Data') %>% \n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\n\nglimpse(globalPop)\n\nRows: 6,204\nColumns: 6\n$ Country    <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\",…\n$ Year       <int> 1996, 1998, 2000, 2002, 2004, 2006, 2008, 2010, 2012, 2014,…\n$ Young      <dbl> 83.6, 84.1, 84.6, 85.1, 84.5, 84.3, 84.1, 83.7, 82.9, 82.1,…\n$ Old        <dbl> 4.5, 4.5, 4.5, 4.5, 4.5, 4.6, 4.6, 4.6, 4.6, 4.7, 4.7, 4.7,…\n$ Population <dbl> 21559.9, 22912.8, 23898.2, 25268.4, 28513.7, 31057.0, 32738…\n$ Continent  <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,…\n\n\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\n\nThis line applies the factor() function to each column specified in the col argument. Character to factor. It takes column indices or column names in strings format as inputs, and returns a data frame with new columns for each column in the input data frame, where each new column is the result of applying the specified function to the corresponding column in the input data frame.\nThe fun argument specifies the function to apply to each column, and factor(.) is a way to specify the factor works as an argument.\n\nmutate of dplyr package is used to convert data values of Year field into integer.\n\nas.character(x), as.integer(x), as.numeric(x), as.factor(x) (for categorical data)\n\n\n\n\n\n\n\n\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot\n\n\n\n\n\n\nNote\n\n\n\nA bubble plot is created when a third numeric variable is assigned to size argument inside a ggplot with geom_point.\n\n\n\nggplot(data= globalPop,\n       aes(x= Old,\n           y=Young,\n           size= Population,\n           color=Country)) +\n  geom_point(alpha = 0.7,\n             show.legend = FALSE) +\n  scale_color_manual(values=country_colors) +  #<<< who has defined country_colors?\n  scale_size(range= c(2,12)) +\n  labs(title='Year:{frame_time}',\n       x = '% Aged',\n       y= '% Young')\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\nThe scale_size(range= c(2,12)) sets the range of point sizes to be used in the plot to between 2 and 12.\nPopulation is mapped to size aes in ggplot, thus this range parameter controls the min nad max size of the points.\n\n\n\n\n\n\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(data= globalPop,\n       aes(x= Old,\n           y=Young,\n           size= Population,\n           color=Country)) +\n  geom_point( alpha = 0.7,\n             show.legend = FALSE) +\n  scale_color_manual(values=country_colors) +  #<<< gapminder lib\n  scale_size(range= c(2,12)) +\n  labs(title='Year:{frame_time}',\n       x = '% Aged',\n       y= '% Young') +\n  transition_time(Year) +\n  ease_aes(\"linear\")\n\n\n\n\n\n\n\n\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\nIn this sub-section, I will learn how to create an animated bubble plot by using ggplotly() method.\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,    #<<< perform aes mapping for each frame\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp <- globalPop %>% \n  plot_ly( x = ~Old,\n           y= ~Young,\n           color = ~Continent,#<< 6 unique\n           frame= ~Year,\n           text= ~Country,\n           hoverinfo='text',\n           type = 'scatter',\n           mode= 'markers')\n\n\nbp\n\n\n\n\n\n\n\n\n\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\n\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nparameter refers to the degree of freedom\nAn effect size of 0.77 is a standardized measure of the magnitude of a treatment or intervention effect, or the strength of an association between two variables. Guideline is that an effect size of 0.2 is considered small, 0.5 is considered moderate, and 0.8 is considered large.\nCI of 95% means if we replicate our sampling from underlying distribution many times, 95% of our samples will have their means within this interval.\n\n\n\n\n\n\n\n\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse,nortest, ggdist)\n\n\n\n\nLets import the Exam_data.csv using the read_xls() function.\n\nexam <- read_csv('data/Exam_data.csv')\n\nTake a glimpse at the data.\n\nexam\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   <chr>      <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\nglimpse(exam)\n\nRows: 322\nColumns: 7\n$ ID      <chr> \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   <chr> \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  <chr> \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    <chr> \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH <dbl> 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   <dbl> 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE <dbl> 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nA one-sample test is a statistical hypothesis test used to determine whether the mean of a single sample of data differs significantly from a known or hypothesized value.\nIt is a statistical test that compares the mean of a sample to a specified value, such as a population mean, to see if there is enough evidence to reject the null hypothesis that the sample comes from a population with the specified mean.\n\nH0: EL average score is 60.\n\nset.seed(1234)  #<<< important to set if we use bayes statistics\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='bayes',  #<< '\n             test.value =60,\n             xlab = 'English scores')\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\nReference website from r-bloggers\nThe one-sample Wilcoxon test (non parametric) will tell us whether the scores are significantly different from 60 or not (and thus whether they are different from 60 in the population or not)\nH0: EL scores = 60\nH1: EL scores != 60\nThe scores are assumed to be independent (a student’s score is not impacted or influenced by the score of another student)\n\nwilcox.test(exam$ENGLISH,\n            mu = 60)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  exam$ENGLISH\nV = 38743, p-value = 3.435e-16\nalternative hypothesis: true location is not equal to 60\n\n\nInterpretation\nP-value<0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the EL scores are significantly different from 60.\n\n\n\n\n\n\nNote\n\n\n\nBy default, it is a two-tailed test that is done. As for the t.test() function, we can specify that a one-sided test is required by using either the alternative = “greater” or alternative = “less argument in the wilcox.test() function.\n\n\nCombine statistical test and plot\n\nset.seed(1234)\n\ngghistostats(data=exam,\n             x = ENGLISH,\n             type='nonparametric', #nonparametric (median) = Wilcoxon, parametric = t-test (default is look for mean and unequal variance method)\n             test.value =60,\n             conf.level = 0.95,\n             xlab = 'English scores')\n\n\n\n\nDid we forget to check if English scores follow a normal distribution? Use ad.test from nortest library.\nH0: EL scores follows normal distribution\nH1: EL scores do not follow normal distribution.\n\nad.test(exam$ENGLISH)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$ENGLISH\nA = 4.3661, p-value = 7.341e-11\n\n\nResults from the Anderson_darling normality test shows enough statistical evidence to reject the null hypothesis and conclude that the EL scores do not follow normal distribution . Thus the use of non parametric test is correct.\n\n\n\n\n\n\nOn Parametric and Non-parametric types\n\n\n\ntype= parametric: default look for mean and assumes unequal variance method\ntype = Non parametric: student-t test and use median (not mean!!)\n\n\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender (independent).\nH0: Mean of F and M Math scores are the same.\nH1: Mean of F and M Math scores are not the same.\n\nggbetweenstats(data=exam,\n               x=GENDER,\n               y=MATHS,\n               type='np',\n               messages=FALSE)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\nSince p-value > 0.05, we do not have enough statistical evidence to reject the null hypothesis that mean of Math scores of both gender are the same.\nHowever, if we check for normality of Math scores of each gender.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test <- by(exam$MATHS, exam$GENDER, shapiro.test)\n\n# extract p-values\np_values <- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n      Female         Male \n1.603536e-07 6.268520e-08 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe by() function is used to apply a function to subsets of a data frame or vector split by one or more factors. In the above code, we use by() to split the math_score column by gender, and apply the shapiro.test() function to each group.\n\n\nH0: Math scores by gender follows normal distribution.\nH1: Math scores by gender do not follow normal distribution.\nFrom the Shapiro-Wilk test results, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores by gender does not follow a normal distribution. Thus the use of ‘np’ is appropriate.\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race (Independent 4 sample mean).\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci=TRUE,\n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",  # 'ns': shows only non-sig, 's': shows only sig, 'all': both \n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n## might need to call library(PMCMRplus) and library(rstantools) if this code chunck doesnt work.\n\nSince p-value < 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that NOT ALL means of EL scores by race are the same. The results shows that the means of EL scores of Chinese, Indian and Malay are significantly different.\nOnce again, lets go backwards and confirm that the distribution of EL scores by RACE conforms to normal distribution.\n\n# perform Shapiro-Wilk test on math scores by gender\nshapiro_test <- by(exam$ENGLISH, exam$RACE, shapiro.test)\n\n# extract p-values\np_values <- sapply(shapiro_test, function(x) x$p.value)\n# print results\nprint(p_values)\n\n     Chinese       Indian        Malay       Others \n1.305153e-07 8.482600e-01 1.251020e-02 5.181740e-01 \n\n\nH0: EL scores by Race follow normal distribution. H1: EL scores by Race do not follow normal distribution.\nThe results of the Shapiro-wilk test shows p_value of all EL score distribution by race follows normal distribution.\n\n\n\ntype argument entered by us will determine the centrality tendency measure displayed\n\n\nmean for parametric statistics\nmedian for non-parametric statistics\ntrimmed mean for robust statistics\nMAP estimator for Bayesian statistics\n\n\n\n\n\n\n\n\nEarlier, we have checked that EL scores do not follow a normal distribution. Now we will do the same for Math scores.\n\nad.test(exam$MATHS)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$MATHS\nA = 7.9125, p-value < 2.2e-16\n\n\nSince the p-value < 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that the Math scores also do not follow normal distribution.\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  type='nonparametric', # 'parametric', 'robust', 'bayes'\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have chosen a non parametric version of this test as both Math and EL scores do not follow normal distribution.\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\nWe will create a new dataframe exam1 similar to exam df but with extra column called ‘MATHS_bins’.\n\nexam1 <- exam %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nexam1\n\n# A tibble: 322 × 8\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE MATHS_bins\n   <chr>      <chr> <chr>  <chr>     <dbl> <dbl>   <dbl> <fct>     \n 1 Student321 3I    Male   Malay        21     9      15 (0,60]    \n 2 Student305 3I    Female Malay        24    22      16 (0,60]    \n 3 Student289 3H    Male   Chinese      26    16      16 (0,60]    \n 4 Student227 3F    Male   Chinese      27    77      31 (75,85]   \n 5 Student318 3I    Male   Malay        27    11      25 (0,60]    \n 6 Student306 3I    Female Malay        31    16      16 (0,60]    \n 7 Student313 3I    Male   Chinese      31    21      25 (0,60]    \n 8 Student316 3I    Male   Malay        31    18      27 (0,60]    \n 9 Student312 3I    Male   Malay        33    19      15 (0,60]    \n10 Student297 3H    Male   Indian       34    49      37 (0,60]    \n# ℹ 312 more rows\n\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association.\n(Two categorical variables) H0: There is no association between mathbin and gender.\nH1: There is an association between mathbin and gender.\n\nggbarstats(exam1,\n            x=MATHS_bins,\n            y=GENDER)\n\n\n\n\nFrom the results above , p-value > 0.05 thus we have not enough statistical evidence to reject the null hypothesis that there is not association between the mathbin and gender variables.\n\n\n\n\nIn this section, I will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\n\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale <- read_xls('data/ToyotaCorolla.xls',\n                       sheet='data')\n\n\nglimpse(car_resale)\n\nRows: 1,436\nColumns: 38\n$ Id               <dbl> 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            <chr> \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            <dbl> 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        <dbl> 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        <dbl> 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         <dbl> 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               <dbl> 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    <dbl> 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           <dbl> 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           <chr> \"100-120\", \"< 100\", \"< 100\", \"< 100\", \"< 100\", \"< 100…\n$ CC_bin           <chr> \"1600\", \">1600\", \">1600\", \">1600\", \">1600\", \">1600\", …\n$ Doors            <dbl> 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        <chr> \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            <chr> \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        <dbl> 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    <dbl> 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            <dbl> 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     <dbl> 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  <dbl> 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period,\n            data=car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\n\n\n\nWe can see high collinearity between Age and Mfg_Year. One is derived from the other. We should remove one of them and repeat muliti collinearity check again for the new model.\n\n\n\nIn the code chunk, check_normality() of performance package.\nNotice that the Mfg_Year variable has been removed from the independent variables list.\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_c1 <- check_collinearity(model1)\nplot(check_c1)\n\n\n\n\n\ncheck_n <- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\nRecap: Assumptions of linear regression\n\n\n\nIn linear regression, one of the key assumptions is that the residuals (the differences between the predicted values and the actual values) are normally distributed. The normality assumption is important because it affects the validity of statistical inference procedures such as hypothesis testing and confidence intervals.\nIf the residuals are not normally distributed, it may indicate that the linear regression model is not a good fit for the data and that alternative modeling approaches may be needed.\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\nHeteroscedasticity refers to a situation where the variance of the errors (or residuals) in the linear regression model is not constant across different levels of the predictor variable(s).\nIf heteroscedasticity is detected, there are several ways to address it, including transforming the data, using weighted least squares regression, or using robust standard errors. In DAl, we rebuild another model by creating subclasses out of the original Y variable.\n\ncheck_h <- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\nFrom the graph above, there is a slight sign of heteroscedasticity as the residuals seem to be funnelled outwards as the fitted values increase.\n\n\n\nWe can also perform the complete check by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "A point estimate is a single number, such as a mean.\n\nUncertainty is expressed as standard error, confidence interval, or credible interval\n\n\n\n\n\n\nImportant\n\n\n\nDon’t confuse the uncertainty of a point estimate (mean, median..) with the variation in the sample (standard deviation sigma, and variation sigma square etc).\nThe standard deviation measures the variation of the values from the mean of ONE sample.\nThe standard error is the standard deviation of both sides of the ‘mother of all means’ of all the sample means. <- refer lecture 4 slide 23\n\n\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate, ggiraph)\n\n\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualizing distributions and uncertainty.\nLets load the student exam data.csv\n\n\nexam <- read_csv('C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex05/data/Exam_data.csv')\n\n\n\nThe code chunk below performs the followings:\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\nmy_sum <- exam %>% \n  group_by(RACE) %>% \n  summarise(n=n(),\n            mean=mean(MATHS),\n            sd = sd(MATHS)) %>% \n  mutate(se=sd/sqrt(n-1))   #<<< standard error formula\n\n\nRefer to lecture 4 slide 20 for mathematical formula explanation.\nmy_sum is specially created for visualisation later using ggplot2.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\n\nThe code chunk below is used to reveal the standard error of mean maths score by race. It shows one standard deviation away from the ‘mother of all means’ for all the means from all the samples.\n\n\n\n\n\n\nNote\n\n\n\nStandard error is a measure of the variation of the mean of all the means from all samples of an underlying distribution.\n\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(aes(x=RACE,\n                    ymin=mean-se,\n                    ymax=mean+se),\n                width = 0.2,\n                colour = 'black',\n                alpha = 0.9,\n                size=0.5) +\n    geom_point(aes(x=RACE,\n                 y=mean),\n             stat = 'identity', #<<< actual points refer to mean \n             color='red', \n             size = 1.5,\n             alpha = 1) + \n  ggtitle('Standard error of mean maths score by race')\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the code above, stat = 'identity' means that the y values in the geom_point layer correspond to the actual values in the data frame, rather than a summary statistic like mean or median.\n\n\n\n\n\nLets plot a 95% confidence interval of mean maths score by race. The error bars should be sorted by the average maths scores. (Refer to take-home ex 1 on sorting by mean)\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(aes(x=reorder(RACE,-mean),  # reorder(x,y) means to reorder x based on increasing or decreasing values of y. To sort by descending values of Y, use -Y.\n                    ymin=mean-1.96*se,    #<<<< formula to calc 95% CI\n                    ymax=mean+ 1.96*se),  #<<<<\n                width = 0.2,\n                colour = 'black',\n                alpha = 0.9,\n                size=0.5) +\n    geom_point(aes(x=RACE,\n                 y=mean),\n             stat = 'identity', #<<< actual points refer to mean \n             color='red', \n             size = 1.5,\n             alpha = 1) + \n  ggtitle('95% confidence interval of maths score by race')\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat is the difference between standard error plot above and this 95% confidence interval plot?\nEarlier , we plot error bars of 1 standard deviation away from the mother of all means.\nHere, we are plotting 1.96 * standard deviation away from the mother of all means. The higher the % CI, the greater the margin or error.\n\n\n\n\n\nTask: Plot an interactive error bars for the 99% confidence interval of mean maths score by race., and add the source data table on the right. Table and plot have coordinated views.\nRecall in hands-on 3 that we create can create interactive plots using ggiraph and ggplot geometries that can understand 3 arguments; namely tooltip, data_id and onclick.\nI will use one of them methods (geom_errorbar_interactive with girafe) to construct here. The other method is to use plotly.\nSTEP 1: CREATE INTERACTIVE ERROR BAR for 99% CI\n\n\nCode\n# create a new column tooltip to contain the tooltip text.\nmy_sum$tooltip <- c(paste0(\"RACE: \",\n                           my_sum$RACE,\n                           \"\\n N= \",\n                            my_sum$n,\n                           \"\\n Ave Score: \",\n                           round(my_sum$mean, digits=2)\n                           ))\n\np <- ggplot(my_sum) +\n  geom_errorbar_interactive(aes(x=reorder(RACE,-mean),\n                    ymin=mean-2.58*se,    #<<<< formula to calc 95% CI\n                    ymax=mean+ 2.58*se,\n                    tooltip=tooltip),  \n                width = 0.2,\n                colour = 'black',\n                alpha = 0.9,\n                size=0.5) +\n    geom_point(aes(x=RACE,\n                 y=mean),\n             stat = 'identity', #<<< actual points refer to mean \n             color='red', \n             size = 1.5,\n             alpha = 1) + \n  labs(title='99% confidence interval of maths score by race',\n       x = 'Race') +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_text(face = \"bold\"),\n        axis.line = element_line(size = 0.2))\n\ngirafe(ggobj = p,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\nSTEP 2: CREATE AN INTERACTIVE DATA TABLE USING DT()\n\nWe use the %>% operator to pipe the data frame into the mutate_if() function, where we specify the condition is.numeric to select only the numeric columns. We use the tilde ~ symbol to specify the rounding function, and pass the digits argument to round to two decimal places. The resulting data frame df will have all numerical columns rounded to two decimal places.\n\n\n\nCode\n# Round all numerical columns to two decimal places\nmy_sum <- my_sum %>% \n  mutate_if(is.numeric, ~ round(., digits = 2))\n\nDT::datatable(my_sum, class='compact')\n\n\n\n\n\n\n\nSTEP 3: COMBINE BOTH\nNext, combine uncertainty graph and plotly interactive table using Crosstalk, which is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n(Refer to hands-on 3)\nThe datatable is linked to the visualisation on the left. Click on multiple rows to filter according.\n\n\nCode\n# Use highlight_key() to add a unique key to the data frame my_sum3 so that it can be linked to interactive plots later\nd <- highlight_key(my_sum) \n\n\np <- ggplot(d) +\n  geom_errorbar(aes(x=reorder(RACE,-mean),\n                    ymin=mean-2.58*se,    #<<<< formula to calc 95% CI\n                    ymax=mean+ 2.58*se),  \n                width = 0.2,\n                colour = 'black',\n                alpha = 0.9,\n                size=0.5) +\n    geom_point(aes(x=RACE,\n                 y=mean),\n             stat = 'identity', #<<< actual points refer to mean \n             color='red', \n             size = 1.5,\n             alpha = 1) + \n  labs(title='99% confidence interval of maths score by race',\n       x = 'Race') +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_text(face = \"bold\"),\n        axis.line = element_line(size = 0.2))\n\n# Convert ggplot to an interactive plotly plot using the ggplotly(), \"plotly click\" specifies that highlight should be based on click\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(list(width = 7,gg),\n                  list(width=5,DT::datatable(d)),\n                  widths=5)\n\n\n\n\n\n\n7\n\n\n\n\n5\n\n\n\n\n\n\n\nMy mistakes on the attempt to produce this plot:\n\nDid not use d in ggplot(d) and datatable(d) in crosstalk. As a result, the plot and table were both not able to communicate with each other.\nWrongly added geom_errorbar_interactive(aes(tooltip) and girafe together with plotly and crosstalk::bscols. As as result, no error bars were seen.\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\nstat_pointinterval means points and multiple intervals. The default confidence interval us 95%. To change the level to 99%, add conf.level = 0.99 to stat_pointinterval function.\nTake note that the default .width values are set to c(0.66, 0.95) confidence intervals.\n\n\nCode\nexam %>% \n  ggplot(aes(x=RACE,       #<< plot the base layer\n             y=MATHS)) +\n  stat_pointinterval() +   #<< .width=c(0.66,0.95)\n  labs(\n    title='Visualising confidence intervals of mean math score',\n    subtitle = \"Mean Point + Multiple-interval plot 66% and 95%\")\n\n\n\n\n\nSome of the arguments (there are many , have to read the syntax reference for more details)\n\n.width: For intervals, the interval width as a numeric value in [0, 1]. For slabs, the width of the smallest interval containing that value of the slab.\npoint_interval: This function determines the point summary (typically mean, median, or mode) and interval type (quantile interval, qi; highest-density interval, hdi; or highest-density continuous interval, hdci)\n\n\nCode\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot, 95% only\")\n\n\n\n\n\n\n\n\n\nTask: Makeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\n\nCode\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot, 95% and 99%\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nstat(level) calculates the confidence interval limits based on the specified conf.level argument, and interval_color maps the calculated interval color to the interval_color argument.\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\n\nCode\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"green\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\nStep 1: Installing ungeviz package\n\n#devtools::install_github(\"wilkelab/ungeviz\")\n\n\nlibrary(ungeviz)\n\nWhat are HOPs? Rather than showing a continuous probability distribution, Hypothetical Outcome Plots (or HOPs) visualize a set of draws from a distribution, where each draw is shown as a new plot in either a small multiples or animated form.\nExplanation of the code below:\nThe code is creating a ggplot object to visualize the distribution of MATHS scores for different races in the exam dataset using the geom_point() and geom_hpline() functions from ggplot2.\nSpecifically, it is creating a scatterplot (geom_point()) of MATHS scores against RACE with some jitter (position_jitter()) added to the points to avoid overplotting. The factor() function is used to convert the RACE variable to a categorical variable.\nAdditionally, a horizontal line (geom_hpline()) is added to the plot to represent the median MATHS score for each race, calculated using sampler() function with 25 samples drawn from the original dataset for each race. The line is colored in #D55E00.\nThe transition_states() function is used to create an animation by specifying the .draw column as the states for the animation. The animation has 3 states (1, 2, 3) and will animate the plot with a transition between each state.\nFinally, the theme_bw() function is used to set the theme of the plot to a black and white color scheme.\n\n\nCode\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\nA jitter plot is a variant of the strip plot with a better view of overlapping data points, used to visualise the distribution of many individual one-dimensional values. The values are plotted as dots along one axis, and the dots are then shifted randomly along the other axis, which has no meaning in itself data-wise, allowing the dots not to overlap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on_Ex07",
    "section": "11.1 Overview",
    "text": "11.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#installing-and-launching-r-packages",
    "title": "Hands-on_Ex07",
    "section": "11.2 Installing and Launching R Packages",
    "text": "11.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "title": "Hands-on_Ex07",
    "section": "11.3 Importing Data",
    "text": "11.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\nmutate_if will convert columns in chr format as factor.\n\n\ncovid19 <- read_csv('C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex05/data/COVID-19_DKI_Jakarta.csv') %>% \n  mutate_if(is.character, as.factor)\n\n\ncovid19\n\n# A tibble: 267 × 7\n   `Sub-district ID` City       District `Sub-district` Positive Recovered Death\n               <dbl> <fct>      <fct>    <fct>             <dbl>     <dbl> <dbl>\n 1        3172051003 JAKARTA U… PADEMAN… ANCOL              1776      1691    26\n 2        3173041007 JAKARTA B… TAMBORA  ANGKE              1783      1720    29\n 3        3175041005 JAKARTA T… KRAMAT … BALE KAMBANG       2049      1964    31\n 4        3175031003 JAKARTA T… JATINEG… BALI MESTER         827       797    13\n 5        3175101006 JAKARTA T… CIPAYUNG BAMBU APUS         2866      2792    27\n 6        3174031002 JAKARTA S… MAMPANG… BANGKA             1828      1757    26\n 7        3175051002 JAKARTA T… PASAR R… BARU               2541      2433    37\n 8        3175041004 JAKARTA T… KRAMAT … BATU AMPAR         3608      3445    68\n 9        3171071002 JAKARTA P… TANAH A… BENDUNGAN HIL…     2012      1937    38\n10        3175031002 JAKARTA T… JATINEG… BIDARA CINA        2900      2773    52\n# ℹ 257 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#funnelplotr-methods",
    "title": "Hands-on_Ex07",
    "section": "11.4 FunnelPlotR methods",
    "text": "11.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n11.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR” (stands for standardised Ratio)\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n11.4.2 FunnelPlotR methods: Makeover 1\nThe changes made:\n\ndata_type changed to ‘PR’, which stands for proportions of deaths/positive cases. (derieved using numerator and denominator)\nRanges of x and y axes to suit the visualisation\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<  proportions\n  xrange = c(0, 6500),  #<<\n  yrange = c(0, 0.05)   #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n11.4.3 FunnelPlotR methods: Makeover 2\nThe changes made:\n\nlabel = NA to remove the default outliers feature\nEdited the x and y axis titles to understand the chart better.\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative \\nTotal Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #<<\n  y_label = \"Cumulative Fatality Rate\"  #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on_Ex07",
    "section": "11.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "11.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, Iwill gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance my working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n11.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate (rate) and standard error of cumulative death rate (rate.se). Take note that the formula for SE of Proportions will be used here. (Lecture 4 slide 25)\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\nNext, the fit.mean is computed by using the code chunk below.\nThe function calculates the weighted mean of the rate column in the df data frame, where the weights are the inverse squares of the corresponding standard errors (rate.se).\n\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\nfit.mean\n\n[1] 0.01496959\n\n\n\n\n11.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\nThe number.seq creates a sequence of numbers from 1 to the maximum number of positive cases in the data frame. (max = 6231)\nWe then calculate the lower and upper 95% confidence intervals and the lower and upper 99.9% confidence intervals for the mean rate of death at each number in the sequence.\nFinally, a new data frame dfCI is created that contains the lower and upper confidence intervals and mean rate of death for each number in the sequence.\n\n\nCode\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \n\n# creates a new dataframe using data.frame()\ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n11.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\n\nCode\np <- ggplot(df, aes(x = Positive, y = rate)) +  #<<< death rates vs positive case\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  \n  # 95% line is dashed\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  \n  # 99% line is solid\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n11.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\n\nCode\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "Overview",
    "text": "Overview\nIn this hands-on exercise, I will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, I will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.2 Getting Started",
    "text": "27.2 Getting Started\n\n27.2.1 Installing and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\n\nShow the code\npacman::p_load(igraph, tidygraph, ggraph,\n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts,knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.3 The Data",
    "text": "27.3 The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n27.3.1 The edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n27.3.2 The nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n27.3.3 Importing network data from files\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\n\nShow the code\nGAStech_nodes <- read_csv('data/GAStech_email_node.csv')\nGAStech_edges <- read_csv('data/GAStech_email_edge-v2.csv')\n\n\n\n\n27.3.4 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow the code\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nShow the code\n# list()\n# summary()\n# class ()\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. We have to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n27.3.5 Wrangling time\n\n\nShow the code\nGAStech_edges <- GAStech_edges %>% \n  mutate(SentDate = dmy(SentDate)) %>% \n  mutate(Weekday = wday(SentDate,\n                         label = TRUE,  # ordered factor if true\n                         abbr = FALSE))\n\n\nCodes to check the number of unique Weekdays\n\n\nShow the code\nunique(GAStech_edges %>% pull(Weekday))\n\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \n7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\n\n\n\n\n\n\n\n\nLearning from codes above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number (1-7) or an ordered factor (Monday, Tuesday,..) if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n27.3.6 Reviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges with the correct data formats.\n\n\nRows: 9,063\nColumns: 9\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ Weekday     <ord> Monday, Monday, Monday, Monday, Monday, Monday, Monday, Mo…\n\n\n\n\n27.3.7 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will\n\nfilter Work related emails\ngroup-by senders, receivers and day of week\naggregate to get the total count of each unique combination of the above to get Weight as a new column\nfilter twice to remove self-loops and edges that occurred only once\nungroup() function is used to remove the grouping created by group_by() so that the resulting dataframe is not grouped by any variable(s) anymore. This is useful when we want to apply further operations or analysis to the individual rows of data rather than grouped results.\n\nThe code chunk:\n\n\nShow the code\nGAStech_edges_aggregated <- GAStech_edges %>%  # after filter 6935 rows remain\n  filter(MainSubject=='Work related') %>% \n  group_by(source, target, Weekday) %>% \n  summarise(Weight = n()) %>%           # 3706 rows remaining and Weight col added\n  filter(source != target) %>%          #3493 rows remaining\n  filter(Weight >1) %>%                # 1456 rows remaining\n  ungroup()                            # 1456 x 4 columns \n\n\n\n\n\n\n\n\nThings to learn from code above\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\nAfter ungroup(), we can analyse row by row instead of by unique combination of source, target and weekday\n\n\n\n\n\n27.3.8 Reviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges_aggregated data frame\n\n\nRows: 1,456\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ Weekday <ord> Monday, Tuesday, Wednesday, Friday, Monday, Tuesday, Wednesday…\n$ Weight  <int> 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.4 Creating network objects using tidygraph",
    "text": "27.4 Creating network objects using tidygraph\nIn this section, I will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way\n\nto switch between the two tables and provides dplyr verbs for manipulating them.\nto access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\nBefore getting started, please read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n27.4.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n27.4.2 The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n%>% mutate(Species = ifelse(leaf, as.character(iris$Species)[label], NA)) - This line adds a new column called Species to the nodes data frame. The ifelse() function assigns a value to this column based on whether the node is a leaf or not. If it is a leaf, the value is taken from the label column of the iris$Species data frame (which contains the actual species names), and if it is not a leaf, the value is set to NA.\n%>% mutate(to_setose = .N()$Species[to] == 'setosa') - This line adds a new column called to_setose to the edges data frame. The ifelse() function assigns a value to this column based on whether the target node of each edge is a member of the setosa species or not. The to variable refers to the index of the target node in the nodes data frame, and the .N() function allows access to the Species column of the nodes data frame.\n\n\n\n27.4.3 Using tbl_graph() to build tidygraph data model.\nIn this section, I will use tbl_graph() of tidygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, review to reference guide of tbl_graph()\n\n\n\n\n\n\nRequired format of nodes and edges data\n\n\n\nGAStech_nodes has ID of nodes as first column. Label is optional?\nGAStech_edges_aggregated contains source and target as column 1 and 2.\n\n\n\n\nShow the code\nGAStech_graph<- tbl_graph(nodes=GAStech_nodes,\n                          edges = GAStech_edges_aggregated,\n                          directed = TRUE)\n\n\n\n\n27.4.4 Reviewing the output tidygraph’s graph object\n\n\nShow the code\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,456 × 4\n   from    to Weekday   Weight\n  <int> <int> <ord>      <int>\n1     1     2 Monday         4\n2     1     2 Tuesday        3\n3     1     2 Wednesday      5\n# ℹ 1,453 more rows\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active (Node data is on top of Edge data). The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n27.4.6 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\n\nShow the code\nGAStech_graph %>% \n  activate(edges) %>% \n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,456 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1    40    41 Tuesday     23\n2    40    43 Tuesday     19\n3    41    43 Tuesday     15\n4    41    40 Tuesday     14\n5    42    41 Tuesday     13\n6    42    40 Tuesday     12\n# ℹ 1,450 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.5 Plotting Static Network Graphs with ggraph package",
    "text": "27.5 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n27.5.1 Plotting a basic network graph\nThe code chunk below uses\n\nggraph(),\ngeom-edge_link() and\ngeom_node_point() to plot a network graph by using GAStech_graph.\n\nBefore getting started, it is advisable to read their respective reference guide at least once.\n\n\nShow the code\nggraph(GAStech_graph) +  #<<< GAStech_graph is a tbl_graph object\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from code chunk above\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() canaccept either an igraph object or a tbl_graph object.\nigraph uses an adjacency matrix or an edge list and is more focused on traditional graph theory algorithms and operations.\nIn an edge list, each row represents an edge, with the first two columns containing the indices of the nodes that the edge connects.\ntidygraph uses a tbl_graph object, which is a tidy data frame representation of a graph. The nodes and edges data frames contain the metadata about the nodes and edges, respectively, and can be manipulated using the dplyr syntax\n\n\n\n\n\n27.5.2 Changing the default network graph theme\nIn this section, use theme_graph() to remove the x and y axes. Before getting started, it is advisable to read it’s reference guide at least once.\n\n\nShow the code\ng <- ggraph(GAStech_graph) +  #<<< GAStech_graph is a tbl_graph object\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn form codes above\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nShow the code\nclass(g)\n\n\n[1] \"ggraph\" \"gg\"     \"ggplot\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nObject g is a tbl graph object and is a dataframe with nodes, edges, and plot characteristics information. This dataframe changes everytime I overwrite g\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph) +  #<<< GAStech_graph is a tbl_graph object\n  geom_edge_link(aes(colour ='grey50'),show.legend = FALSE) +  #<< refer to ggraph documentation\n  geom_node_point(aes(colour ='grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'orange')\n\n\n\n\n\n\n\n27.5.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \n\n\n27.5.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\nTo change the layout of ggraphs, refer to ggraph_layout\n\n\nShow the code\ng <- ggraph(GAStech_graph, layout='fr') +  #<<< refer to ggraph_layout link above\n  geom_edge_link(aes()) +  #<< refer to ggraph documentation\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the codes above\n\n\n\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n\n27.5.6 Modifying network nodes\nIn this section, I will colour each node by referring to their respective departments.\n\n\nShow the code\ng <- ggraph(GAStech_graph, layout='nicely') +  #<<< refer to ggraph_layout link above\n  geom_edge_link(aes()) +  #<< refer to ggraph documentation\n  geom_node_point(aes(colour=Department, size =3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n\n27.5.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable. geom_edge_link search individually\n\n\nShow the code\ng <- ggraph(GAStech_graph, layout='nicely') +  #<<< refer to ggraph_layout link above\n  geom_edge_link(aes(width=Weight, alpha= 0.2)) +  #<< thickness by weight and change alpha\n  scale_edge_width(range = c(0.1, 5)) +  #<< control max size of edge , else my plot is ugly\n  geom_node_point(aes(colour=Department, size =3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from codes above\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\nThe code chuck below assign colour to the nodes manually without hard-coding. I have also change the background and text colour.\n\nactivate the nodes df and extract the unique department using the pull() function\nuse length() function to find the nunique departments\nbrewer.pal() function generates a set of colors based on the number of unique departments\nsetNames() function is used to map the colors to the departments\nscale_color_manual() function is used to apply the color mapping to the Department nodes.\n\n\n\nShow the code\nlibrary(RColorBrewer)\n\n# Get unique departments from data\ndepartments <- unique(GAStech_graph %>% activate(nodes) %>% pull(Department))\n\n# Generate color palette based on number of unique departments\nnum_departments <- length(departments)\ncolor_palette <- brewer.pal(num_departments, \"Set3\")\n\n# Create color mapping for Department nodes\ncolor_mapping <- setNames(color_palette, departments)\n\n# Create plot with color mapping\nj <- ggraph(GAStech_graph, layout='fr') + \n     geom_edge_link(aes(alpha=0.1, colour='white'),show.legend = FALSE) +\n     geom_node_point(aes(colour=Department), size = 3) +\n     scale_color_manual(values = color_mapping) +\n     theme_graph(background = 'grey10',text_colour = 'orange')\nj\n\n\n\n\n\nIf choose to hard code, refer to the code chunk below. I have tried ‘star’ layout , wonder if the red adminstrative node in the middle is related to high centrality?\n\n\nShow the code\nggraph(GAStech_graph, layout='star') + \n     geom_edge_link(aes()) +\n     geom_node_point(aes(colour=Department), size =4) +\n     scale_color_manual(values = c(\"Administration\" = \"red\", \n                                   \"Engineering\" = \"blue\", \n                                   \"Executive\" = \"green\", \n                                   \"Facilities\" = \"purple\", \n                                   \"Information Technology\" = \"yellow\", \n                                   \"Security\" = \"pink\")) +\n     theme_graph()\n\n\n\n\n\nI have tried to color code the edges by Weekday. However this graph is hard to interpret due to overplotting. We should try to facet by Weekday instead.\n\n\nShow the code\ng <- ggraph(GAStech_graph) +  #<<< GAStech_graph is a tbl_graph object\n  geom_edge_link(aes(alpha=0.2, colour=Weekday)) +\n  geom_node_point()\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.6 Creating facet graphs",
    "text": "27.6 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_edges() whereby nodes are always drawn in a panel even if the node data contains an attribute named the same as the one used for the edge facetting,\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n27.6.1 Working with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once. Also can refer to ggraph().\n\n\nShow the code\nset_graph_style()   #<< using this command provide plot settings for next few plots\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\nset_graph_style() defaults\n\n\n\nRefer to thomas85github_theme_graph\n\nset_graph_style\n\n(family = ‘Arial Narrow’, face = ‘plain’, size = 11,\ntext_size = 11, text_colour = ‘black’, …)\n\nunset_graph_style() – to reset the graph style to default\n\n\n\n\n\n27.6.2 Working with facet_edges(): change legend position\nThe code chunk below uses theme() to change the position of the legend.\n\n\nShow the code\nset_graph_style()\n\ng<- ggraph(GAStech_graph,\n            layout='nicely') +\n  geom_edge_link(aes(width=Weight),\n                     alpha=0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour=Department),\n                  size =2) +\n  theme(legend.position = 'bottom')\n                   \n                   \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n27.6.3 A framed facet graph\nThe code chunk below adds frame to each graph.\n\n\nShow the code\nset_graph_style()\n\ng<- ggraph(GAStech_graph,\n           layout='nicely') +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour=Department),\n                  size = 2)\n\ng + facet_edges(~Weekday) +\n  th_foreground(foreground = 'steelblue',\n                fg_text_colour = 'white',\n                border = TRUE) +\n  \n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n27.6.4 Working with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\nset_graph_style()\n\ng<- ggraph(GAStech_graph,\n           layout='nicely') +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour=Department),\n                  size = 2)\n\ng + facet_nodes(~Department) +\n  th_foreground(foreground = 'steelblue',\n                fg_text_colour = 'white',\n                border = TRUE) +\n  \n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.7 Network Metrics Analysis",
    "text": "27.7 Network Metrics Analysis\n\n27.7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\nTidygraph documentation : so far we used tbl_graph() to create a tbl_graph network object / dataframe called GAStech_graph. Then we practise activate() to swtich between nodes and edges tibbles. This network obj is passed into ggraph() to plot the charts.\nNow we are using tidy graph to perform centrality calculations.\nggraph documentation : node, edge, layouts\n\n\nShow the code\nset.seed (1234)\ng <- GAStech_graph %>% \n  mutate(betweenness_centrality = centrality_betweenness()) %>% #<< tidygraph doc\n  ggraph(layout='fr') +\n  geom_edge_link(aes(width=Weight,\n                     alpha= 0.2)) + \n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(colour=Department, size = betweenness_centrality))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code above\n\n\n\n\nmutate() of dplyr is used to perform the computation and create a new col called betweenness_centrality. Use tidyverse commands on tidygraph object.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\nTo see the centrality values\n\n\nShow the code\nGAStech_graph %>% \n  mutate(betweenness_centrality = centrality_betweenness()) \n\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 5\n     id label               Department     Title          betweenness_centrality\n  <dbl> <chr>               <chr>          <chr>                           <dbl>\n1     1 Mat.Bramar          Administration Assistant to …                  307. \n2     2 Anda.Ribera         Administration Assistant to …                  170. \n3     3 Rachel.Pantanal     Administration Assistant to …                   32.8\n4     4 Linda.Lagos         Administration Assistant to …                    0  \n5     5 Ruscella.Mies.Haber Administration Assistant to …                  501. \n6     6 Carla.Forluniau     Administration Assistant to …                   28.2\n# ℹ 48 more rows\n#\n# A tibble: 1,456 × 4\n   from    to Weekday   Weight\n  <int> <int> <ord>      <int>\n1     1     2 Monday         4\n2     1     2 Tuesday        3\n3     1     2 Wednesday      5\n# ℹ 1,453 more rows\n\n\n\n\n27.7.2 Visualising network metrics (without computing col above)\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\nset.seed (1234)\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))   #<<<< access betweenness values directly using `centrality_betweenness()`\ng + theme_graph()\n\n\n\n\n\n\n\n27.7.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used\nthomas85/tidygraph/group\n\n\nShow the code\nset.seed (1234)\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\nWhat does the n_groups() argument do? Set the number of communities?\n\n\nShow the code\nset.seed (1234)\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, \n                                                      directed = TRUE, \n                                                      n_groups = 20))) %>%  #<< should i try to change the number of groups/community?\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.8 Building Interactive Network Graph with visNetwork",
    "text": "27.8 Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns. (Instead of ‘source’ and ‘target’?)\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n27.8.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below. In this df, there is no weekday column, unlike GAStech_edges_aggregated df.\n\n\nShow the code\nGAStech_edges_aggregated2 <- GAStech_edges %>% \n  left_join(GAStech_nodes, by =c('sourceLabel' = 'label')) %>%  #edges$sourceLabel == nodes.label and the key of right table does not appear\n  rename(from = id) %>%   #rename 'id' column to 'from'\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>% \n  filter(MainSubject == 'Work related') %>% \n  group_by(from,to) %>% \n  summarise(weight=n()) %>% \n  filter(from != to) %>% \n  filter(weight >1) %>% \n  ungroup()\n\n\n\n\n27.8.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated2)\n\n\n\n\n27.8.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated2) %>% \n  visIgraphLayout(layout = 'layout_with_fr')\n\n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n27.8.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nShow the code\nGAStech_nodes <- GAStech_nodes %>% rename(group= Department)\n\n\nRunning the code chunk above with the rename column will allow visNet to shade the nodes by assigning a unique colour to each category in the group field.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated2) %>% \n  visIgraphLayout(layout = 'layout_with_fr') %>% \n  visLegend() \n\n\n\n\n\n\nShow the code\n# %>% visLayout(randomSeed = 1234)\n\n\n\n\n27.8.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated2) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visLegend() \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Option to find out more about visEdges’s argument.\narrows : “to”, “from”, “middle”, “middle;to”\ndashes: TRUE , FALSE\ntitle: paste(‘Text’, 1:8) – tooltip\nsmooth: FALSE, TRUE\nshadow: TRUE, FALSE\nvisNetwork(nodes, edges, height = “500px”, width = “100%”)\n\n\n\n\n27.8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n- The argument highlightNearest highlights nearest when clicking a node. (Does it highlight the ego-network?)\n- The argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated2) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reference",
    "title": "Hands-on_Ex08 (Network graphs)",
    "section": "27.9 Reference",
    "text": "27.9 Reference\ntidygraph (tbl obj, centrality calc)\nggraph documentation (Node, edge, layout)\nggraph\ngeom-edge_link()\ngeom_node_point()\ntheme_graph()\nggraph_layout\nFacet\nfacet_edges()\nfacet_nodes()\nfacet_graph()\nset_graph_style() defaults\nthomas85github_theme_graph\ncentrality\nthomas85/tidygraph/group\nvisNetwork()\nIgraph\nvisEdge arguments"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "title": "Hands-on Exercise 11 (Week 6: heatMap)",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 11 (Week 6: heatMap)",
    "section": "14.2 Installing and Launching R Packages",
    "text": "14.2 Installing and Launching R Packages\nBefore you get started, you are required:\n\nto start a new R project, and\nto create a new R Markdown document.\n\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\n\nShow the code\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, lubridate, clock,\n               knitr,plotly,ggthemes, treemap)\n\n\n\n14.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\n\nShow the code\nwh <- read_csv('C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex09/data/WHData-2018.csv')\n\n\n\n\nShow the code\nglimpse(wh)\n\n\nRows: 156\nColumns: 12\n$ Country                        <chr> \"Albania\", \"Bosnia and Herzegovina\", \"B…\n$ Region                         <chr> \"Central and Eastern Europe\", \"Central …\n$ `Happiness score`              <dbl> 4.586, 5.129, 4.933, 5.321, 6.711, 5.73…\n$ `Whisker-high`                 <dbl> 4.695, 5.224, 5.022, 5.398, 6.783, 5.81…\n$ `Whisker-low`                  <dbl> 4.477, 5.035, 4.844, 5.244, 6.639, 5.66…\n$ Dystopia                       <dbl> 1.462, 1.883, 1.219, 1.769, 2.494, 1.45…\n$ `GDP per capita`               <dbl> 0.916, 0.915, 1.054, 1.115, 1.233, 1.20…\n$ `Social support`               <dbl> 0.817, 1.078, 1.515, 1.161, 1.489, 1.53…\n$ `Healthy life expectancy`      <dbl> 0.790, 0.758, 0.712, 0.737, 0.854, 0.73…\n$ `Freedom to make life choices` <dbl> 0.419, 0.280, 0.359, 0.380, 0.543, 0.55…\n$ Generosity                     <dbl> 0.149, 0.216, 0.064, 0.120, 0.064, 0.08…\n$ `Perceptions of corruption`    <dbl> 0.032, 0.000, 0.009, 0.039, 0.034, 0.17…\n\n\n\n\n14.3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\n\nShow the code\nrow.names(wh) <- wh$Country\n\n\nNotice that the row number has been replaced into the country name, instead of 1,2,3,..\n\n\n14.3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\n\nShow the code\nwh1 <- dplyr::select(wh, c(3, 7:12))\nwh_matrix <- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#static-heatmap",
    "title": "Hands-on Exercise 11 (Week 6: heatMap)",
    "section": "14.4 Static Heatmap",
    "text": "14.4 Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nShow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\n\nShow the code\nwh_heatmap <- heatmap(wh_matrix)\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and yellow small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\n\nShow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      scale=\"column\",  #<< this is to normalised?\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 11 (Week 6: heatMap)",
    "section": "14.5 Creating Interactive Heatmap",
    "text": "14.5 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n14.5.1 Working with heatmaply\n\n\nShow the code\nhead(mtcars,5)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\nShow the code\nheatmaply(mtcars)\n\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\n\nShow the code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n14.5.2 Data transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely:\n\nscale,\nnormalise and\npercentilise.\n\n\n14.5.2.1 Scaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\n\nShow the code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n14.5.2.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n14.5.2.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow the code\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n14.5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n14.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\n14.5.5 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nShow the code\nwh_d <- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")  #<< finding distnace matrix? \n\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nShow the code\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n14.5.6 Seriation\n{TBC}\n\n\n14.5.7 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3, \n          seriate = \"OLO\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n14.5.8 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\nExplanation of margin parameter: numeric vector of length 4 (default is c(50,50,NA,0)) containing the margins (see layout) for column, row and main title names, respectively. The top margin is NA by default. If main==“” then the top margin will be set to 0, otherwise it will get 30. For a multiline title a larger default for the 3rd element should be set. The right margin is NA by default, meaning it will be zero if row_dend_left is FALSE, or 100 if row_dend_left is TRUE.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          Colv=NA,\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\n\n\n\n\n\n\nOn take-home2\n\n\nShow the code\nmc2_seafood_edges_agg_vis<- readRDS(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc2_seafood_edges_agg_vis.rds\")\n\n\n\n\nShow the code\nbet_ent <- mc2_seafood_edges_agg_vis %>% \n  group_by(from,to) %>% \n  summarise(Weight=sum(Weight)) %>% \n  ungroup() %>% \n  arrange(desc(Weight))\n\n\n\n\nShow the code\ntop10_bet_ent <- bet_ent %>% \n  pivot_wider(names_from = to, values_from = Weight)\n\n\n\n\nShow the code\ntop10_bet_ent <- replace(top10_bet_ent, is.na(top10_bet_ent), 0)\n\n\n\n\nShow the code\nrow.names(top10_bet_ent) <- top10_bet_ent$from\n\n\n\n\nShow the code\ntop10_bet_ent_matrix <- data.matrix(top10_bet_ent)\n\n\n\n\nShow the code\nheatmap(top10_bet_ent_matrix[, -c(1)],\n        \n        Rowv=NA, \n        Colv=NA,\n        scale = 'column',\n        margins = c(15,12),\n        xlab = \"Receiving companies\", ylab =  \"Shipping companies\",\n              main = \"Heat map of business relationship\"\n        )\n\n\n\n\n\nTree Map\n\n\n\n\n\n\n\n\nShow the code\nseafood_tree <-mc2_seafood_edges_agg_vis %>% \n  group_by(from,to,hscode) %>% \n  summarise(TotalInteractions=sum(Weight),\n            MedianCargoWeight_daily= median(Totalweight)) %>% \n  ungroup() %>% \n  arrange(desc(TotalInteractions))\n\n\n\n\nShow the code\ntm<- treemap(seafood_tree,\n        index=c(\"from\", \"to\"),\n        vSize=\"TotalInteractions\",\n        vColor=\"MedianCargoWeight_daily\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        algorithm = \"squarified\",\n        title='Shipping and receiving companies relationship',\n        title.legend = \"Median Cargo Weight per day\"\n        )\n\n\n\n\n\n\n\nShow the code\nlibrary(d3treeR)\nd3tree(tm,rootname = \"Shipping companies\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "",
    "text": "plotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#getting-started",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "17.2 Getting Started",
    "text": "17.2 Getting Started"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#do-it-yourself",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#do-it-yourself",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "17.3 Do It Yourself",
    "text": "17.3 Do It Yourself\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\n\nShow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "17.4 Plotting Calendar Heatmap",
    "text": "17.4 Plotting Calendar Heatmap\nIn this section, you will learn how to plot a calender heatmap programmetically by using ggplot2 package.\nObjectives for this section:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n17.4.1 The Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n17.4.2 Importing the data\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\n\nShow the code\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n\n17.4.3 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n17.4.4 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\n\nShow the code\nmake_hr_wkday <- function (ts, sc, tz) {\n  real_times <- ymd_hms(ts,\n                        tz = tz[1],     #<< for group by tz later, simply use the first tz value of groupby\n                        quiet = TRUE)\n  \n  dt <- data.table(source_country = sc,\n                   wkday= weekdays(real_times),\n                   hour = hour(real_times))\n  return (dt)\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nShow the code\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(wkday, levels = wkday_levels),\n         hour = factor(hour, levels = 0:23))  #<< 24 hrs \n\n\nExplanation of the code chunk above\n\ntz refers to the tz column of the attacks dataframe.\ntz[1] selects the first element of the tz column within each group.\nThe group_by(tz) statement groups the data based on unique values in the tz column.\nThe do() function is used to apply the make_hr_wkday function to each group of the grouped dataframe.\nWithin the make_hr_wkday function, tz[1] is used to specify the time zone for the conversion of the timestamps within each group. Since each group has the same time zone value, using tz[1] ensures that the function uses the correct time zone for each group.\n\nBy grouping the attacks dataframe by the tz column and applying the make_hr_wkday function within each group, the resulting dataframe will contain the source country, weekday, and hour columns based on the timestamp, source country, and time zone information, grouped by the unique time zones in the tz column of the attacks dataframe.\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n17.4.5 Building the Calendar Heatmaps\n\n\nShow the code\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>% \n  na.omit()\n\nggplot(grouped,\n       aes(hour,\n           wkday,\n           fill = n)) +\n  geom_tile(color = 'white',  #<<< border color and lnie size of tiles\n            size= 0.1) +\n  theme_tufte(base_family = 'Helvetica') + #<< to remove unneccessary graph features\n  coord_equal() + #<< asp ratio of 1:1\n  \n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\ncount() is use to count the number of records for each pair of wkday and hour\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, we can comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\n\n17.4.6 Building Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n17.4.7 Plotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\n\nShow the code\nattacks_by_country <- count(attacks, source_country) %>% \n  mutate(percent = percent(n/sum(n))) %>% \n  arrange(desc(n))\n\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\nFirst, prepare vector of top 4 country names\n\n\nShow the code\ntop4 <- attacks_by_country$source_country[1:4]\nsummary(top4)\n\n\n   Length     Class      Mode \n        4 character character \n\n\nShow the code\ntop4\n\n\n[1] \"CN\" \"US\" \"KR\" \"NL\"\n\n\nWe can achieve the same thing using the code chunk below\n\n\nShow the code\ntop4 <- attacks_by_country %>%  top_n(4, wt= n) %>% pull(source_country)\n\n\nNext, create top4_attacks dataframe by using ‘top4’ list to filter ‘attacks’ dataframe.\n\n\nShow the code\ntop4_attacks <- attacks %>% \n  filter(source_country %in% top4) %>% \n  count(source_country, wkday,hour) %>% \n  ungroup() %>% \n  mutate(source_country = factor(\n    source_country, levels = top4)) %>% \n  na.omit()\n\n\n\n\n\n\n\n\nNote\n\n\n\n‘top4_attacks’ dataframe is similar to ‘grouped’ dataframe, except that it has one more column called ‘source_country’. This additional column will be useful for faceting later.\n\n\n\n\n17.4.8 Plotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\n\nShow the code\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "17.5 Plotting Cycle Plot",
    "text": "17.5 Plotting Cycle Plot\nIn this section, i will practise how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n17.5.1 Step 1: Data Import\nthe dataset arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nShow the code\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\nair %>% head(5) %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\n\n\n\n\n\n\n\n17.5.2 Step 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\n\nShow the code\nair$month <- factor(month(air$`Month-Year`),  # extract month\n                    levels = 1:12,  #<< 12 months\n                    ordered= TRUE)\n\nair$year <- year(ymd(air$`Month-Year`))   # extract year\n\n\n\n\n17.5.3 Step 4: Extracting the target country\nNext, the code chunk below is use to create dataframe containing the arrival data for vietnam from 2010 onwards by selecting columns ‘Vietname’, ’ month’ and ‘year’ from the air dataframe.\n\n\nShow the code\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n\n17.5.4 Step 5: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month for all the years in Vietnam dataframe.\n\n\nShow the code\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n17.5.5 Step 6: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\n\nShow the code\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex14/Hands-on_Ex14.html#plotting-slopegraph",
    "title": "Hands-on Exercise 14 (Week 7: Visualising and Analysing Time-oriented Data)",
    "section": "17.6 Plotting Slopegraph",
    "text": "17.6 Plotting Slopegraph\nIn this section I will practise how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n17.6.1 Step 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\n\nShow the code\nrice <- read_csv(\"data/rice.csv\")\n\n\nNotice that ‘year’ column is in the wrong format of number. We should convert it to factor instead.\n\n\n17.6.2 Step 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\nSince slopegraph contains information only for two years, we will filter data in 1960 and 1980 only.\n\n\nShow the code\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"My VA Prof: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex15/Hands-on_Ex15.html",
    "href": "Hands-on_Ex/Hands-on_Ex15/Hands-on_Ex15.html",
    "title": "Hands-on Exercise 15 (Week 7: Time on the Horizon: ggHoriPlot methods)",
    "section": "",
    "text": "A horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series as shown in the figure below.\n\nA horizon graph essentially an area chart that has been split into slices and the slices then layered on top of one another with the areas representing the highest (absolute) values on top. Each slice has a greater intensity of colour based on the absolute value it represents.\n\nIn this section, we will plot a horizon graph by using ggHoriPlot package.\n\n\n\n\n\n\nNote\n\n\n\nBefore getting started, please visit Getting Started to learn more about the functions of ggHoriPlot package. Next, read geom_horizon() to learn more about the usage of its arguments.\n\n\n\n\n\n\n\nShow the code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\nUse the code chunk below to import the AVERP.csv file into R environment.\n\n\nShow the code\naverp <- read_csv(\"C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex14/data/AVERP.csv\") \n\n\nThe date field is in the wrong format and in chr field.\n\n\nShow the code\naverp <- averp %>%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\nNext, the code chunk below will be used to plot the horizon graph.\n\n\nShow the code\naverp %>% \n  filter(Date >= '2018-01-01') %>% \n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values),\n               origin = 'midpoint',      #<< mirror at midpoint\n               horizonscale = 6) +       #<< 6 months intervals\n  \n  facet_grid(`Consumer Items` ~.) +   #<< put a function to inform to output as facet grid??\n  \n  theme_few() +\n  scale_fill_hcl(palette = 'RdBu') + \n  theme(panel.spacing.y=unit(0,'lines'), \n        strip.text.y = element_text(size = 5,     #<<< the labels size, try and error\n                                    angle = 0,\n                                    hjust = 0),\n        legend.position = 'none',\n        axis.text.y = element_blank(),\n        axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   size=7),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.border = element_blank()) +\n  \n  scale_x_date(expand=c(0,0), \n               date_breaks = \"3 month\", \n               date_labels = \"%b%y\") +\n  \n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class_Ex01",
    "section": "",
    "text": "pacman:: p_load(tidyverse)\n\n\nexam_data <- read_csv(\"data1/Exam_data.csv\")\n\n\n\nIn this section, I will explore the theme_minimal() and modify the components of a theme (e.g. plot fill and gridline colours).\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill ='lavender', color ='white',linetype ='solid'),\n        panel.grid.major = element_line (color= 'white', linetype = 'solid'),\n        panel.grid.minor = element_line (colour='white', size= 0.2, linetype = 'solid'),\n        plot.title= element_text(size=rel(1.5))) +\n  ggtitle('Number of students by Race') +\n  labs(y='Number of students')\n\nReference website : R bloggers (How to change the background colour of ggplot2?)\nTo modify components of a theme , refer to this ggplot2 webpage\n\n\n\n\n\n\nThere are several flaws in the design below, namely:\n\ny-axis label is not clear (i.e. count)\nTo support effective comparison, the bars should be sorted by their respective frequencies.\nFor static graph, frequency values should be added to provide addition information. (labelled on the graph)\n\n\n\nCode\nggplot(data=exam_data,\n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\nThe designs below are improvised versions with the following features:\n\nBoth axes labelled clearly.\nBars are sorted by - count (descending order).\nCount and percentage are labelled above the bars.\n\n\nMakeover 1 (NYX)Makeover 1 code chunk\n\n\n\n\n\n\n\n\n\n\nexam_data %>%\n  group_by(RACE) %>% \n  summarise(count = n())\n\n# A tibble: 4 × 2\n  RACE    count\n  <chr>   <int>\n1 Chinese   193\n2 Indian     12\n3 Malay     108\n4 Others      9\n\n\nTHe output of the code below is C,M,I O\n\nt <- exam_data %>%\n  group_by(RACE) %>% \n  summarise(count = n())\n\nreorder(t$RACE, (-t$count))\n\n[1] Chinese Indian  Malay   Others \nattr(,\"scores\")\nChinese  Indian   Malay  Others \n   -193     -12    -108      -9 \nLevels: Chinese Malay Indian Others\n\n\n\nexam_data %>%\n  group_by(RACE) %>% \n  summarise(count = n()) %>% \n  ggplot(aes(x = reorder(RACE, (-count)), y = count)) +\n  geom_bar(stat = 'identity', color='black', fill = '#DD8888') +\n  ylim(0,220) +\n  geom_text(aes(label = paste0(count,', ', round(count/sum(count)*100,1), '%')),\n            position = position_dodge(width = 0.8), vjust= -1, size = 3.5) +\n  ggtitle('Distribution of Race') +\n  labs(y='No. \\nof \\nPupils', x = 'Race') +\n  theme(plot.title = element_text(face='bold', hjust = 0.5),   #bold title and center-justify\n        axis.title.y=element_text(angle=0)) \n\nMeaning of the argument ‘identity’ in the ‘stat’ parameter:\nIf we provide the argument stat=“identity” to geom_bar() then we’re telling R to calculate the sum of the y variable, grouped by the x variable and use bars to display the sums\nThere are three arguments in the reorder() function.\n\ncategorical variable to be sorted\nvariable to sort (1) by\na function that returns numerical value on how to sort (1) by\n\nReferences:\nhttps://www.roelpeters.be/reorder-ggplot2-bar-chart-by-count/ http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization#bar-plot-with-labels\n\n\n\n\nMakeover 2 (Prof)Makeover 2 code chunkUnderstanding Reorder()\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=reorder(RACE,RACE,\n                     function(x)-length(x)))) +\n  geom_bar() +\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100, 1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\nOutput of the code below is C,M,I,O\n\nreorder(exam_data$RACE,exam_data$RACE,function(x)-length(x))\n\n  [1] Malay   Malay   Chinese Chinese Malay   Malay   Chinese Malay   Malay  \n [10] Indian  Chinese Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n [19] Malay   Malay   Malay   Malay   Chinese Others  Malay   Malay   Malay  \n [28] Malay   Indian  Indian  Malay   Malay   Chinese Malay   Chinese Malay  \n [37] Chinese Chinese Malay   Chinese Malay   Malay   Malay   Malay   Malay  \n [46] Chinese Chinese Chinese Indian  Malay   Chinese Chinese Chinese Malay  \n [55] Chinese Chinese Chinese Malay   Chinese Chinese Malay   Indian  Malay  \n [64] Chinese Malay   Malay   Malay   Malay   Chinese Chinese Malay   Malay  \n [73] Malay   Chinese Malay   Chinese Malay   Malay   Malay   Indian  Malay  \n [82] Malay   Chinese Chinese Malay   Indian  Chinese Chinese Chinese Chinese\n [91] Malay   Malay   Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n[100] Malay   Chinese Chinese Chinese Chinese Malay   Chinese Chinese Others \n[109] Indian  Chinese Malay   Chinese Malay   Malay   Chinese Malay   Chinese\n[118] Chinese Chinese Chinese Malay   Malay   Malay   Chinese Malay   Malay  \n[127] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Malay   Chinese\n[136] Chinese Malay   Others  Malay   Chinese Chinese Malay   Chinese Chinese\n[145] Chinese Malay   Chinese Chinese Malay   Others  Malay   Chinese Malay  \n[154] Malay   Chinese Chinese Malay   Chinese Others  Malay   Malay   Malay  \n[163] Malay   Chinese Malay   Chinese Chinese Others  Malay   Chinese Chinese\n[172] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[181] Malay   Chinese Chinese Chinese Chinese Chinese Chinese Chinese Malay  \n[190] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Indian  Malay  \n[199] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[208] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[217] Chinese Chinese Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[226] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese\n[235] Chinese Malay   Chinese Chinese Malay   Chinese Malay   Malay   Malay  \n[244] Chinese Chinese Indian  Malay   Others  Malay   Chinese Chinese Chinese\n[253] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese\n[262] Chinese Chinese Malay   Chinese Chinese Malay   Chinese Chinese Malay  \n[271] Chinese Malay   Chinese Others  Chinese Chinese Malay   Malay   Malay  \n[280] Chinese Indian  Chinese Chinese Chinese Chinese Chinese Malay   Chinese\n[289] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[298] Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[307] Chinese Chinese Chinese Indian  Chinese Chinese Malay   Chinese Chinese\n[316] Chinese Chinese Chinese Others  Chinese Chinese Chinese\nattr(,\"scores\")\nChinese  Indian   Malay  Others \n   -193     -12    -108      -9 \nLevels: Chinese Malay Indian Others\n\n\n\n\n\n\nMakeover 3 (Forcats package)Makeover 3 code chunkUnderstanding fct_infreq\n\n\n\n\n\n\n\n\n\n\nexam_data %>%\n  mutate(RACE = fct_infreq(RACE)) %>%\n  ggplot(aes(x = RACE)) + \n  geom_bar()+\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100,\n            1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\nOutput of the code below is also C,M,I,O\n\nfct_infreq(exam_data$RACE)\n\n  [1] Malay   Malay   Chinese Chinese Malay   Malay   Chinese Malay   Malay  \n [10] Indian  Chinese Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n [19] Malay   Malay   Malay   Malay   Chinese Others  Malay   Malay   Malay  \n [28] Malay   Indian  Indian  Malay   Malay   Chinese Malay   Chinese Malay  \n [37] Chinese Chinese Malay   Chinese Malay   Malay   Malay   Malay   Malay  \n [46] Chinese Chinese Chinese Indian  Malay   Chinese Chinese Chinese Malay  \n [55] Chinese Chinese Chinese Malay   Chinese Chinese Malay   Indian  Malay  \n [64] Chinese Malay   Malay   Malay   Malay   Chinese Chinese Malay   Malay  \n [73] Malay   Chinese Malay   Chinese Malay   Malay   Malay   Indian  Malay  \n [82] Malay   Chinese Chinese Malay   Indian  Chinese Chinese Chinese Chinese\n [91] Malay   Malay   Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n[100] Malay   Chinese Chinese Chinese Chinese Malay   Chinese Chinese Others \n[109] Indian  Chinese Malay   Chinese Malay   Malay   Chinese Malay   Chinese\n[118] Chinese Chinese Chinese Malay   Malay   Malay   Chinese Malay   Malay  \n[127] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Malay   Chinese\n[136] Chinese Malay   Others  Malay   Chinese Chinese Malay   Chinese Chinese\n[145] Chinese Malay   Chinese Chinese Malay   Others  Malay   Chinese Malay  \n[154] Malay   Chinese Chinese Malay   Chinese Others  Malay   Malay   Malay  \n[163] Malay   Chinese Malay   Chinese Chinese Others  Malay   Chinese Chinese\n[172] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[181] Malay   Chinese Chinese Chinese Chinese Chinese Chinese Chinese Malay  \n[190] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Indian  Malay  \n[199] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[208] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[217] Chinese Chinese Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[226] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese\n[235] Chinese Malay   Chinese Chinese Malay   Chinese Malay   Malay   Malay  \n[244] Chinese Chinese Indian  Malay   Others  Malay   Chinese Chinese Chinese\n[253] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese\n[262] Chinese Chinese Malay   Chinese Chinese Malay   Chinese Chinese Malay  \n[271] Chinese Malay   Chinese Others  Chinese Chinese Malay   Malay   Malay  \n[280] Chinese Indian  Chinese Chinese Chinese Chinese Chinese Malay   Chinese\n[289] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[298] Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[307] Chinese Chinese Chinese Indian  Chinese Chinese Malay   Chinese Chinese\n[316] Chinese Chinese Chinese Others  Chinese Chinese Chinese\nLevels: Chinese Malay Indian Others\n\n\n\n\n\nSorting a boxplot by median of Math scores using reorder().\nIn the viz below, we are able to achieve several things in one go\n\nsort the boxplot by median of the Math scores in descending order.\nadd mean value by Race\nuse colours to distinguish between outliers and jitters\n\n\nBoxplot designCode ChunkReorder()\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, aes(x = reorder(RACE, -MATHS, median), y = MATHS, fill=RACE)) +\n   geom_boxplot(outlier.colour=\"blue\", outlier.size=1) +\n   geom_point(position = 'jitter',size=0.5) +\n   stat_summary(fun.y=mean, geom=\"point\", shape=20, size=3, color=\"pink\", fill=\"red\") +\n   xlab(\"Race\") +\n   ylab(\"Math Score\") +\n   ggtitle(\"Math Scores by Race\") +\n   scale_fill_brewer(palette='Set2') +\n   theme(plot.title = element_text(hjust = 0.5),\n         legend.position = 'none')\n\nReference: Link\n\n\n\n\nCode\nreorder(exam_data$RACE, -exam_data$MATHS, median)\n\n\n  [1] Malay   Malay   Chinese Chinese Malay   Malay   Chinese Malay   Malay  \n [10] Indian  Chinese Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n [19] Malay   Malay   Malay   Malay   Chinese Others  Malay   Malay   Malay  \n [28] Malay   Indian  Indian  Malay   Malay   Chinese Malay   Chinese Malay  \n [37] Chinese Chinese Malay   Chinese Malay   Malay   Malay   Malay   Malay  \n [46] Chinese Chinese Chinese Indian  Malay   Chinese Chinese Chinese Malay  \n [55] Chinese Chinese Chinese Malay   Chinese Chinese Malay   Indian  Malay  \n [64] Chinese Malay   Malay   Malay   Malay   Chinese Chinese Malay   Malay  \n [73] Malay   Chinese Malay   Chinese Malay   Malay   Malay   Indian  Malay  \n [82] Malay   Chinese Chinese Malay   Indian  Chinese Chinese Chinese Chinese\n [91] Malay   Malay   Malay   Chinese Chinese Chinese Chinese Malay   Malay  \n[100] Malay   Chinese Chinese Chinese Chinese Malay   Chinese Chinese Others \n[109] Indian  Chinese Malay   Chinese Malay   Malay   Chinese Malay   Chinese\n[118] Chinese Chinese Chinese Malay   Malay   Malay   Chinese Malay   Malay  \n[127] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Malay   Chinese\n[136] Chinese Malay   Others  Malay   Chinese Chinese Malay   Chinese Chinese\n[145] Chinese Malay   Chinese Chinese Malay   Others  Malay   Chinese Malay  \n[154] Malay   Chinese Chinese Malay   Chinese Others  Malay   Malay   Malay  \n[163] Malay   Chinese Malay   Chinese Chinese Others  Malay   Chinese Chinese\n[172] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[181] Malay   Chinese Chinese Chinese Chinese Chinese Chinese Chinese Malay  \n[190] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Indian  Malay  \n[199] Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese Chinese\n[208] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[217] Chinese Chinese Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[226] Chinese Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese\n[235] Chinese Malay   Chinese Chinese Malay   Chinese Malay   Malay   Malay  \n[244] Chinese Chinese Indian  Malay   Others  Malay   Chinese Chinese Chinese\n[253] Chinese Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese\n[262] Chinese Chinese Malay   Chinese Chinese Malay   Chinese Chinese Malay  \n[271] Chinese Malay   Chinese Others  Chinese Chinese Malay   Malay   Malay  \n[280] Chinese Indian  Chinese Chinese Chinese Chinese Chinese Malay   Chinese\n[289] Chinese Chinese Chinese Chinese Malay   Chinese Chinese Chinese Chinese\n[298] Chinese Malay   Malay   Chinese Chinese Chinese Chinese Chinese Chinese\n[307] Chinese Chinese Chinese Indian  Chinese Chinese Malay   Chinese Chinese\n[316] Chinese Chinese Chinese Others  Chinese Chinese Chinese\nattr(,\"scores\")\nChinese  Indian   Malay  Others \n  -79.0   -58.5   -61.5   -70.0 \nLevels: Chinese Others Malay Indian\n\n\n\n\n\n\n\n\nThere are several flaws in the design below, namely:\n\nThe outline of the bars is unclear\nUnable to see how the scores are binned\nAddition of reference line (e.g. 75th percentile) will help users get a better understanding of the data.\n\n\n\nCode\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_histogram(binwidth=5)\n\n\n\n\n\nThe designs below are improvised version with the following features:\nMakeover 1:\n\nIncludes the 50th and 75th percentile line in the plot.\n\nMakeover 2:\n\nAdding mean and median lines on the histogram plot.\nChange fill color and line color\n\n\nMakeover design 1 (NYX)Makeover 1 code chunk (NYX)\n\n\n\n\n\n\n\n\n\n\nq <- quantile(exam_data$MATHS, probs = c(0.25, 0.5, 0.75))\n\nggplot(data=exam_data,\n  aes(x=MATHS)) +\n  geom_histogram(binwidth = 5, color='black',size= 0.3, fill = '#DD8888') +\n  geom_vline(xintercept = q[2], linetype='dotted', size = 0.8, color='blue') +\n  geom_vline(xintercept = q[3], linetype='dotted', size = 0.8) +\n  annotate('text' , x= 70, y=50, label='50th \\npercentile', size = 3) +\n  annotate('text' , x= 90, y=50, label='75th \\npercentile', size = 3) +\n  labs(y= 'No. of \\nPupils', x='math Score') +\n  theme(axis.title.y=element_text(angle = 0)) +\n  ggtitle('Distribution of Math scores')\n\nReference: link\n\n\n\n\nMakeover Design 2 (Adapted from Prof)Makeover 2 code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept = mean(MATHS, na.rm=T)),\n             color='red',\n             linetype = 'dashed',\n             size = 1) +\n  annotate(\"text\", x=65, y=50, label=\"mean\", angle=0, size = 4) +\n  geom_vline(aes(xintercept = median(MATHS, na.rm=T)),\n             color='gray30',\n             linetype = 'dashed',\n             size = 1) +\n  annotate(\"text\", x=79, y=50, label=\"median\", angle=0, size = 4)\n\n\n\n\n\n\n\nThe viz below shows the distribution by gender. How can we make it more informative? Could we try to add a background layer of histogram that describe the overall distribution of ENGLISH scores?\n\n\nCode\nggplot(data=exam_data,\n       aes(x=ENGLISH, fill= GENDER)) +\n  geom_histogram(bins=20) +\n  facet_wrap(~GENDER)\n\n\n\n\n\nThe makeover design below now include overall distribution in the backgrounds of the trellis plots by gender.\n\nMakeover design 1Makeover 1 code chunk\n\n\n\n\n\n\n\n\n\n\nd_bg <- exam_data[, -3]  # background data without GENDER column\nggplot(data=exam_data,\n       aes(x=ENGLISH, fill=GENDER)) +\n  geom_histogram(data=d_bg, bins=20, fill='grey',alpha=0.5) +\n  geom_histogram(colour='black') +\n  facet_wrap(~GENDER) +\n  guides(fill=FALSE) + \n  theme_minimal() +\n  labs(y= 'No. of \\nPupils', x='English Score') +\n  theme(axis.title.y=element_text(angle = 0)) +\n  ggtitle('Distribution of English scores by Gender')\n\nThe geom_histogram() function is used twice to create two overlapping histograms.\nThe first histogram is created using the d_bg dataset (no GENDER data, rep all GENDERs) and is filled with grey. This histogram represents the background distribution of ENGLISH marks across ALL GENDERs.\nThe second histogram is created using the entire exam dataset and makes use of the fill = GENDER in ggplot() to color gender differently. This results in a legend and is removed with guides(fill=FALSE). It also has a black border. This histogram represents the distribution of ENGLISH scores for each individual GENDER.\nFinally, facet_wrap(~GENDER) will create panels to represent distribution for each GENDER for easy comparison.\nReference: Plot background histogram data\n\n\n\n\n\n\nThe original design below has several flaws:\n\nThe limits and scale ticks of both axes are inconsistent.\n\n\n\nCode\nggplot(data=exam_data,\n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point()\n\n\n\n\n\nThe improvised design in Makeover 1 now includes:\n\nCommon x and Y axes scale and range.\nReference lines at 50 marks for both subjects.\n\n\nMakeover 1Makeover 1 code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  ylim(0,100) +\n  xlim(0,100) +\n  geom_vline(xintercept=50, linetype='dashed') +\n  geom_hline(yintercept=50, linetype='dashed')\n\n\n\n\nIn makeover 2, I have included a third numerical variable, Science score into the scatterplot.\nThe position of the legend has been shifted to the bottom.\nIn makeover 3, I have used the ggExtra library to add distribution of the English and Math scores in the form of histograms at the margins of the plot.\n\nMakeover 2Makeover 3\n\n\n\n\nCode\nggplot(data=exam_data,\n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point(aes(color=SCIENCE)) +\n  ylim(0,100) +\n  xlim(0,100) +\n  geom_vline(xintercept=50, linetype='dashed') +\n  geom_hline(yintercept=50, linetype='dashed') +\n  ggtitle('Scatterplot of English and Math scores') +\n  theme_minimal() +\n  theme(legend.position='bottom') +\n  scale_color_gradient(low=\"darkgreen\", high=\"green\")\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggExtra)\n\nggMarginal(ggplot(data=exam_data,\n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point(aes(color=SCIENCE)) +\n  ylim(0,100) +\n  xlim(0,100) +\n  geom_vline(xintercept=50, linetype='dashed') +\n  geom_hline(yintercept=50, linetype='dashed') +\n  ggtitle('Scatterplot of English and Math scores') +\n  theme_minimal() +\n  theme(legend.position='bottom') +\n  scale_color_gradient(low=\"darkgreen\", high=\"green\"), \n  \n  type=\"histogram\", fill =4)\n\n\n\n\n\nReference: ‘How to change ggplot2 colours manually and automatically?’"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "In today’s in class exercise, Prof shared with us how to combine a qqplot and tabular results of Shapiro test in a single plot."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-libraries",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-libraries",
    "title": "In-class_Ex04",
    "section": "Import libraries",
    "text": "Import libraries\nThe new libraries used today are :\n\nrstatic: Allows us to perform basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\ngt() : starting from a tibble table, customise a table and export in various formats. Most importantly, it works with patch. We will save the tabular results from shapiro test as gt object and export using gtsave() into .png file later.\n\n\n\n\n\n\nOn importing data\n\n\n\n\nImporting tidyverse: will automatically provide read_r() <- for read_csv()\nread.csv() will insert ‘.’ between words. Thus we normally use read_csv()\nTo read excel files, load readxl and use read_xls()\n\n\n\n\n\npacman::p_load(rstatix, gt, patchwork,tidyverse,nortest)\n\n\nexam <- read_csv('C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex05/data/Exam_data.csv')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#background-info",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#background-info",
    "title": "In-class_Ex04",
    "section": "Background info",
    "text": "Background info\n\nThe Anderson_darling test\nUsually, when we check for normality of a distribution, we can use the Anderson-darling test or the Shapiro test. Hitting the three commands below will give us the results, but no visualisation.\n\nad.test(exam$ENGLISH)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$ENGLISH\nA = 4.3661, p-value = 7.341e-11\n\n\n\n\nThe shapiro.test\nUsing shapiro.test will generate result as a HTML object.\n\nshapiro.test(exam$ENGLISH)\n\n\n    Shapiro-Wilk normality test\n\ndata:  exam$ENGLISH\nW = 0.9543, p-value = 1.811e-08\n\n\nUsing shapiro_test will generate result as a tibble object.\n\nexam %>% \n  shapiro_test(ENGLISH)\n\n# A tibble: 1 × 3\n  variable statistic            p\n  <chr>        <dbl>        <dbl>\n1 ENGLISH      0.954 0.0000000181\n\n\n\n\nQQplot\nWe can also generate the qqplot to check for normality. However qqplot does not print any p-values.\n\nggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#task-for-today",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#task-for-today",
    "title": "In-class_Ex04",
    "section": "Task for today:",
    "text": "Task for today:\n\nCombine qqplot with results from Shapiro-test.\nRecall that in hands-on 3, we use DT to create an interactive table , however it is not recognized by patchwork.\nWe start by storing the shapiro test in a tibble table as shown above. Then we will use the gt() package and export it as a .png using gtsave().\n\nqq <- ggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam %>% \n  shapiro_test(ENGLISH) %>% gt()   #<<< make into a gt format (will give a nice table)  shapiro.test is not used here as it gives output in another format.\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\nI tried to customise the gt() table.\n\nqq <- ggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam %>% \n  shapiro_test(ENGLISH) %>% gt()  %>%  \n  tab_header(\n    title = 'Shapiro Test for Normality',\n    subtitle = 'English scores')\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png  # use patchwork to stitch\n\n\n\n\nThe results of the Shapiro test shows that p-value < 0.05 and we have enough statistical evidence to reject the null hypothesis and conclude that English scores do not follow normal distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class_Ex05 (vast challenge Data)",
    "section": "",
    "text": "Note\n\n\n\nEdge data should be organised as such: (can use dplyr methods)\nFirst column: Source id (FK to Node second column) - compulsory\nSecond column: Target id (FK to Node second column) - compulsory\nNode data\nFirst column: ID - compulsory\nSecond column: Label (contains all the distinct values of source and target in Edge data) (only need if Id are all integers) (what is present in edge data must exists in Labels of node data, must not be missing in node data)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTry not to use R built-in NA/NULL function. Manually type “unknown’ / ‘missing’ as a value instead.\n\n\nIn today’s in class exercise,\nImport libraries\nThe new libraries used today are :\n\njsonlite to import json file\n\n\n\nShow the code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph,\n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts,knitr)\n\n\n\n\nShow the code\nMC1 <- jsonlite::fromJSON(\"C:/yixin-neo/ISSS608-VAA/Project/data/MC1.json\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nProblem with dataset of links:\nSource and Data columns are at the back instead of the first 2 columns\n\n\nPull out the nodes and edge data and save them as tibble data frames.\n\n\nShow the code\nMC1_nodes <- as_tibble(MC1$nodes) %>% \n  select(id,type,country)\n\n\n\n\nShow the code\nMC1_edges <- as_tibble(MC1$links) %>% \n  select(source,target,type,weight,key)  \n# can exclude dataste column as they all contain the same values.\n\n\nBack to GAStech dataset\n\n\nShow the code\nGAStech_nodes <- read_csv(\"C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex08/data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv('C:/yixin-neo/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex08/data/GAStech_email_edge-v2.csv')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to my ISSS608 Visual Analytics and Applications HomePage! In this website, you will find my journey in using R for visualisation."
  },
  {
    "objectID": "Project/Project_MC2.html",
    "href": "Project/Project_MC2.html",
    "title": "Project_MC2",
    "section": "",
    "text": "Note\n\n\n\nEdge data should be organised as such: (can use dplyr methods)\nFirst column: Source id (FK to Node second column) - compulsory\nSecond column: Target id (FK to Node second column) - compulsory\nNode data\nFirst column: ID - compulsory\nSecond column: Label (contains all the distinct values of source and target in Edge data) (only need if Id are all integers) (what is present in edge data must exists in Labels of node data, must not be missing in node data)"
  },
  {
    "objectID": "Project/Project_MC2.html#data-dictionary",
    "href": "Project/Project_MC2.html#data-dictionary",
    "title": "Project_MC2",
    "section": "1.1 Data dictionary",
    "text": "1.1 Data dictionary\nNode Attributes:\nid -- Name of the company that originated (or received) the shipment\nshpcountry -- Country the company most often associated with when shipping\nrcvcountry -- Country the company most often associated with when receiving\ndataset -- Always ‘MC2’\nEdge Attributes:\narrivaldate -- Date the shipment arrived at port in YYYY-MM-DD format.\nhscode -- Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu -- Customs-declared value of the total shipment, in Oceanus\nMonetary Units (OMU)\nvolumeteu -- The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required. (Actual number of\ncontainers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg -- The weight of the shipment in kilograms (if known)\ndataset -- Always ‘MC2’\ntype -- Always ‘shipment’ for MC2\ngenerated_by -- Name of the program that generated the edge. (Only found on ‘bundle’ records.)"
  },
  {
    "objectID": "Project/Project_MC2.html#importing-the-datasets",
    "href": "Project/Project_MC2.html#importing-the-datasets",
    "title": "Project_MC2",
    "section": "1.2 Importing the datasets",
    "text": "1.2 Importing the datasets\nImport libraries\nThe new libraries used today are :\n\njsonlite to import json file\n\n\n\nShow the code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph,\n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts,knitr)\n\n\n\n\nShow the code\nMC2 <- jsonlite::fromJSON(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc2_challenge_graph.json\")\n\n\n\n\nShow the code\ncarp <- jsonlite::fromJSON(\"C:/yixin-neo/ISSS608-VAA/Project/data/bundles/carp.json\")\n\n\nPull out the nodes and edge data and save them as tibble data frames.\n\n\nShow the code\nMC2_nodes <- as_tibble(MC2$nodes) %>% \n  select(id,shpcountry,rcvcountry)\n\n\nRearranging the columns in edge file as we require source and target columns to be the first two columns.\n\n\nShow the code\nMC2_edges <- as_tibble(MC2$links) %>% \n  select(source,target,arrivaldate,hscode,valueofgoods_omu,volumeteu,weightkg,valueofgoodsusd)  \n# can exclude dataste column as they all contain the same values.\n\n\n\n\nShow the code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\n\n\nShow the code\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 8\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…"
  },
  {
    "objectID": "Project/Project_MC2.html#data-cleaning",
    "href": "Project/Project_MC2.html#data-cleaning",
    "title": "Project_MC2",
    "section": "1.3 Data cleaning",
    "text": "1.3 Data cleaning\n\n1.3.1 Check for null values\nCheck whether each column in MC2_nodes and MC2_edges contains null and prints the percentage of null for each column.\nFor MC2_nodes dataframe:\nThere are no null values in the id column of Nodes file, which is great.\n\n\nShow the code\n# Check for null values in each column\nnull_counts_nodes <- sapply(MC2_nodes, function(x) sum(is.null(x) | is.na(x)))\n\n# Calculate the percentage of null values for each column\nnull_percentages_nodes <- null_counts_nodes / nrow(MC2_nodes) * 100\n\n# Display the results\n#print(null_percentages)\n\nknitr::kable(null_percentages_nodes, \"simple\")\n\n\n\n\n\nx\n\n\n\n\n\nid\n0.00000\n\n\nshpcountry\n64.66624\n\n\nrcvcountry\n8.41335\n\n\n\n\n\nFor MC2_edges dataframe:\nAs there are a lot zeros inside MC2_edges$volumteu col, we will consider 0 as equivalent to null values.\nWe can see that the columns valueofgoods_omu and volumeteu are mainly null. valueofgoodusd column contains more than 50% null values. There are 4 records of source with 0 as value, but 0 is their unique identifier so we do not consider 0 as null in source column. It means to say that only source, target, arrivaldate, hscode and weight columns will be helpful in our analysis.\n\n\nShow the code\n# Check for null values in each column\nnull_counts <- sapply(MC2_edges, function(x) sum(is.null(x) | is.na(x) | x==0))\n\n# Calculate the percentage of null values for each column\nnull_percentages <- null_counts / nrow(MC2_edges) * 100\n\n# Display the results\n#print(null_percentages)\n\nknitr::kable(null_percentages, \"simple\")\n\n\n\n\n\nx\n\n\n\n\n\nsource\n0.0000732\n\n\ntarget\n0.0000000\n\n\narrivaldate\n0.0000000\n\n\nhscode\n0.0000000\n\n\nvalueofgoods_omu\n99.9948576\n\n\nvolumeteu\n85.6684146\n\n\nweightkg\n1.0136561\n\n\nvalueofgoodsusd\n55.3358864\n\n\n\n\n\nWe will be dropping the valueofgoods_omu , valueofgoodusdand volumeteu columns from our dataframe.\n\n\nShow the code\nMC2_edges <- MC2_edges %>% select('source','target', 'arrivaldate', 'hscode','weightkg')\n\n\n\n\n\n\n\n1.3.2 Lets check for duplicates\nFor MC2_nodes dataframe:\nThere are no duplicated nodes, which is great.\n\n\nShow the code\n# check for nay duplicates\nany(duplicated(MC2_nodes))\n\n\nFor MC2_edges dataframe:\nThere are about 0.15 mil records (2 % out of total records) that are duplicated.\n\n\nShow the code\n#duplicated only\nprint(any(duplicated(MC2_edges)))\n\n\n[1] TRUE\n\n\nShow the code\nMC2_edges_dup <- MC2_edges[duplicated(MC2_edges), ]\nprint(nrow(MC2_edges_dup))\n\n\n[1] 273971\n\n\nWe will drop the duplicates.\n\n\nShow the code\n# Drop duplicate rows from the dataframe\nMC2_edges_no_dup <- MC2_edges[!duplicated(MC2_edges), ]\n\n\n\n\n1.3.3 Check on the HScodes.\nCheck the unique number of hscodes in the dataset. There are 4761 unique HScodes.\n\n\nShow the code\n# Find the number of unique values in hscode\nlength(unique(MC2_edges_no_dup$hscode))\n\n\n[1] 4761\n\n\nWith reference to World Custom Organisation Harmonized System codes, Section 1 and 4 are related to seafood. We will filter for records that has HScodes starting with 1604 and 1605 as they refer to seafood commodities, thus removing many other transactions like ‘television’, ‘steel parts’ etc…\n\n\nShow the code\nmc2_seafood_edges<- MC2_edges_no_dup[grepl('^1605|^1604', MC2_edges_no_dup$hscode), ]\n#MC2_edges[startsWith(MC2_edges$hscode, \"1601\"), ]\n\n\n\n\nShow the code\n#unique(mc2_seafood_edges$hscode)\n#unique(mc2_seafood_edges$source)\nmc2_seafood_edges_agg <- mc2_seafood_edges %>%  \n  group_by(source, target,arrivaldate) %>% \n  summarise(Weight=n(),\n            Totalweight = sum(weightkg),\n            hscode=first(hscode)) %>% \n  filter(Weight >=5) %>% \n  ungroup()\n\n\nWhen i tried to plot the graph, i found several disconnected components. Thus I am going to inspect the frequency of source and target actors, and remove those actors below a frequency count of 5. First , we remove low frequency source actors under 5 counts.\n\n\nShow the code\n# Calculate the frequency count of values in 'source'\nfrequency_table <- table(mc2_seafood_edges_agg$source)\n\n# Get the values in 'col1' with a frequency count greater than or equal to 5\nvalid_source <- names(frequency_table[frequency_table >= 5])\n\n# Subset the dataframe to keep only rows with valid values in 'col1'\nmc2_seafood_edges_agg <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$source %in% valid_source, ]\n\n\nNext, remove target actors with frequency count less than 5:\n\n\nShow the code\n# Calculate the frequency count of values in 'source'\nfrequency_table <- table(mc2_seafood_edges_agg$target)\n\n# Get the values in 'col1' with a frequency count greater than or equal to 5\nvalid_target <- names(frequency_table[frequency_table >= 5])\n\n# Subset the dataframe to keep only rows with valid values in 'col1'\nmc2_seafood_edges_agg <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$target %in% valid_target, ]\n\n\n# Print the filtered dataframe\n#print(mc2_seafood_edges_agg)\n\n\n\n\nShow the code\n #sort(table(mc2_seafood_edges_agg$target))\n\n\n\n\n1.3.4 Preparation of Nodes\nWe will include only nodes that are in source and target columns in the mc2_seafood_edges_agg dataframe\n\n\nShow the code\nnodes_seafood <- MC2_nodes %>%\n  filter (id %in% c(mc2_seafood_edges_agg$source, mc2_seafood_edges_agg$target))"
  },
  {
    "objectID": "Project/Project_MC2.html#creating-the-network-graph-dataframe-using-tbl_graph-of-the-tidygraph-package.",
    "href": "Project/Project_MC2.html#creating-the-network-graph-dataframe-using-tbl_graph-of-the-tidygraph-package.",
    "title": "Project_MC2",
    "section": "2.1 Creating the network graph dataframe using tbl_graph() of the tidygraph package.",
    "text": "2.1 Creating the network graph dataframe using tbl_graph() of the tidygraph package.\n\n\n\n\n\n\nNote\n\n\n\nNode file needs to have ID of nodes as first column.\nEdge file need to contain source and target as column 1 and 2.\n\n\nTo create the dataframe\n\n\nShow the code\nseafood_graph<- tbl_graph(nodes=nodes_seafood,\n                          edges = mc2_seafood_edges_agg,\n                          directed = TRUE)\n\n\n\n\nShow the code\nseafood_graph\n\n\n# A tbl_graph: 191 nodes and 3337 edges\n#\n# A directed multigraph with 3 components\n#\n# A tibble: 191 × 3\n  id                               shpcountry rcvcountry\n  <chr>                            <chr>      <chr>     \n1 Olas del Mar Worldwide           Oceanus    Oceanus   \n2 Panope Limited Liability Company Vesperanda Oceanus   \n3 hǎi dǎn Corporation Wharf        Marebak    Oceanus   \n4 Sea Breezes GmbH & Co. KG Shark  Oceanus    Oceanus   \n5 Costa de la Felicidad Shipping   Alverossia Oceanus   \n6 Mar del Este CJSC                Merigrad   Oceanus   \n# ℹ 185 more rows\n#\n# A tibble: 3,337 × 6\n   from    to arrivaldate Weight Totalweight hscode\n  <int> <int> <chr>        <int>       <int> <chr> \n1    77    13 2029-08-03       5       76235 160521\n2    77    29 2028-01-02       6       87635 160521\n3    77    29 2030-02-05       7       98075 160521\n# ℹ 3,334 more rows"
  },
  {
    "objectID": "Project/Project_MC2.html#plot-basic-network-graph-cross-fingers",
    "href": "Project/Project_MC2.html#plot-basic-network-graph-cross-fingers",
    "title": "Project_MC2",
    "section": "2.2 Plot basic network graph… cross fingers",
    "text": "2.2 Plot basic network graph… cross fingers\n\n\nShow the code\na <- ggraph(seafood_graph) +  #<<< GAStech_graph is a tbl_graph object\n  geom_edge_link(aes(width=Weight, alpha= 0.2)) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point()\na\n\n\n\n\n\nI have actually already excluded source and target actors with frequency counts of 4 and under. I should not exclude anymore ‘innocent’ actors. Lets use visnetwork to get the id of the two pairs that are still inside there….\nWe first need to rename the edge file first two columns to from and to for visNetwork to be able to regconise them.. .\n\n\nShow the code\nmc2_seafood_edges_agg_vis <- mc2_seafood_edges_agg %>% \n  rename(from = source) %>% \n  rename(to = target)\n\n\nNext, I will rename the shpcountry column to group because i would like visNetwork looks for group column to colour the nodes.\n\n\nShow the code\nnodes_seafood_vis <- nodes_seafood %>% \n  rename(group= shpcountry)\n\n\nThe code chunk below plots in interactive network graph using visNetwork.\n\n\nShow the code\nvisNetwork(nodes_seafood_vis,\n           mc2_seafood_edges_agg_vis) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nThe four isolated actors id are ‘Rift Valley fishery OJSC’, ‘Bujagali Falls Pic Family’, ‘Neptune’s Realm NV Navigation’ and ’Rybachit Sagl and Son’s. Thanks to this interactive graph that I know which nodes to remove from my network graph tmr……"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "This exerise aims to reveal the demographic and financial characteristics of the city of Engagement, using appropriate static and interactive statistical graphics methods. It also requires a user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.\nThe dataset consists of a sample survey of 1000 representative residents that collects data related to their household demographic and spending patterns, among other things. There are primarily two datasets used in this exercise\n\n’FinancialJournal.csv”: Contains 1513635 number of daily transaction records (different categories of income and expenses) over a period of twelve months from March 2022 to February 2023.\n’Particpants.csv” : Contains demographics information like household size, age, education level, interest groups, joviality index and whether each household has kids.\n\nIn this exercise, each dataset will be cleansed separately and then joined by ‘participantID’ as primary key to form the final dataset used for further analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-load-the-required-libraries",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-load-the-required-libraries",
    "title": "Take-home_Ex01",
    "section": "2.1 Install and load the required libraries",
    "text": "2.1 Install and load the required libraries\nThe code chunk below uses pacman::p_load() to check if packages are installed. If they are, they will be launched into R. The packages installed are\n\nplotly: Used for creating interactive web-based graphs.\nknitr: Used for dynamic report generation\npatchwork: Used to combine plots\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nggthemes: Provide additional themes for ggplot2\nggstatsplot: Used for creating graphics with details from statistical tests.\nggdist: Used for visualising distribution and uncertainty\nrstatix: Allows us to perform basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\ngt : starting from a tibble table, customise a table and export in various formats. Most importantly, it works with patch. We will save the tabular results from shapiro test as gt object and export using gtsave() into .png file later.\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots.\n\n\npacman::p_load(plotly, knitr, patchwork, tidyverse, ggthemes,hrbrthemes, ggiraph, ggstatsplot, ggdist, ggridges, colorspace, png, gifski, rstatix, gt)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#import-the-dataset",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#import-the-dataset",
    "title": "Take-home_Ex01",
    "section": "2.2 Import the dataset",
    "text": "2.2 Import the dataset\nThe datasets are imported using tidyverse’s readr::read_csv() function.\n’FinancialJournal.csv” is stored as finance variable.\n\nfinance <- read_csv('data/FinancialJournal.csv')\n\n\n\n# A tibble: 6 × 4\n  participantId timestamp           category  amount\n          <dbl> <dttm>              <chr>      <dbl>\n1             0 2022-03-01 00:00:00 Wage      2473. \n2             0 2022-03-01 00:00:00 Shelter   -555. \n3             0 2022-03-01 00:00:00 Education  -38.0\n4             1 2022-03-01 00:00:00 Wage      2047. \n5             1 2022-03-01 00:00:00 Shelter   -555. \n6             1 2022-03-01 00:00:00 Education  -38.0\n\n\nCheck for empty values in the finance table using the is.na() function.\n\nany(is.na(finance))\n\n[1] FALSE\n\n\n’Particpants.csv” is stored as ptcp variable.\n\nptcp <- read_csv('data/Participants.csv')\n\n\n\n# A tibble: 6 × 7\n  participantId householdSize haveKids   age educationLevel      interestGroup\n          <dbl>         <dbl> <lgl>    <dbl> <chr>               <chr>        \n1             0             3 TRUE        36 HighSchoolOrCollege H            \n2             1             3 TRUE        25 HighSchoolOrCollege B            \n3             2             3 TRUE        35 HighSchoolOrCollege A            \n4             3             3 TRUE        21 HighSchoolOrCollege I            \n5             4             3 TRUE        43 Bachelors           H            \n6             5             3 TRUE        32 HighSchoolOrCollege D            \n# ℹ 1 more variable: joviality <dbl>\n\n\nChecking for empty values in ptcp table using the is.na()` function.\n\nany(is.na(ptcp))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-issues-and-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-issues-and-wrangling",
    "title": "Take-home_Ex01",
    "section": "2.3 Data Issues and wrangling",
    "text": "2.3 Data Issues and wrangling\nI will discuss the issues in the datasets and proposed cleaning methods.\n\n2.3.1 finance dataset issues:\n\nparticipantId should be converted from <dbl> format to <chr> format. It should be a categorical and not numerical data type.\ntimestamp should be converted from <dttm> format to <date> format as I will not be analysing time in this exercise.\nNegative values of amount that belong to the expenses categories should be converted to positive values. The amount will also be rounded to two decimal places.\n\nThe code chunk below does the following:\n\nuse the as.character() function to convert participantId to <chr> format\ncreate a new column month_year by extracting the year and month from the timestamp column using the format() function with the %Y-%m format specifier.\nuse the abs() function to convert negative values amount to positive and round the values to 2 decimal places using the round() function.\n\n\n# Convert participantId to character\nfinance <- finance %>% mutate(participantId = as.character(participantId))\n\n# Extract month and year from timestamp\nfinance <- finance %>% \n  mutate(month_year = format(timestamp, \"%m-%Y\"))\n\n# Transform negative amounts to positive and round to 2 decimal places\nfinance <- finance %>% \n  mutate(amount = abs(amount),\n         amount = round(amount, 2))\n\nA check for duplicates using the duplicated() function reveals that there are 1,113 records of duplicates.\n\nThe duplicated() function to identify the duplicate rows. It returns a logical vector indicating whether each row is a duplicate of a previous row in the data frame. We can then use this logical vector to subset the data frame and show the duplicate rows. The logical vector is stored in a filter duplicated_rows which is used to subset the finance data.\n\n\n# Show duplicate rows\nduplicated_rows <- finance[duplicated(finance),]\nglimpse(duplicated_rows)\n\nRows: 1,113\nColumns: 5\n$ participantId <chr> \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"…\n$ timestamp     <dttm> 2022-03-01, 2022-03-01, 2022-03-01, 2022-03-01, 2022-03…\n$ category      <chr> \"Shelter\", \"Education\", \"Shelter\", \"Education\", \"Shelter…\n$ amount        <dbl> 554.99, 38.01, 554.99, 38.01, 556.55, 12.81, 554.99, 38.…\n$ month_year    <chr> \"03-2022\", \"03-2022\", \"03-2022\", \"03-2022\", \"03-2022\", \"…\n\n\n\nunique() function is used to remove the duplicate rows form finance data\n\n\n# Remove duplicate rows\nfinance <- unique(finance)\n\n\nPerform a final check to verify that there are no more duplicate using any() function\n\n\nany(duplicated(finance))\n\n[1] FALSE\n\n\nThe last thing to do is to create a new column date that is in <date> format using the as.Date function.\n\nthe paste0() function is used to concatenate “01-” with each value in the month_year column. This is because as.Date() requires a complete date in the format “dd-mm-yyyy”\n\nfinance$date <- as.Date(paste0(\"01-\", finance$month_year), format = \"%d-%m-%Y\")\n\n\nOther issues\nWhen the finance dataset is group-by the date variable , it is noticed that the number of distinct participantID who took part in the survey was 1,011 in March 2022 and suddenly reduced to a constant value of 880 from April 2022 onwards. It seems to suggest that there are 131 residents who moved out of the city at the end of March 2022.\nIn the code chunk below:\n\ndataset is group-by date and the distinct count of participantID is generated using n_distinct function\nthe missing dataframe is displayed below using knitr::kable() function\n\n\nmissing_summary <- finance %>%\n  group_by(date) %>% \n  summarise(n_distinct=n_distinct(participantId)) %>% \n  rename(`Number of unique participantId` = n_distinct)\n\nknitr::kable(missing_summary, \"simple\")\n\n\n\n\ndate\nNumber of unique participantId\n\n\n\n\n2022-03-01\n1011\n\n\n2022-04-01\n880\n\n\n2022-05-01\n880\n\n\n2022-06-01\n880\n\n\n2022-07-01\n880\n\n\n2022-08-01\n880\n\n\n2022-09-01\n880\n\n\n2022-10-01\n880\n\n\n2022-11-01\n880\n\n\n2022-12-01\n880\n\n\n2023-01-01\n880\n\n\n2023-02-01\n880\n\n\n\n\n\nSince 11 out of 12 months of records are missing for these 131 residents, we will delete their records from the finance dataset.\nThe code chunk below does the following:\n\nextract the participantIds of residents whose records exists in March 22 but not in all April 22 using the anti-join function\npass the unique participantId column name as an argument to pull() and use the as.vector() function to convert the resulting tibble column to a vector\nresulting dataframe will only contain participantIds that are in ‘2022-03-01’ but not in ‘2022-04-01’ onwards. There are 131 of them.\n\n\nmissing_id <- finance %>%\n  filter(date == as.Date('2022-03-01')) %>% # filter for '2022-03-01' date\n  anti_join(finance %>%\n             filter(date == as.Date('2022-04-01')), # filter for '2022-04-01' date\n             by = 'participantId') %>% # anti-join by 'participantId'\n  select(participantId) %>% \n  distinct(participantId)\n\n# extract participantId column as convert this column to vector.\nmissing_id_vector <- as.vector(pull(missing_id, participantId))\n\nmissing_id_vector \n\n  [1] \"44\"  \"127\" \"142\" \"154\" \"161\" \"256\" \"262\" \"267\" \"279\" \"285\" \"288\" \"298\"\n [13] \"301\" \"346\" \"352\" \"356\" \"380\" \"382\" \"383\" \"384\" \"392\" \"406\" \"407\" \"509\"\n [25] \"510\" \"512\" \"514\" \"523\" \"526\" \"539\" \"541\" \"553\" \"558\" \"567\" \"568\" \"572\"\n [37] \"574\" \"575\" \"577\" \"580\" \"589\" \"595\" \"599\" \"602\" \"603\" \"604\" \"605\" \"611\"\n [49] \"615\" \"617\" \"621\" \"628\" \"629\" \"634\" \"639\" \"641\" \"643\" \"647\" \"653\" \"655\"\n [61] \"657\" \"658\" \"663\" \"668\" \"670\" \"756\" \"757\" \"760\" \"761\" \"762\" \"768\" \"771\"\n [73] \"773\" \"774\" \"780\" \"785\" \"789\" \"790\" \"791\" \"792\" \"793\" \"794\" \"799\" \"802\"\n [85] \"806\" \"808\" \"816\" \"817\" \"818\" \"824\" \"825\" \"827\" \"828\" \"831\" \"832\" \"834\"\n [97] \"839\" \"842\" \"846\" \"847\" \"853\" \"855\" \"856\" \"858\" \"859\" \"860\" \"862\" \"864\"\n[109] \"867\" \"872\" \"875\" \"876\" \"883\" \"884\" \"885\" \"886\" \"887\" \"892\" \"896\" \"897\"\n[121] \"900\" \"901\" \"902\" \"907\" \"909\" \"911\" \"919\" \"920\" \"923\" \"924\" \"925\"\n\n\nNext, we will remove all records of the 131 potentially non-residents from the finance dataset .\nIn the code chunk below:\n\nthe %in% operator is to check if each id value is contained in the missing_id_vector\nthe negation operator ! ensures the resulting filtered data frame will not contain the rows where the id values are in missing_id_vector\n\n\nfinance1 <- finance[!finance$participantId %in% missing_id_vector, ]\nfinance1\n\n# A tibble: 1,509,897 × 6\n   participantId timestamp           category  amount month_year date      \n   <chr>         <dttm>              <chr>      <dbl> <chr>      <date>    \n 1 0             2022-03-01 00:00:00 Wage      2473.  03-2022    2022-03-01\n 2 0             2022-03-01 00:00:00 Shelter    555.  03-2022    2022-03-01\n 3 0             2022-03-01 00:00:00 Education   38.0 03-2022    2022-03-01\n 4 1             2022-03-01 00:00:00 Wage      2047.  03-2022    2022-03-01\n 5 1             2022-03-01 00:00:00 Shelter    555.  03-2022    2022-03-01\n 6 1             2022-03-01 00:00:00 Education   38.0 03-2022    2022-03-01\n 7 2             2022-03-01 00:00:00 Wage      2437.  03-2022    2022-03-01\n 8 2             2022-03-01 00:00:00 Shelter    557.  03-2022    2022-03-01\n 9 2             2022-03-01 00:00:00 Education   12.8 03-2022    2022-03-01\n10 3             2022-03-01 00:00:00 Wage      2367.  03-2022    2022-03-01\n# ℹ 1,509,887 more rows\n\n\nWe will double check that the records of 131 non-residents have been removed from finance1 dataframe.\nIn the code below\n\ndistinct() function to extract the distinct participantId values from finance1\nthe n_distinct() function will count the number of distinct participantId values in the resulting tibble\n\n\nfinance1 %>% \n  distinct(participantId) %>% \n  n_distinct()\n\n[1] 880\n\n\n\n\n2.3.2 ptcp dataset issues:\n\nparticipantId should be converted from <dbl> format to <chr> format\nhouseholdSize should be converted from <dbl> format to <fct> format. It does not make sense to have 2.5 persons.\nage should be converted from <dbl> format to <int> format.\neducationLevel should be converted from <chr> to <fct> . It should also be ordered according to ‘Low’, ‘HighSchoolOrCollege’, ‘Bachelors’ and ‘Graduate’.\n\nThe code chunk below does the following:\n\nas.character and as.factor functions are used to convert participantId to <chr> , householdSize to <fct> and age to <int>.\nfactor(educationLevel, levels=c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\"))) not only converts educationLevel to factor, but also order the values inside.\n\n\n# convert to factor\nptcp <- ptcp %>% mutate(participantId = as.character(participantId))\nptcp <- ptcp %>% mutate(householdSize = as.factor(householdSize))\n\n# Convert educationLevel to factor and order accordingly\nptcp <- ptcp %>% mutate(educationLevel = factor(educationLevel, levels=c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")))\n\n# convert age to int\nptcp <- ptcp %>% mutate(age = as.integer(age))\n\nThe columns format are all in order now.\n\nglimpse(ptcp)\n\nRows: 1,011\nColumns: 7\n$ participantId  <chr> \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",…\n$ householdSize  <fct> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ haveKids       <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ age            <int> 36, 25, 35, 21, 43, 32, 26, 27, 20, 35, 48, 27, 34, 18,…\n$ educationLevel <fct> HighSchoolOrCollege, HighSchoolOrCollege, HighSchoolOrC…\n$ interestGroup  <chr> \"H\", \"B\", \"A\", \"I\", \"H\", \"D\", \"I\", \"A\", \"G\", \"D\", \"D\", …\n$ joviality      <dbl> 0.001626703, 0.328086500, 0.393469590, 0.138063446, 0.8…\n\n\nUse distinct() and n_distinct() to check on the number of unique participantIds in ptcp table.\n\nptcp %>% \n  distinct(participantId) %>% \n  n_distinct()\n\n[1] 1011\n\n\nCurrently, the ptcp table still contain the demographic records of the 131 residents who moved out. Let us remove their records by using similar method used in removing the same records in financial table.\n\nptcp1 <- ptcp[!ptcp$participantId %in% missing_id_vector, ]\n\nptcp1 %>% \n  distinct(participantId) %>% \n  n_distinct()\n\n[1] 880\n\n\nBoth finance1 and ptcp1 tables now contains information about the same number of participantIds.\n\n\n2.3.3 Convert finance1 table to wide format and perform left outer join with ptcp1 table.\nWe will now convert the finance1 dataframe from a long to a wide format. The code chunk below does the following:\n\ngroup the data by participantId , date and category using the group_by function\nuse the sum function to calculate the total monthly amount for each category per participantId per month\nthe pivot_wider function will convert the category column to wide format with total monthly values in the amount column.\n\nfinance1_wide<- finance1 %>%\n  group_by(participantId, date, category) %>%\n  summarise(total_amount = sum(amount)) %>%\n  pivot_wider(names_from = category, values_from = total_amount)\n\n\n\n# A tibble: 10,560 × 8\n# Groups:   participantId, date [10,560]\n   participantId date       Education  Food Recreation Shelter   Wage\n   <chr>         <date>         <dbl> <dbl>      <dbl>   <dbl>  <dbl>\n 1 0             2022-03-01      38.0  268.      349.     555. 11932.\n 2 0             2022-04-01      38.0  266.      219.     555.  8637.\n 3 0             2022-05-01      38.0  265.      383.     555.  9048.\n 4 0             2022-06-01      38.0  257.      466.     555.  9048.\n 5 0             2022-07-01      38.0  270.     1069.     555.  8637.\n 6 0             2022-08-01      38.0  262.      314.     555.  9459.\n 7 0             2022-09-01      38.0  256.      295.     555.  9048.\n 8 0             2022-10-01      38.0  267.       25.0    555.  8637.\n 9 0             2022-11-01      38.0  261       377.     555.  9048.\n10 0             2022-12-01      38.0  266.      357.     555.  9048.\n# ℹ 10,550 more rows\n# ℹ 1 more variable: RentAdjustment <dbl>\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbout finance1_wide table\nfinance_wide is a table that has one row for each unique combination of participantId and month and one column for each unique category from the former finance1 table.\n\n\nThe code chunk below performs a left outer join with finance1_wide table (left) and ptcp1 table (right) with join key participantId.\n\n# left outer join\nfinance1_wide_ptcp1 <- left_join(finance1_wide, ptcp1, by = \"participantId\")\n\nThe first 12 rows of the cleansed finance1_wide_ptcp1 is displayed using knitr::kable() function. It contains 10,560 rows and 14 columns.\n\nknitr::kable(head(finance1_wide_ptcp1,12), \"simple\") \n\n\n\n\nparticipantId\ndate\nEducation\nFood\nRecreation\nShelter\nWage\nRentAdjustment\nhouseholdSize\nhaveKids\nage\neducationLevel\ninterestGroup\njoviality\n\n\n\n\n0\n2022-03-01\n38.01\n268.26\n348.68\n554.99\n11931.95\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-04-01\n38.01\n265.79\n219.42\n554.99\n8636.88\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-05-01\n38.01\n264.54\n382.99\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-06-01\n38.01\n256.90\n465.67\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-07-01\n38.01\n270.13\n1069.48\n554.99\n8636.88\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-08-01\n38.01\n261.76\n314.13\n554.99\n9459.44\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-09-01\n38.01\n256.04\n294.64\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-10-01\n38.01\n266.67\n25.01\n554.99\n8636.88\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-11-01\n38.01\n261.00\n377.41\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2022-12-01\n38.01\n265.98\n356.69\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2023-01-01\n38.01\n264.97\n209.77\n554.99\n9048.16\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267\n\n\n0\n2023-02-01\n38.01\n239.05\n319.93\n554.99\n8225.60\nNA\n3\nTRUE\n36\nHighSchoolOrCollege\nH\n0.0016267"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#wage-and-categories-of-expenses",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#wage-and-categories-of-expenses",
    "title": "Take-home_Ex01",
    "section": "3.1 Wage and categories of expenses",
    "text": "3.1 Wage and categories of expenses\nIn this section, I will explore the dataset from high level and then zoom into interesting patterns (if I can find any =))\n\n3.1.1 Normality assumptions of annual wage\nBefore zooming into wages in March, we will first perform a test to confirm whether wages in March follows the normal distribution.\nH0: The wage does not follow a normal distribution.\nH1: The wage follows a normal distribution.\n\n\nCode\nmarch_records <- finance1_wide_ptcp1 %>% \n               filter(date == as.Date(\"2022-03-01\"))\n\nqq <- ggplot(march_records,\n             aes(sample=Wage)) +\n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  # add plot title\n\nsw_t <- shapiro_test(march_records$Wage) %>% \n  as_tibble() %>% \n  mutate(variable = \"Wage in March\")%>% gt()  #<<< make into a gt format (will give a nice table)  shapiro.test is not used here as it gives output in another format.\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\n\nFrom the Shapiro test , p-value < 0.05 and we have enough statistical evidence to reject the null hypothesis and conclude that Wage in March does not follow the normal distribution.\n\n\n3.1.2 Interactive Line charts of wages by month\nPreparing the data, creating a highlevel dataframe containing the rows date (month) and aggregated columns of expenses and wage only.\n\n\nCode\nhighlevel <- finance1_wide_ptcp1 %>%\n  group_by(date) %>%\n  summarize(Education = round(sum(Education, na.rm = TRUE)),\n                Food = round(sum(Food, na.rm = TRUE)),\n                Recreation = round(sum(Recreation, na.rm = TRUE)),\n                Shelter = round(sum(Shelter, na.rm = TRUE)),\n                Wage = round(sum(Wage, na.rm = TRUE)),\n                RentAdjustment = round(sum(RentAdjustment, na.rm = TRUE)),\n                ExpenseP = sum(Education, Food, Recreation, Shelter),  #<<<\n                Income = sum(Wage, RentAdjustment),                    #<<<\n                Saving = Income - ExpenseP,                            #<<<\n                Expense = ExpenseP * -1                                #<<<\n                )\nhead(highlevel,5)\n\n\n# A tibble: 5 × 11\n  date       Education   Food Recreation Shelter    Wage RentAdjustment ExpenseP\n  <date>         <dbl>  <dbl>      <dbl>   <dbl>   <dbl>          <dbl>    <dbl>\n1 2022-03-01     11424 320126     649580  631623 6068610          53504  1612753\n2 2022-04-01     11424 304282     389688  559919 3468757           1429  1265313\n3 2022-05-01     11424 313538     336413  558451 3623068              0  1219826\n4 2022-06-01     11424 302893     314804  558451 3608883              0  1187572\n5 2022-07-01     11424 313803     329608  558451 3485799              0  1213286\n# ℹ 3 more variables: Income <dbl>, Saving <dbl>, Expense <dbl>\n\n\nAn interactive line chart showing us an overview of total income across months.\n\n\nCode\nlibrary(scales)\n\nq1<-highlevel %>%\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = Wage, color = \"Red\", linetype = \"Wage\"), size = 1) +\n\n  \n  # annotating the plot\n  geom_text(aes(x=as.Date(\"2022-04-01\"),\n                y=6000000,\n                label=\"High wages \\nobserved in \\nMarch\"), \n            hjust=1, vjust=1, color='black', size=2.5) +\n  geom_text(aes(x=as.Date(\"2022-12-01\"), y=3800000, label=\"Wage\"),\n            hjust=1, vjust=1,color='red', size=2.5) +\n\n\n  # scale control\n  labs(x = \"Month\", y = \"Amount\") +\n  scale_x_date(date_breaks = '1 month',date_labels = \"%b %Y\") +\n  scale_y_continuous(limits = c(0, 6500000), breaks=seq(0, 6500000, 1000000),\n                     labels= comma) +\n  \n  theme_light(base_size = 12) +\n  theme(axis.title = element_text(size = 10 , face = \"bold\"),\n        axis.text = element_text(size = 10),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.line = element_line(size = 1),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_line(colour='black'),\n        panel.border = element_blank(),\n        legend.position = \"none\",\n        legend.title = element_blank()) +\n\n\n  labs(title= 'Cumulative Income across Months',\n       x='Month',\n       y='Amount')\n\nggplotly(q1,tooltip = c('labels','x','y'))\n\n\n\n\n\n\n\n\n3.1.3 Boxplot, One-way Anova and Error Plot of wage across months\nNow we willl examine the distribution of wages across the months by education levels using boxplots.\n\nBoxplotsOne way Anova plotError plots\n\n\nThe boxplot shows the distribution of wages of the residents across all the months. The red dot represents the median wage of that month. Wage is much higher in March and possible reasons could be due to Harvest / Bonus month. Are the medians of the month wage significantly different from one another? (See next tab)\n\n\nCode\nlibrary(lubridate)\n\ndf_wage_edu_month <- finance1_wide_ptcp1 %>%\n  mutate(month = month(date))\n\ndf_wage_edu_month$month <- factor(month(finance1_wide_ptcp1$date), \n                                  levels = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2), \n                                  labels = c(\"Mar 22\", \"Apr 22\", \"May 22\", \"Jun 22\", \"Jul 22\", \"Aug 22\", \"Sep 22\", \"Oct 22\", \"Nov 22\", \"Dec 22\", \"Jan 23\", \"Feb 23\"))\n\n\nggplot(df_wage_edu_month,\n       aes(x = month, y = Wage)) +\n  geom_boxplot(aes(fill = educationLevel)) +\n  stat_summary(fun.y = \"median\", geom = \"point\", color = \"red\", size = 2) +\n  labs(x = \"Month\", y = \"Wage\", fill = \"Education Level\", title='Wage Across education level by month') +\n  scale_fill_brewer(palette=\"RdBu\") + \n  theme_minimal() +\n  theme(legend.key.size = unit(0.5,'cm'),\n        legend.position=\"bottom\",\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        axis.title = element_text(size = 11 , face = \"bold\"),\n        axis.text = element_text(size = 10),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nSince the wages in March does not follow a normal distribution, we will use a non parametric test for testing. (As long as one of the wage distribution across the months do not follow normal distribution, choose non-parametric test).\nThe plot below shows us the pairs of months where wage are significantly different. The plot shows that Median Wage of March is significantly different from all the wages of the other months.\nCheck the next tab to see the error plots.\n\n\nCode\nggbetweenstats(data = df_wage_edu_month, x = month, y = Wage,\n               xlab = \"Month\", ylab = \"Wage\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Wage across Months\") +\n # scale_y_continuous(limits = c(0, 300000)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\n\n\nAs we are analyzing survey data, each of our sample mean could vary from the actual population mean. Thus we have to visualize the margin of error. The higher the CI, the higher the margin of error.\n95% and 99% confidence intervals are constructed for the median wage for each month.\nNote: A confidence level of 95% means the true result will be within the error bar range 95 times out of 100 sampling tries.\n\n\nCode\ndf_wage_edu_month %>%\n  ggplot(aes(x = month, y = Wage)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of median wage\",\n    subtitle = \"Median Point + Multiple-interval plot, 95% and 99%\",\n    x = \"Months\", y = \"Wage\") +\n  \n  theme_ipsum() +\n  \n  theme(axis.title = element_text(size = 10 , face = \"bold\"),\n        axis.text = element_text(size = 10),\n        axis.title.y=element_text(angle = 0, \n                                  vjust=0.9, \n                                  size = 10, \n                                  face='bold'),\n        axis.title.x=element_text(size = 10,\n                                   face='bold'),\n        axis.text.x = element_text(angle = 45,\n                                   hjust = 1),\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\n\n\n3.1.4 Interactive Line charts of expenditures by month\nDesign considerations:\nInstead of combining Education, Recreation, Food and Shelter expense in one chart, I have plotted them on one chart each with different Y axis range. This will be enable us to visualise variability of amount across categories clearly.\n\n\nCode\ns <- highlevel %>%\n  plot_ly(x = ~date, y = ~Shelter, type = 'scatter', mode = 'lines', name='Shelter') %>%\n  layout(\n         xaxis = list(title = \"Date\"), \n         yaxis = list(title = \"Shelter\"),\n         plot_bgcolor = \"#e5ecf6\")\n\n\ne <- highlevel %>%\n  plot_ly(x = ~date, y = ~Education, type = 'scatter', mode = 'lines', name='Education') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Education\"))\n\nf <- highlevel %>%\n  plot_ly(x = ~date, y = ~Food, type = 'scatter', mode = 'lines', name='Food') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Food\"))\n\nr <- highlevel %>%\n  plot_ly(x = ~date, y = ~Recreation, type = 'scatter', mode = 'lines', name='Recreation') %>%\n  layout(xaxis = list(title = \"Date\"), yaxis = list(title = \"Recreation\"))\n\n#subplot(s, e, f, r,titleX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>% layout(title = \"Custom Hover Text\")\n\nsubplot(s, e, f, r, shareX=TRUE, titleY=TRUE, nrows = 2, margin = 0.1) %>%\n  layout(title = \"<b>Annual expenses by month<b>\",\n         plot_bgcolor='#e5ecf6', \n         xaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'), \n         yaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'))\n\n\n\n\n\n\nFrom all the plots above, March seems to be an exciting month where there is several anomalies observed. There are unusual spikes in wage, recreational and shelter spending.\nNext, we will plot a coordinated dotplot to studying the distribution of expenses. Since we have the daily trasnaction data of participant, which we can aggregate to find the annual spending for each type of expense.\nFirst prepare the annual dataframe that contains participantId, their demographics data (e.g. educationLevel, haveKids etc.. ) and annual expense amount (e.g. Education, Shelter etc..) . The first 5 rows of the annual df is displayed using knitr::kable() function. It contains 880 rows and 12 columns.\n\n\nCode\nannual <- finance1_wide_ptcp1 %>% \n  group_by(participantId, householdSize, haveKids, educationLevel, interestGroup, joviality) %>% \n  summarize(Education = sum(Education, na.rm = TRUE),\n            Food = sum(Food, na.rm = TRUE),\n            Recreation = sum(Recreation, na.rm = TRUE),\n            Shelter = sum(Shelter, na.rm = TRUE),\n            Wage = sum(Wage, na.rm = TRUE),\n            RentAdjustment = sum(RentAdjustment, na.rm = TRUE)) \n\nknitr::kable(head(annual,3), \"simple\")\n\n\n\n\n\nparticipantId\nhouseholdSize\nhaveKids\neducationLevel\ninterestGroup\njoviality\nEducation\nFood\nRecreation\nShelter\nWage\nRentAdjustment\n\n\n\n\n0\n3\nTRUE\nHighSchoolOrCollege\nH\n0.0016267\n456.12\n3141.09\n4383.82\n6659.88\n109816.59\n0\n\n\n1\n3\nTRUE\nHighSchoolOrCollege\nB\n0.3280865\n456.12\n3167.93\n6637.42\n6659.88\n96374.57\n0\n\n\n10\n3\nTRUE\nHighSchoolOrCollege\nD\n0.5571760\n153.72\n4739.39\n3087.83\n6730.80\n79304.72\n0\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the na.rm = TRUE argument is used in the sum function to handle missing values in the columns during aggregation.\n\n\n\n\n3.1.5 Coordinated interactive dotplot of amount spent across categories\nHovering the cursor over the dots will show whether a participant have kids (True) or do not have kids (False). A participant will incur educational expenses when they have kids. Also observed that those with kids are among those who spend slightly more in shelter.\n\n\nCode\nsdot <- ggplot(data=annual,\n            aes(x=Shelter)) +\n  geom_dotplot_interactive(aes(tooltip = haveKids,  #<<<\n                               data_id = haveKids),  #<<<\n                           stackgroups = TRUE,\n                           #binwidth = 2500,\n                           method = \"histodot\",\n                           dotsize= 0.2) +\n  scale_y_continuous(NULL,\n                     breaks= NULL) +  #null to suppress axis labels\n \n  theme_bw()  +\n  labs(title= 'Amount-spent distribution across category') +\n  theme(plot.title = element_text(size = 13,\n                                  face='bold'))\n\nedot <- ggplot(data=annual,\n            aes(x=Education)) +\n  geom_dotplot_interactive(aes(tooltip = haveKids,\n                               data_id = haveKids), \n                           stackgroups = TRUE,\n                           #binwidth = 2500,\n                           method = \"histodot\",\n                           dotsize= 0.2) +\n  scale_y_continuous(NULL,\n                     breaks= NULL) +\n \n  theme_bw()\n\nfdot <- ggplot(data=annual,\n            aes(x=Food)) +\n  geom_dotplot_interactive(aes(tooltip = haveKids,\n                               data_id = haveKids), \n                           stackgroups = TRUE,\n                           #binwidth = 2500,\n                           method = \"histodot\",\n                           dotsize= 0.2) +\n  scale_y_continuous(NULL,\n                     breaks= NULL) +\n \n  theme_bw()\n\nrdot <- ggplot(data=annual,\n            aes(x=Recreation)) +\n  geom_dotplot_interactive(aes(tooltip = haveKids,\n                               data_id = haveKids), \n                           stackgroups = TRUE,\n                           #binwidth = 2500,\n                           method = \"histodot\",\n                           dotsize= 0.2) +\n  scale_y_continuous(NULL,\n                     breaks= NULL) +\n \n  theme_bw()\n\ngirafe(code = print(sdot + edot + fdot + rdot), \n       width_svg = 10,\n       height_svg = 10 *0.618,\n       options = list(                          #<<<\n         opts_hover(css='fill: blue;'),      #<<<\n         opts_hover_inv(css = 'opacity: 0.2;')  #<<<\n         )\n       )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#demographics---finance-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#demographics---finance-analysis",
    "title": "Take-home_Ex01",
    "section": "3.2 Demographics - Finance Analysis",
    "text": "3.2 Demographics - Finance Analysis\n\n3.2.1 Distribution of Annual wage across Education Levels\n\n3.2.1.1 Ridgeline plot\nWe will use the annual dataframe to visualise the distribution of annual wage across education levels.\n\n\nCode\nggplot(annual, \n       aes(x = Wage, \n           y = educationLevel,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Wage\",\n                       option = \"D\") +\n  scale_x_continuous(\n    name = \"Annual Wage\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n3.2.1.4 One- way Anova plot\nFirst, check whether wages across education levels conforms to normality.\n\n\nCode\nlow_records <- annual %>% \n               filter(educationLevel=='Low')\nqq <- ggplot(low_records,\n             aes(sample=Wage)) + \n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  \n\nsw_t <- shapiro_test(low_records$Wage) %>% \n  as_tibble() %>% \n  mutate(variable = \"Wage for 'Low' EducationLevel\") %>% gt()  \n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\n\nSince p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and conclude that wages of the participants belong to ’Low” educationLevel do not follow normal distribution. Hence I will choose a non parametric test to compare whether there is significant difference in the median of wage between education levels.\n\n\nCode\nggbetweenstats(data = annual, x = educationLevel, y = Wage,\n               xlab = \"Education level\", ylab = \"Annual Wage\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Annual Wage across Education Levels\") +\n  scale_y_continuous(limits = c(0, 300000)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\nFor 4 categories of education levels, we can have a total of 4C2 = k(k-1)/2 (=6) possible combinations of pairs.\nFrom the results, all six pairwise comparison p-values are less than 0.05 and thus we can reject the null hypothesis and conclude that the median wages across all different educational levels are all different from one another.\n\n\n3.2.1.5 Error plots\nThe error bar for mean Median Wage is the longest for Graduate education level and this could be due to outliers in this category.\n\n\nCode\nannual %>%\n  ggplot(aes(x = educationLevel, y = Wage)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot, 95% and 99%\",\n    x = \"Education  level\", y = \"Wage\") +\n  \n  theme_ipsum() +\n  \n  theme(axis.title.y=element_text(angle = 0, \n                                  vjust=0.9, \n                                  size = 10, \n                                  face='bold'),\n        axis.title.x=element_text(size = 10,\n                                   face='bold'),\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\n3.2.2 Distribution of Joviality Index across Education levels\n\n3.2.2.1 Ridgeline plot\n\n\nCode\nggplot(annual, \n       aes(x = joviality, \n           y = educationLevel,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Joviality\",\n                       option = \"D\") +\n  scale_x_continuous(\n    name = \"Joviality\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n3.2.2.2 Normality check on Joviality Index\nPerform a test to confirm whether joviality index follows the normal distribution.\n\n\nCode\nqq <- ggplot(low_records,\n             aes(sample=joviality)) + \n  \n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"QQ plot with Shapiro-Wilk test results\")  \n\nsw_t <- shapiro_test(low_records$joviality) %>% \n  as_tibble() %>% \n  mutate(variable = \"Joviality Index of 'Low' educationlevel\")%>% gt()  \n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\n\nP-value is less than 0.05. We have enough statistical evidence to reject the null hypothesis that joviality index of ‘Low’ Educationlevel follows normal distribution. Use a non parametric test below to test for difference in median of joviality index across education levels.\n\n\n3.2.2.2 One way Anova plot\nFrom the results, only 2 pairwise comparison p-values <0.05 and thus we can reject the null hypothesis for (High School & Graduate) and (High School & Bachelors) and conclude that the median Joviality index for these two pairs of education levels are different.\n\n\nCode\nggbetweenstats(data = annual, x = educationLevel, y = joviality,\n               xlab = \"Education level\", ylab = \"Joviality Index\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Joviality index across Education Levels\") +\n  scale_y_continuous(limits = c(0, 2)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\n\n\n3.2.2.3 Error Plot\n\n\nCode\nannual %>%\n  ggplot(aes(x = educationLevel, y = joviality)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(.width = c(0.95,0.99),\n  .point = median,\n  .interval = qi,\n  aes(interval_color=stat(level)),\n  show.legend = FALSE) +\n  \n  #Defining the color of the intervals \n  scale_color_manual(\n    values = c(\"blue\", \"darkblue\"),\n    aesthetics = \"interval_color\") +\n  \n  #Title, subtitle, and caption\n  labs(\n    title = \"Visualising confidence intervals of median joviality index\",\n    subtitle = \"Median Point + Multiple-interval plot, 95% and 99%\",\n    x = \"Education  level\", y = \"Joviality Index\") +\n  \n  theme_ipsum() +\n  \n  theme(axis.title.y=element_text(angle = 0, \n                                  vjust=0.9, \n                                  size = 10, \n                                  face='bold'),\n        axis.title.x=element_text(size = 10,\n                                   face='bold'),\n        plot.title = element_text(size = 12,\n                                  face='bold'),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank())"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#correlation-between-annual-shelter-cost-and-annual-wage",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#correlation-between-annual-shelter-cost-and-annual-wage",
    "title": "Take-home_Ex01",
    "section": "3.3 Correlation between Annual Shelter cost and Annual Wage",
    "text": "3.3 Correlation between Annual Shelter cost and Annual Wage\nIn this section, we will investigate if people who earn more will also spend more on shelter. We will also subset the data across education levels.\nWe will use the non-parametric Spearman correlation analysis instead of Pearson correlation since the wage data is not normally distributed.\n\n\nCode\nlow_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Low\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Low Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n\nhigh_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"HighSchoolOrCollege\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"High Sch Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n\nbac_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Bachelors\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000), \n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000), \n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Degree Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\ngrad_correl <- ggscatterstats(data = annual |> filter(educationLevel == \"Graduate\"), \n                           x = Wage, y = Shelter,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 70000),\n                     breaks=seq(0, 70000, 10000), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 20000),\n                     breaks=seq(0, 20000, 5000), \n                     labels= comma) +\n  \n  labs(title = \"Graduate Education Status\", \n       x = \"Annual Wage\", y = \"Annual Shelter fee\") \n\n\n# combining plots using patchwork\np_correl <- (low_correl + high_correl) / (bac_correl + grad_correl) #+ plot_spacer() + plot_spacer()\np_correl + plot_annotation(title = \"Correlation between Shelter spending and Annual Wage\", \n                           theme = theme(plot.title = element_text(size = 18),\n                                         plot.subtitle = element_text(size = 12))) + plot_layout(heights = c(2,2))\n\n\n\n\n\nH0: There is no [monotonic] association between the annual shelter fees and wage.\nH1: There is association between the annual shelter fees and wage.\nFrom the plots above, there are no strong correlation values above 0.7. only the ‘low’ and ‘degree’ education group showed p-values less than 0.05 which shows that there is a significant association between shelter fee and wage. However, the correlation is weak between shelter spending and wage.\n\n\n\n\n\n\nNote\n\n\n\nSpearman correlation\nThe Spearman correlation is not a linear correlation of the data, but a linear correlation of a transformed version of the data -- specifically, the correlation of the rank-transformed data. Do not be mislead by the slope direction."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/horizon chart.html",
    "href": "Take-home_Ex/Take-home_Ex02/horizon chart.html",
    "title": "gghorizon",
    "section": "",
    "text": "Show the code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph,\n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts,knitr,plotly, \n               ggHoriPlot, ggthemes,patchwork)\n\n\n\n\nShow the code\nutils::data(climate_CPH)\n\n\n\n\nShow the code\nglimpse(climate_CPH)\n\n\nRows: 9,132\nColumns: 9\n$ Region         <chr> \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Europe\", \"Euro…\n$ Country        <chr> \"Denmark\", \"Denmark\", \"Denmark\", \"Denmark\", \"Denmark\", …\n$ State          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ City           <chr> \"Copenhagen\", \"Copenhagen\", \"Copenhagen\", \"Copenhagen\",…\n$ Month          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Day            <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ Year           <dbl> 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1…\n$ AvgTemperature <dbl> 1.2222222, -0.8333333, -4.0000000, -3.0555556, -4.83333…\n$ date_mine      <date> 2021-01-01, 2021-01-02, 2021-01-03, 2021-01-04, 2021-0…\n\n\n\nThe mutate() function is used to create a new column named “outlier” in the climate_CPH dataframe.\nThe between() function is applied to the “AvgTemperature” column, checking if each value falls within a certain range.\nThe range is defined using the lower and upper cutpoints. The lower cutpoint is calculated as the first quartile (25th percentile) of the “AvgTemperature” column minus 1.5 times the interquartile range (IQR), while the upper cutpoint is calculated as the third quartile (75th percentile) plus 1.5 times the IQR. The quantile() function is used to calculate the quartiles, and the IQR() function is used to calculate the interquartile range.\nThe resulting logical values are stored in the “outlier” column.\nFinally, the filter() function is used to keep only the rows where the “outlier” column has a value of TRUE, effectively filtering out the rows that do not meet the outlier criteria based on the “AvgTemperature” values.\n\n\n\nShow the code\ncutpoints <- climate_CPH  %>% \n  mutate(\n    outlier = between(\n      AvgTemperature, \n      quantile(AvgTemperature, 0.25, na.rm=T)-\n        1.5*IQR(AvgTemperature, na.rm=T),\n      quantile(AvgTemperature, 0.75, na.rm=T)+\n        1.5*IQR(AvgTemperature, na.rm=T))) %>% \n  filter(outlier)\n\n\n\nThe sum() function is used to calculate the sum of the minimum and maximum values of the “AvgTemperature” column in the cutpoints dataframe. The range() function returns a vector containing the minimum and maximum values of the specified column. The result is divided by 2 to obtain the midpoint of the range. This midpoint is stored in the variable ori.\nThe seq() function is used to generate a sequence of values. It takes three arguments: the start value, the end value, and the length of the sequence. In this case, the start value is the minimum value of the “AvgTemperature” column, and the end value is the maximum value of the “AvgTemperature” column. The length of the sequence is set to 7. The [-4] at the end of the sequence generation excludes the fourth element from the sequence. The resulting sequence of values is stored in the variable sca.\n\nOverall, the code calculates the midpoint of the range of values in the “AvgTemperature” column and generates a sequence of values based on the minimum and maximum values in the same column. The generated sequence is used for further analysis or plotting purposes.\n\n\nShow the code\nori <- sum(range(cutpoints$AvgTemperature))/2\n\nsca <- seq(range(cutpoints$AvgTemperature)[1], \n           range(cutpoints$AvgTemperature)[2], \n           length.out = 7)[-4]\n\n\n\n\nShow the code\nround(ori, 2) # The origin\n\n\n[1] 6.58\n\n\nShow the code\n#> [1] 6.58\n\nround(sca, 2) # The horizon scale cutpoints\n\n\n[1] -12.11  -5.88   0.35  12.81  19.05  25.28\n\n\nShow the code\n#> [1] -12.11  -5.88   0.35  12.81  19.05  25.28\n\n\nPlotting the horizon\n\nThe ggplot() function initializes the plot.\nThe geom_horizon() function creates the horizon plot. It uses the date_mine column as the x-axis, the AvgTemperature column as the y-axis, and the ..Cutpoints.. variable for filling the horizons.\nThe origin parameter sets the origin of the horizon plot to the value stored in the variable ori.\nThe horizonscale parameter sets the scale of the horizons using the values stored in the variable sca.\nThe scale_fill_hcl() function sets the color palette for the filled horizons. It uses the ‘RdBu’ palette and reverses the order of the colors with reverse = T.\nThe facet_grid() function creates a grid of facets (small multiples) based on the Year column.\nThe theme_few() function applies a pre-defined minimalistic theme to the plot.\nThe theme() function is used to modify specific aspects of the plot’s appearance. It removes vertical panel spacing, adjusts the size, angle, and justification of y-axis strip text, removes y-axis text, title, and ticks, and removes the panel border.\nThe scale_x_date() function is used to customize the x-axis. It sets the expand parameter to c(0,0) to remove padding, sets the date breaks to “1 month”, and sets the date labels to “%b” for abbreviated month names.\nThe xlab() function sets the label for the x-axis.\nThe ggtitle() function sets the plot title and subtitle.\n\n\n\nShow the code\nclimate_CPH %>% ggplot() +\n  geom_horizon(aes(date_mine, \n                   AvgTemperature,\n                   fill = ..Cutpoints..), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(Year~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y=unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand=c(0,0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Average daily temperature in Copenhagen', \n          'from 1995 to 2019')\n\n\n\n\n\n\n2 Trying it on nodes_seafood_vis\n\n\nShow the code\nmc2_seafood_edges_agg_vis<- readRDS(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc2_seafood_edges_agg_vis.rds\")\n\n\n\n\nShow the code\nglimpse(mc2_seafood_edges_agg_vis)\n\n\nRows: 2,054\nColumns: 12\n$ from        <chr> \"Adair Salmon ОАО Family\", \"Adair Salmon ОАО Family\", \"Ada…\n$ to          <chr> \"Panope Limited Liability Company\", \"Panope Limited Liabil…\n$ arrivaldate <date> 2032-12-25, 2033-01-17, 2033-01-11, 2033-12-22, 2033-12-2…\n$ Weight      <int> 6, 8, 7, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 9, 8, 6, 7, 9, 6, 6…\n$ Totalweight <int> 143655, 180145, 125925, 155265, 155250, 155295, 216095, 18…\n$ hscode      <chr> \"160414\", \"160414\", \"160414\", \"160414\", \"160414\", \"160414\"…\n$ year        <dbl> 2032, 2033, 2033, 2033, 2033, 2034, 2033, 2033, 2033, 2033…\n$ month       <dbl> 12, 1, 1, 12, 12, 1, 3, 4, 7, 8, 9, 9, 9, 10, 10, 1, 1, 2,…\n$ day         <int> 25, 17, 11, 22, 25, 2, 18, 16, 24, 28, 10, 19, 25, 2, 6, 2…\n$ weekday     <ord> Saturday, Monday, Tuesday, Thursday, Sunday, Monday, Frida…\n$ weeknumber  <dbl> 52, 3, 2, 51, 51, 1, 11, 15, 29, 34, 36, 38, 38, 39, 40, 3…\n$ title       <chr> \"Total Weight =  143655 \\n HSCODE = 160414\", \"Total Weight…\n\n\n\n\nShow the code\nmc2_seafood_edges_agg_vis %>%  group_by(from) %>%  summarise(Allweight = sum(Weight)) %>%\n  arrange(desc(Allweight))\n\n\n# A tibble: 76 × 2\n   from                                           Allweight\n   <chr>                                              <int>\n 1 \"Playa del Tesoro OJSC\"                             6139\n 2 \"Estrella de la Costa SRL\"                          2797\n 3 \"OceanicOrigin Foods Co Consulting\"                 1629\n 4 \"Náutica del Sol Brothers\"                           816\n 5 \"Beachcomber's Bounty Sea spray\"                     557\n 6 \"Mar de la Aventura Limited Liability Company\"       544\n 7 \"Shou gan  Oyj Overseas\"                             483\n 8 \"Diao er  Limited Liability Company\"                 425\n 9 \"Oceanfront Oasis Company Green \"                    390\n10 \"Gujarat   Tide NV Solutions\"                        380\n# ℹ 66 more rows\n\n\n\n\nShow the code\nplaya <- mc2_seafood_edges_agg_vis %>%\n  filter(from == \"Playa del Tesoro OJSC\")\n\n\n\n\nShow the code\ncutpoints_playa <- playa  %>% \n  mutate(\n    outlier = between(\n      Weight, \n      quantile(Weight, 0.25, na.rm=T)-\n        1.5*IQR(Weight, na.rm=T),\n      quantile(Weight, 0.75, na.rm=T)+\n        1.5*IQR(Weight, na.rm=T))) %>% \n  filter(outlier)\n\n\n\n\nShow the code\nori <- sum(range(cutpoints_playa$Weight))/2\n\nsca <- seq(range(cutpoints_playa$Weight)[1], \n           range(cutpoints_playa$Weight)[2], \n           length.out = 7)[-4]\n\n\nFirst we have to fix the arrivaldate column such that all the year value are the same (for the purpose for plotting horizon chart later). All the year values will get a default ‘2021’\n\n\nShow the code\nplaya$mine_date <- sprintf(\"2021-%s-%s\", substr(playa$arrivaldate, 6, 7), substr(playa$arrivaldate, 9, 10))\nplaya$mine_date <- as.Date(playa$mine_date)\n\n\n\n\nShow the code\nplaya %>% ggplot() +\n  geom_horizon(aes(mine_date, \n                   Weight,\n                   fill = ..Cutpoints..), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(year~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y=unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand=c(0,0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ggtitle('Weight of goods trade by Playa del Tesoro OJSC from 2028 to 2034')\n\n\n\n\n\n\n\nShow the code\nasd <-playa %>% ggplot() +\n  geom_horizon(aes(arrivaldate, \n                   Weight,\n                   fill = ..Cutpoints..), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  #facet_grid(year~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y=unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand=c(0,0), \n               date_breaks = \"6 month\",\n               date_labels = \"%b\") +\n  xlab('Date') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nFOR ENTIRE except playa\n\n\nShow the code\ntop10_outdeg <-subset(mc2_seafood_edges_agg_vis, from %in% c('Playa del Tesoro OJSC', 'Estrella de la Costa SRL', 'OceanicOrigin Foods Co Consulting', 'Náutica del Sol Brothers', 'Beachcomber\\'s Bounty Sea spray', 'Mar de la Aventura Limited Liability Company', 'Shou gan  Oyj Overseas', 'Diao er  Limited Liability Company', 'Oceanfront Oasis Company Green ', 'Gujarat   Tide NV Solutions'))\n\n\n\n\nShow the code\ncutpoints_top10_outdeg <- top10_outdeg  %>% \n  mutate(\n    outlier = between(\n      Weight, \n      quantile(Weight, 0.25, na.rm=T)-\n        1.5*IQR(Weight, na.rm=T),\n      quantile(Weight, 0.75, na.rm=T)+\n        1.5*IQR(Weight, na.rm=T))) %>% \n  filter(outlier)\n\n\n\n\nShow the code\nori <- sum(range(top10_outdeg$Weight))/2\n\nsca <- seq(range(top10_outdeg$Weight)[1], \n           range(top10_outdeg$Weight)[2], \n           length.out = 7)[-4]\n\n\n\n\nShow the code\ntop10_outdeg$mine_date <- sprintf(\"2021-%s-%s\", substr(top10_outdeg$arrivaldate, 6, 7), substr(top10_outdeg$arrivaldate, 9, 10))\n\ntop10_outdeg$mine_date <- as.Date(top10_outdeg$mine_date)\n\n\n\n\nShow the code\ntop10_outdeg %>% ggplot() +\n  geom_horizon(aes(arrivaldate, \n                   Weight,\n                   fill = ..Cutpoints..), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(from~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y=unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand=c(0,0), \n               date_breaks = \"6 month\",\n               date_labels = \"%b\") +\n  xlab('Date') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nTRYING FOR DISCONNECTED COMPONENETS\n\n\nShow the code\nmc2_seafood_edges_agg_vis_with_disconnected<- readRDS(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc2_seafood_edges_agg_vis_withdisconnected.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "Note\n\n\n\nEdge data should be organised as such: (can use dplyr methods)\nFirst column: Source id (FK to Node second column) - compulsory\nSecond column: Target id (FK to Node second column) - compulsory\nNode data\nFirst column: ID (contains all the distinct values of source and target in Edge data) - compulsory\n\nNodes present in edge data must exists in ID of node data, must not have missing in node ID.\n\nSecond column: Label (only need if Id are all integers)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-dictionary",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-dictionary",
    "title": "Take-home_Ex02",
    "section": "1.1 Data dictionary",
    "text": "1.1 Data dictionary\nNode Attributes:\n\nid -- Name of the company that originated (or received) the shipment\nshpcountry -- Country the company most often associated with when shipping\nrcvcountry -- Country the company most often associated with when receiving\ndataset -- Always ‘MC2’\n\nEdge Attributes:\n\narrivaldate -- Date the shipment arrived at port in YYYY-MM-DD format.\nhscode -- Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu -- Customs-declared value of the total shipment, in Oceanus\nMonetary Units (OMU)\nvolumeteu -- The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required. (Actual number of containers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg -- The weight of the shipment in kilograms (if known)\ndataset -- Always ‘MC2’\ntype -- Always ‘shipment’ for MC2\ngenerated_by -- Name of the program that generated the edge. (Only found on ‘bundle’ records.)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-the-datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-the-datasets",
    "title": "Take-home_Ex02",
    "section": "1.2 Importing the datasets",
    "text": "1.2 Importing the datasets\nImport libraries\nThe new libraries used today are :\n\njsonlite to import json file\n\n\n\nShow the code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph,\n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts,knitr,plotly, \n               ggHoriPlot, ggthemes,hrbrthemes,treemap,patchwork, ggiraph,\n               ggstatsplot)\n\n\n\n\nShow the code\nMC2 <- jsonlite::fromJSON(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc2_challenge_graph.json\")\n\n\nPull out the nodes and edge data and save them as tibble data frames.\n\n\nShow the code\nMC2_nodes <- as_tibble(MC2$nodes) %>% \n  select(id,shpcountry,rcvcountry)\n\n\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\nRearranging the columns in edge file as we require source and target columns to be the first two columns.\n\n\nShow the code\nMC2_edges <- as_tibble(MC2$links) %>% \n  select(source,target,arrivaldate,hscode,valueofgoods_omu,volumeteu,weightkg,valueofgoodsusd)  \n\n\n\n\nRows: 5,464,378\nColumns: 8\n$ source           <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son…\n$ target           <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica M…\n$ arrivaldate      <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23…\n$ hscode           <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"47…\n$ valueofgoods_omu <dbl> 141015, 141015, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ volumeteu        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weightkg         <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, …\n$ valueofgoodsusd  <dbl> NA, NA, NA, NA, NA, NA, 87110, 188140, NA, 221110, 58…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lets-check-for-duplicates",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#lets-check-for-duplicates",
    "title": "Take-home_Ex02",
    "section": "2.1 Lets check for duplicates",
    "text": "2.1 Lets check for duplicates\nFor MC2_nodes dataframe:\nThere are no duplicated nodes, which is great.\n\n\nShow the code\n# check for any duplicates\nany(duplicated(MC2_nodes))\n\n\nFor MC2_edges dataframe:\nThere are about 155291 records (2% out of total records) that are duplicated.\n\n\nShow the code\n#duplicated only\n# print(any(duplicated(MC2_edges)))\nMC2_edges_dup <- MC2_edges[duplicated(MC2_edges), ]\nprint(nrow(MC2_edges_dup))\n\n\n[1] 155291\n\n\nWe will drop the duplicates.\n\n\nShow the code\n# Drop duplicate rows from the dataframe\nMC2_edges_no_dup <- MC2_edges[!duplicated(MC2_edges), ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#check-for-null-values",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#check-for-null-values",
    "title": "Take-home_Ex02",
    "section": "2.2 Check for null values",
    "text": "2.2 Check for null values\nCheck whether each column in MC2_nodes and MC2_edges contains null and prints the percentage of null for each column.\nFor MC2_nodes dataframe:\nThere are no null values in the id column of Nodes file, which is great.\n\n\nShow the code\n# Check for null values in each column\nnull_counts_nodes <- sapply(MC2_nodes, function(x) sum(is.null(x) | is.na(x)))\n\n# Calculate the percentage of null values for each column\nnull_percentages_nodes <- null_counts_nodes / nrow(MC2_nodes) * 100\n\n# Display the results\n#print(null_percentages)\n\nknitr::kable(null_percentages_nodes, \"simple\", col.names = c(\"Null Percentage\"))\n\n\n\n\n\nNull Percenta\nge\n\n\n\n\nid\n0.00000\n\n\nshpcountry\n64.66624\n\n\nrcvcountry\n8.41335\n\n\n\n\n\nFor MC2_edges dataframe:\nAs there are a lot zeros inside MC2_edges$volumteu col, we will consider 0 as equivalent to null values.\nWe can see that the columns valueofgoods_omu and volumeteu are mainly null. valueofgoodusd column contains more than 50% null values. There are 4 records of source with 0 as value, but 0 is their unique identifier so we do not consider 0 as null in source column. It means to say that only source, target, arrivaldate, hscode and weight columns will be helpful in our analysis.\n\n\nShow the code\n# Check for null values in each column\nnull_counts <- sapply(MC2_edges_no_dup, function(x) sum(is.null(x) | is.na(x) | x==0))\n\n# Calculate the percentage of null values for each column\nnull_percentages <- null_counts / nrow(MC2_edges_no_dup) * 100\n\n# Display the results\n#print(null_percentages)\n\nknitr::kable(null_percentages, \"simple\", col.names = c('Percentage null'))\n\n\n\n\n\nPercentage null\n\n\n\n\n\nsource\n0.0000753\n\n\ntarget\n0.0000000\n\n\narrivaldate\n0.0000000\n\n\nhscode\n0.0000000\n\n\nvalueofgoods_omu\n99.9947072\n\n\nvolumeteu\n85.2925183\n\n\nweightkg\n0.9012849\n\n\nvalueofgoodsusd\n54.4461223\n\n\n\n\n\nWe will be dropping the valueofgoods_omu , valueofgoodusdand volumeteu columns from our dataframe by selecting only the columns that we need.\n\n\nShow the code\nMC2_edges_no_dup <- MC2_edges_no_dup %>% select('source','target', 'arrivaldate', 'hscode','weightkg')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#check-on-the-hscodes",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#check-on-the-hscodes",
    "title": "Take-home_Ex02",
    "section": "2.3 Check on the HScodes",
    "text": "2.3 Check on the HScodes\nCheck the unique number of hscodes in the dataset. There are 4761 unique HScodes, however many are not related to fishing related activties.\n\n\nShow the code\n# Find the number of unique values in hscode\nlength(unique(MC2_edges_no_dup$hscode))\n\n\n[1] 4761\n\n\nWith reference two websites on “Harmonized System of Nomenclature” namely, World Custom Organisation Harmonized System codes and connect2India , we will filter for records that have HScodes starting with 1604xx, 1605xx and 301xxx to 308xxx as they refer to seafood commodities, thus removing many other transactions like ‘television’, ‘steel parts’ etc… This will help to filter away the noises and help us to focus on trading activities related to the fishing industry.\n\n\nShow the code\nmc2_seafood_edges<- MC2_edges_no_dup[grepl('^1605|^1604|^301|^302|^303|^304|^305|^306|^307|^308', MC2_edges_no_dup$hscode), ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#preparation-of-edges",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#preparation-of-edges",
    "title": "Take-home_Ex02",
    "section": "2.4 Preparation of Edges",
    "text": "2.4 Preparation of Edges\nWe will perform a group by ‘source’, ‘target’ and ‘arrivaldate’ and aggregate the total count of interactions, ‘Weight’, between each pair of companies. At this moment, we should not be filtering the records because we would like to calculate the network centrality scores first before zooming into records of interest. As we are interested to see whether there are patterns of self-loops, we will not be removing any company shipping to itself.\n\n\nShow the code\n#unique(mc2_seafood_edges$hscode)\n#unique(mc2_seafood_edges$source)\nmc2_seafood_edges_agg <- mc2_seafood_edges %>%  \n  group_by(source, target,arrivaldate) %>% \n  summarise(weight=n(),\n            sum_goods_weightkg = sum(weightkg),\n            hscode=first(hscode)) %>% \n  #filter(Weight >=8) %>% \n  ungroup()\n\n\nLet us wrangle the date columns to prepare dataframe for temporal analysis later.\n(1) change the arrivaldate column to date data type\n\n\nShow the code\nmc2_seafood_edges_agg$arrivaldate <- as.Date(mc2_seafood_edges_agg$arrivaldate)\n\n\n(2) create year, month, weekday, weeknumber columns\n\n\nShow the code\nmc2_seafood_edges_agg <- mc2_seafood_edges_agg %>% \n  mutate(year = year(arrivaldate)) %>% \n  mutate(month = month(arrivaldate)) %>% \n  mutate(day = day(arrivaldate)) %>% \n  mutate(weekday = wday(arrivaldate,\n                        label= TRUE,\n                        abbr = FALSE)) %>% \n  mutate(weeknumber = isoweek(arrivaldate))\n\n\nTBC: Inspect the frequency of source and target actors, and remove those actors below a frequency count of 5.\nTBC: First , we remove low frequency source actors under 5 counts.\n\n\n\nNext, remove target actors with frequency count less than 5:"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#preparation-of-nodes",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#preparation-of-nodes",
    "title": "Take-home_Ex02",
    "section": "2.5 Preparation of Nodes",
    "text": "2.5 Preparation of Nodes\nWe will include only nodes that are in ‘source’ and ‘target’ columns in the mc2_seafood_edges_agg dataframe after the first round of data filtering.\n\n\nShow the code\nnodes_seafood <- MC2_nodes %>%\n  filter (id %in% c(mc2_seafood_edges_agg$source, mc2_seafood_edges_agg$target))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-the-network-graph-dataframe-using-tbl_graph-of-the-tidygraph-package.",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#creating-the-network-graph-dataframe-using-tbl_graph-of-the-tidygraph-package.",
    "title": "Take-home_Ex02",
    "section": "2.6 Creating the network graph dataframe using tbl_graph() of the tidygraph package.",
    "text": "2.6 Creating the network graph dataframe using tbl_graph() of the tidygraph package.\n\n\n\n\n\n\nNote\n\n\n\nNode file needs to have ID of nodes as first column.\nEdge file need to contain source and target as column 1 and 2.\n\n\nTo create the network graph dataframe\n\n\nShow the code\nseafood_graph<- tbl_graph(nodes=nodes_seafood,\n                          edges = mc2_seafood_edges_agg,\n                          directed = TRUE)\n\n\nThe dataframe ‘seafood_graph’ has 11539 nodes with 374709 edges. It is a directed graph with 214 components.\n\n\n# A tbl_graph: 11539 nodes and 374709 edges\n#\n# A directed multigraph with 214 components\n#\n# A tibble: 11,539 × 3\n  id                                shpcountry rcvcountry\n  <chr>                             <chr>      <chr>     \n1 AquaDelight Inc and Son's         Polarinda  Oceanus   \n2 Yu gan  Sea spray GmbH Industrial Oceanus    Oceanus   \n3 Olas del Mar Worldwide            Oceanus    Oceanus   \n4 French Crab S.p.A. Worldwide      Kondanovia Utoporiana\n5 Panope Limited Liability Company  Vesperanda Oceanus   \n6 hǎi dǎn Corporation Wharf         Marebak    Oceanus   \n# ℹ 11,533 more rows\n#\n# A tibble: 374,709 × 11\n   from    to arrivaldate weight sum_goods_weightkg hscode  year month   day\n  <int> <int> <date>       <int>              <int> <chr>  <dbl> <dbl> <int>\n1  1632  5401 2029-12-21       2              42950 307590  2029    12    21\n2  1632  5401 2030-01-19       2              42585 307590  2030     1    19\n3  1632  5401 2030-06-25       1              21110 307590  2030     6    25\n# ℹ 374,706 more rows\n# ℹ 2 more variables: weekday <ord>, weeknumber <dbl>\n\n\nRunning the code chunk below to confirm that ‘seafood_graph’ is not a connected graph.\n\n\nShow the code\nis.connected(seafood_graph)\n\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculate-the-various-centrality-measures-of-seafood_graph.",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#calculate-the-various-centrality-measures-of-seafood_graph.",
    "title": "Take-home_Ex02",
    "section": "2.7 Calculate the various centrality measures of seafood_graph.",
    "text": "2.7 Calculate the various centrality measures of seafood_graph.\nThe top 10 nodes with reference to various centrality scores are printed using kable() function from knitr.\nReference was made from this link. The tidyverse centrality functions can be taken from here.\nFirst compute ‘betweenness’, ‘in-deg’ and ‘out-deg’ and ‘pagerank’ scores. All my betweenness scores were zero (Investigation in progress)…\n\n\nShow the code\nseafood_graph<- seafood_graph %>%\n  activate(\"nodes\") %>% \n  mutate(betweenness_centrality = centrality_betweenness(directed = TRUE)) %>% \n  mutate(in_deg_centrality = centrality_degree(weights = weight, \n                                               mode = \"in\")) %>% \n  mutate(out_deg_centrality = centrality_degree(weights = weight, \n                                               mode = \"out\")) %>% \n  mutate(pagerank = centrality_pagerank(weights = weight,\n                                        directed = TRUE)) #%>% \n  #mutate(community = as.factor(group_edge_betweenness(weights = Weight, \n                                                      #directed = TRUE,\n                                                      #n = 10)))\n\n\nLet us take a look at the top 10 nodes with high ‘betweenness’ centrality scores:\n\n\n\nTo see the top 20 nodes with ‘out-deg’ scores:"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-3-betweenness-centrality-companies",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-3-betweenness-centrality-companies",
    "title": "Take-home_Ex02",
    "section": "3.1 Interactive graph for top 3 BETWEENNESS CENTRALITY companies",
    "text": "3.1 Interactive graph for top 3 BETWEENNESS CENTRALITY companies\nWe will attempt to plot a network involving the top 3 companies including ‘Drakensberg S.A. de C.V. Marine ecology’,‘Playa de Oro BV’ and ‘bái suō wěn lú S.p.A.’ in terms of high betweenness centrality scores. We might be able to see their relationship through the graph.\nGetting a list of top 3 companies in terms of betweeness scores.\n\n\nShow the code\nn <- 3\ntop_n_bet_list <- seafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(betweenness_centrality)) %>% \n  top_n(n,wt = betweenness_centrality) %>%\n  pull(id)\n\n\nFilter all records in the edge file involving the top 3 companies. There are 586 records after this step.\n\n\nShow the code\nmc2_seafood_edges_agg_bet <- mc2_seafood_edges_agg %>%\n  filter(target %in% top_n_bet_list | source %in% top_n_bet_list)\n\n\nThe code chunk below first extracts the node dataframe from tbl_graph() object ‘seafood_graph’ created earlier. The reason for doing so is because we want the centrality values inside for tooltip later.\nNext, trim this node dataframe by including nodes found only in the ‘from’ and ‘target’ columns in ‘mc2_seafood_edges_agg_bet’ edge file. There are 96 nodes after this step.\n\n\nShow the code\nnodes_seafood_bet <- as.data.frame(seafood_graph %>% activate(nodes))\n\nnodes_seafood_bet <- nodes_seafood_bet %>%\n  filter (id %in% c(mc2_seafood_edges_agg_bet$source, mc2_seafood_edges_agg_bet$target))\n\n\nCalculate the total goods shipped between each pair of companies (for edge tooltip later)\n\n\nShow the code\nmc2_seafood_edges_agg_bet <- mc2_seafood_edges_agg_bet %>%\n  group_by(source, target) %>%\n  mutate(total_shipped_weightkg = sum(sum_goods_weightkg))\n\n\nRename the first two columns in the edge file to from and to for visNetwork to be able to recognize them. Create a title column for visNetwork to display the tooltip when we hover our mouse over the edges later.\n\n\nShow the code\nmc2_seafood_edges_agg_vis_bet <- mc2_seafood_edges_agg_bet %>% \n  rename(from = source) %>% \n  rename(to = target) %>% \n  mutate(title = paste('Total shipment weight = ',total_shipped_weightkg, \"\\n HSCODE =\", hscode))\n\n\nSimilarly, in the nodes file we add a column title We will be displaying the ‘in-deg’, ‘betweenness’ and ‘out-deg’ scores in the tooltip. If we want to colour the nodes by their shipping countries, then we would have to rename the shpcountry column to group because visNetwork looks for group column to colour the nodes. However, we will not do this now.\n\n\nShow the code\n# further processing\nnodes_seafood_vis_bet <- nodes_seafood_bet %>% \n  #rename(group= rcvcountry)  %>% \n  mutate(pagerank = round(pagerank, 5)) %>% \n  mutate(title = paste('shpcountry =', shpcountry, ',',\n                       'rcvcountry =', rcvcountry, ',',\n                       '\\n In-deg = ',in_deg_centrality, ',',\n                       \"\\n Betweenness =\", betweenness_centrality, ',',\n                       \"\\n Out-deg =\", out_deg_centrality))\n\n\nAs we want the nodes to be colour-coded by the betweenness centrality scores, we have first have to bin the betweenness scores into intervals using the cut() function. Next, rename the bet_grp column to group column for VisNetwork to colour nodes by betweenness intervals.\n\n\nShow the code\nbet_brks <- c(0, 500000, 1000000, 1500000)\ngrps <- c('500,000 & Below','500,001 -1,000,000', '1,000,000 - 1,254,681')\n\nnodes_seafood_vis_bet$bet_grp <- cut(nodes_seafood_vis_bet$betweenness_centrality, breaks=bet_brks, labels = grps,include.lowest = TRUE)\n\n#nodes_seafood_vis_in$in_deg_grp <- factor(nodes_seafood_vis$in_deg_grp, ordered = TRUE, levels = c('3001-6132','2001-3000','1001-2000','501-1000','500 & Below'))\n\nnodes_seafood_vis_bet <- nodes_seafood_vis_bet %>% \n  rename(group = bet_grp)\n\n\nThe code chunk below plots the interactive network graph.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(nodes_seafood_vis_bet,\n           mc2_seafood_edges_agg_vis_bet,\n           main = \"Ego Network of top 3 Betweenness companies\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInteractive features of the graph above\n(1) Select Id dropdown list\n(2) Select Group dropdown list: The values inside refers to the range of ‘in-deg’ centrality scores of the nodes. The pink colour node will represent the highest in-deg centrality score, followed by green, yellow, red and blue.\n(3) Zoom in to see the node labels, and arrows direction.\n(4) Drag a particular node away from the cluster to admire it.\n(5) Hover mouse over a node will display tooltip (shpcountry, rcvcountry, In-deg, pagerank and out-deg score). It will also display the ‘ego’ network with itself at the ego. Click on the node to freeze the ego network. Click on blank space to reset.\n(6) Hovering the mouse over an edge will display tooltip (Total weight of cargo, hscode of cargo)\n(7) Click and Drag on the graph to move the canvas around, will also temporary disable the edge lines.\n\n\nThe graph above shows the egonetwork of the top 3 betweenness companies and we can even see their tradiing relationship. The nodes in red are the companies with the top 2 betweenness centrality values, the nodes in yellow are ranked lower while the nodes in blue have relatively much lower centrality scores. The table below shows the top 10 companies in terms of centrality values.\n\n\nShow the code\nseafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(betweenness_centrality)) %>% \n  select(id,betweenness_centrality, out_deg_centrality,in_deg_centrality) %>% \n  head(n=10) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nid\nbetweenness_centrality\nout_deg_centrality\nin_deg_centrality\n\n\n\n\nDrakensberg S.A. de C.V. Marine ecology\n1254680.7\n18\n133\n\n\nPlaya de Oro BV\n1231661.8\n605\n15\n\n\nbái suō wěn lú S.p.A.\n839665.4\n42\n95\n\n\nPunjab s Marine conservation\n804016.8\n181\n2716\n\n\nDavid Ltd. Liability Co Forwading\n640974.3\n70\n4473\n\n\nYenisei Eel GmbH & Co. KG Services\n599808.9\n1871\n500\n\n\nCosta de Oro S.p.A.\n563721.1\n113\n175\n\n\nMarine Mates NV Worldwide\n560574.1\n424\n874\n\n\nLiumbwe GmbH & Co. KG\n551504.4\n130\n3\n\n\nTurkish Salmon A/S Marine\n470845.1\n699\n1132\n\n\n\n\n\nIt is observed that the companies with high betweenness scores served as both ‘source’ and ‘target’ companies; meaning they buy and sell seafood related products. Companies like ‘Drakensberg S.A. de C.V. Marine ecology’ focus more on buying than selling, while ‘Playa de Oro BV’ focuses more on selling than buying.\nThe business relationship observed: ‘Drakensberg S.A. de C.V. Marine ecology’ (ranked 1st) is one of the suppliers to ‘Playa de Oro BV’ (ranked 2nd) who in turns supplies to ‘bái suō wěn lú S.p.A.’ (ranked 3rd) who sells to many other companies. To reveal more relationships, the graph can be extended (to see the egonetwork of more yellow nodes ) by increasing the top n companies in the first code chunk in this section. For example, if we increase the size of the our network graph to ego network of top 5 betweenness companies (instead of 3), we would get"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-5-in_deg-centrality-companies",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-5-in_deg-centrality-companies",
    "title": "Take-home_Ex02",
    "section": "3.2 Interactive graph for top 5 IN_DEG CENTRALITY companies",
    "text": "3.2 Interactive graph for top 5 IN_DEG CENTRALITY companies\nIn this section, we will focus on companies with top in-deg centrality scores.\nFirst, obtain the list of top 5 in deg companies\n\n\nShow the code\nn <- 5\ntop_n_in_list <- seafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(in_deg_centrality)) %>% \n  top_n(n,wt = in_deg_centrality) %>%\n  pull(id)\n \n #kable(top_20_in_list, col.names=c('Top 20 in-deg companies'))\n\n\nFilter all records in the edge file involving the top 5 in-deg companies.\nNext calculate the sum of all shipments between each company and each of it supplier company.\nTo further reduce the number of nodes, we will keep the records when the ‘total_shipped_weightkg’ is above 10,000,000 kg. This will retain only trading interactions involving a sizable weight (kg) of goods traded. This is an alternative to filtering using the ‘weight’ column which indicates the number of trading interactions. There are 19k edges after this step.\n\n\nShow the code\n# Subset the dataframe to keep only rows with valid values (top 20 in-deg) in 'target' column\nmc2_seafood_edges_agg_in <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$target %in% top_n_in_list, ]\n\n# Calculate the sum of all the shipment weight between each pair of companies in edge file.\nmc2_seafood_edges_agg_in <- mc2_seafood_edges_agg_in %>%\n  group_by(source, target) %>%\n  mutate(total_shipped_weightkg = sum(sum_goods_weightkg))\n\n# further removal of records where 'weight' column is less than  10\nmc2_seafood_edges_agg_in <- mc2_seafood_edges_agg_in %>% \n  filter(total_shipped_weightkg>=10000000)\n\n\nThe code chunk below first extracts the node dataframe from tbl_graph() object ‘seafood_graph’ created earlier. The reason for doing so is because we want the centrality values inside for tooltip later.\nNext, trim this node dataframe by including nodes found only in the ‘from’ and ‘target’ columns in ‘mc2_seafood_edges_agg_in’ edge file. There are 62 nodes after this step.\n\n\nShow the code\nnodes_seafood_in <- as.data.frame(seafood_graph %>% activate(nodes))\n\nnodes_seafood_in <- nodes_seafood_in %>%\n  filter (id %in% c(mc2_seafood_edges_agg_in$source, mc2_seafood_edges_agg_in$target))\n\n\nRename the first two columns of the edge file to from and to for visNetwork to be able to recognise them. Create a title column for visNetwork to display the tooltip when we hover our mouse over the edges later.\n\n\nShow the code\nmc2_seafood_edges_agg_vis_in <- mc2_seafood_edges_agg_in %>% \n  rename(from = source) %>% \n  rename(to = target) %>% \n  mutate(title = paste('Total shipment weight = ',total_shipped_weightkg, \"\\n HSCODE =\", hscode))\n\n\nSimilarly, in the nodes file we add a column title We will be displaying the ‘in-deg’, ‘betweenness’ and ‘out-deg’ scores in the tooltip.\n\n\nShow the code\n# extract nodes file from seafood_graph as a data frame\n#nodes_seafood_vis <- as.data.frame(seafood_graph %>% activate(nodes))\n\n# further processing\nnodes_seafood_vis_in <- nodes_seafood_in %>% \n  #rename(group= rcvcountry)  %>% \n  mutate(pagerank = round(pagerank, 5)) %>% \n  mutate(title = paste('shpcountry =', shpcountry, ',',\n                       'rcvcountry =', rcvcountry, ',',\n                       '\\n In-deg = ',in_deg_centrality, ',',\n                       \"\\n Betweenness =\", betweenness_centrality, ',',\n                       \"\\n Out-deg =\", out_deg_centrality))\n\n\nAs we want the nodes to be colour-coded by the in-deg centrality scores, we have first have to bin the in-deg scores into intervals using the cut() function. Next, rename the in_deg_grp column to group column for VisNetwork to colour nodes by in-deg score intervals.\n\n\nShow the code\nin_deg_brks <- c(0, 10000, 20000,30000,40000,50000, 60000,70000)\ngrps <- c('10,000 & Below','10,001-20,000', '20,001-30,000', '30,001-40,000','40,001-50,000', '50,001-60,000','60,001-70,000')\n\nnodes_seafood_vis_in$in_deg_grp <- cut(nodes_seafood_vis_in$in_deg_centrality, breaks=in_deg_brks, labels = grps,include.lowest = TRUE)\n\n#nodes_seafood_vis_in$in_deg_grp <- factor(nodes_seafood_vis$in_deg_grp, ordered = TRUE, levels = c('3001-6132','2001-3000','1001-2000','501-1000','500 & Below'))\n\nnodes_seafood_vis_in <- nodes_seafood_vis_in %>% \n  rename(group = in_deg_grp)\n\n\nThe code chunk below plots in interactive network graph using visNetwork.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(nodes_seafood_vis_in,\n           mc2_seafood_edges_agg_vis_in,\n           main = \"Ego Network of top 5 IN-DEG companies\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nThe plot above shows not only the filtered ego network of the top 5 companies (in terms of in-deg scores) but also how these five companies are related to one another. Red node has the highest in-deg centrality scores , followed by blue and yellow. The top 5 in-deg companies are all related and buys from ‘Sea Breezes S.A de C.V Freight’.\nThe table below shows the names of the top 5 in-deg companies.\n\n\nShow the code\nseafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(in_deg_centrality), desc(pagerank)) %>% \n  select(id,in_deg_centrality,pagerank, out_deg_centrality) %>% \n  head(n=5) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nid\nin_deg_centrality\npagerank\nout_deg_centrality\n\n\n\n\nMar del Este CJSC\n63332\n0.0278945\n32\n\n\nhǎi dǎn Corporation Wharf\n57221\n0.0355972\n16\n\n\nCaracola del Sol Services\n50655\n0.0167972\n6\n\n\nPao gan SE Seal\n35156\n0.0124541\n5\n\n\nCosta de la Felicidad Shipping\n30701\n0.0094311\n6\n\n\n\n\n\n\n3.2.1 Temporal analysis of top 20 ‘in-deg’ centrality companies in trading occurrence over the years\nFirst, obtain the list of company ids in the top 20 in-deg ranking.\n\n\nShow the code\nn <- 20\ntop_n_in_list <- seafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(in_deg_centrality)) %>% \n  top_n(n,wt = in_deg_centrality) %>%\n  pull(id)\n\n\nNext, filter the edge file to keep only the records involving the top 20 in-deg companies. There are 185K records after this step.\n\n\nShow the code\n# Subset the dataframe to keep only rows with valid values (top 20 in-deg) in 'target' column\nmc2_seafood_edges_agg_in_hm <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$target %in% top_n_in_list, ]\n\n\nUse the first code chunk below to compute the sum of monthly weights (interactons) between each unique pair of company in our filtered data. The second code chunk is to compute the year-month in which a source company has the max number of shipping interaction throughout the years.\n\n\nShow the code\nhm <- mc2_seafood_edges_agg_in_hm %>%\n  mutate(month = floor_date(arrivaldate, unit = \"month\")) %>%\n  group_by(target, month) %>%\n  summarise(total_weight_by_month = sum(weight)) %>% \n  ungroup() \n\nhm <- hm  %>%  group_by(target) %>%\n  mutate(maxweightmonth = as.Date(month[which.max(total_weight_by_month)])) %>%\n  ungroup()\n\n\nNow lets plot heatmap\n\n\nShow the code\nplotfrom <- \"2028-01-02\"\nplotto <- '2034-01-02'\n\nggplot(hm, aes(x = month, y = fct_reorder(target, maxweightmonth), fill = total_weight_by_month)) +\n  geom_tile(colour=\"White\", show.legend=FALSE) +\n  scale_fill_distiller(palette=\"Spectral\") +\n  scale_y_discrete(name=\"\", expand=c(0,0))+\n  scale_x_date(name=\"Arrival Date\", \n               limits=as.Date(c(plotfrom, plotto)), \n               expand=c(0,0),date_breaks = \"1 year\", \n               date_labels = \"%Y\") +\n  labs(title=\"Heatmap of shipping interactions\",\n       subtitle=paste0(\"Top receiving companies from \", \n                       plotfrom, ' to ', plotto)) +\n  theme_classic() +\n  \n  theme(axis.line.y=element_blank(), plot.subtitle=element_text(size=rel(0.78)),\n        plot.title.position=\"plot\",\n        axis.text.y=element_text(colour=\"Black\",size=5), \n        plot.title=element_text(size=rel(2.3)))\n\n\n\n\n\nAnalysis of the plot above:\nThe heatmap above tells us the temporal trading interactions of the top 20 in-deg companies through the years. It is observed that company ‘Mar del Este CJSC’ has ‘hot’ patterns in the most recent years.\n\n\n3.2.2 Comparing trading patterns of Mar del Este CJSC company and Pao gan SE Seal company with their suppliers\nLet us examine the trading patterns of ‘Mar del Este’, the top leading companies in terms of ‘in-deg’ with four out of its many suppliers (as seen from the interactive graph).\nFirst, filter records with only Mar del Este and 4 of its suppliers.\n\n\nShow the code\nmar_in_df<-mc2_seafood_edges_agg_vis_in %>%\n  filter(to %in% 'Mar del Este CJSC')\n\n\nNext, plot a time series using geom_line() and geom_point_interactive().\n\n\nShow the code\nmar_in_df <-  mar_in_df %>%\n  mutate(tooltip = paste0('# of interaction: ', weight, '\\nDate :', arrivaldate))\n\n \nmar_in_df1<- ggplot(mar_in_df %>%\n                          filter(from=='Olas del Mar N.V.'),\n                        aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) +\n  geom_point_interactive(aes(tooltip= tooltip), \n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 30), \n    breaks = seq(0, 30, by = 5),\n    expand = c(0, 0)) +\n  \n  labs(title='Olas del Mar N.V.', \n       x = 'Date',\n       y='Number of trading occurrence')\n\nmar_in_df2<- ggplot(mar_in_df %>% filter(from=='Sea Breezes S.A. de C.V. Freight '), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), \n                 by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 30),\n    breaks = seq(0, 30, by = 5),\n    expand = c(0, 0)) +  # Remove padding around the y-axis limits\n\n      \n  labs(title='Sea Breezes S.A. de C.V. Freight ', \n       x = 'Date',\n       y='Number of trading occurrence')\n\n\nmar_in_df3<- ggplot(mar_in_df %>% filter(from=='Blue Horizon Family &'), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 30), \n    breaks = seq(0, 30, by = 5),\n    expand = c(0, 0)) +\n  \n  labs(title='Blue Horizon Family &', \n       x = 'Date',\n       y='Number of trading occurrence')\n\nmar_in_df4<- ggplot(mar_in_df %>% filter(from=='Wave Watchers Ltd. Liability Co'), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 30), \n    breaks = seq(0, 30, by = 5),\n    expand = c(0, 0)) +\n  \n  labs(title='Wave Watchers Ltd. Liability Co', \n       x = 'Date',\n       y='Number of trading occurrence')\n\n\n\ngirafe(code = print(mar_in_df1 /mar_in_df2 /mar_in_df3/mar_in_df4),\n       #width_svg = 6,\n       height_svg =8,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\nNow let us examine the trading patterns of ‘Pao gan SE Seal’, one of the few top leading companies in terms of ‘in-deg’ with four out of its many suppliers. First filter all records involving Pao gan and its 4 suppliers.\n\n\nShow the code\npaogan_in_df<- mc2_seafood_edges_agg_vis_in %>%\n  filter(to %in% 'Pao gan SE Seal')\n\n\nNext, plot a time series using geom_line() and geom_point_interactive().\n\n\nShow the code\npaogan_in_df <-  paogan_in_df %>%\n  mutate(tooltip = paste0('# of interaction: ', weight, '\\nDate :', arrivaldate))\n\n \npaogan_in_df1<- ggplot(paogan_in_df %>%\n                          filter(from=='Paradera S.A. de C.V.'),\n                        aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) +\n  geom_point_interactive(aes(tooltip= tooltip), \n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 100), \n    breaks = seq(0, 100, by = 20),\n    expand = c(0, 0)) +\n  \n  labs(title='Paradera S.A. de C.V.', \n       x = 'Date',\n       y='Number of trading occurrence')\n\npaogan_in_df2<- ggplot(paogan_in_df %>% filter(from=='Greek Octopus SRL Logistics'), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), \n                 by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 100),  # Set the y-axis limits\n    breaks = seq(0, 100, by = 20),  # Set the y-axis breaks at intervals of 5\n    #minor_breaks = seq(0, 30, by = 1),  # Set the y-axis minor breaks at intervals of 1\n    expand = c(0, 0)) +  # Remove padding around the y-axis limits\n\n      \n  labs(title='Greek Octopus SRL Logistics', \n       x = 'Date',\n       y='Number of trading occurrence')\n\n\npaogan_in_df3<- ggplot(paogan_in_df %>% filter(from=='Madhya Pradesh  Market LLC'), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 100),\n    breaks = seq(0, 100, by = 20), \n    expand = c(0, 0)) +\n  \n  labs(title='Madhya Pradesh  Market LLC', \n       x = 'Date',\n       y='Number of trading occurrence')\n\npaogan_in_df4<- ggplot(paogan_in_df %>% filter(from=='Sea Breezes S.A. de C.V. Freight '), aes(x=arrivaldate, y=weight)) +\n  geom_line( color=\"steelblue\", size = 0.8) + \n  geom_point_interactive(aes(tooltip= tooltip),\n                         size = 0.5) +\n  xlab(\"\") +\n  theme_light() +\n  theme(#panel.spacing = unit(2, \"lines\"),\n        axis.text.x=element_text(angle=60, hjust=1),\n        panel.grid.major.x = element_blank(),\n        axis.title.y = element_text(size = 10)) +\n  scale_x_date(\n    breaks = seq(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\"), by = \"3 months\"),\n    limits = c(as.Date(\"2028-01-01\"), as.Date(\"2034-12-31\")),\n    labels = function(x) format(x, \"%b %Y\")) +\n  scale_y_continuous(\n    limits = c(0, 100), \n    breaks = seq(0, 100, by = 20),  \n    expand = c(0, 0)) +\n  labs(title='Sea Breezes S.A. de C.V. Freight ', \n       x = 'Date',\n       y='Number of trading occurrence')\n\ngirafe(code = print(paogan_in_df1 /paogan_in_df2 /paogan_in_df3 /paogan_in_df4),\n       #width_svg = 6,\n       height_svg =8,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\nAnalysis of the plot above: The trading interaction of ‘Mar del Este’ with its suppliers is more consistent throughout as compared to ‘Pao gan SE Seal’. Throughout the years, ‘Pao gan SE Seal’ has interactions many suppliers but some of these relationships are short term. . It dawned on me that we can identify IUU companies that frequently close down and re-register their companies from the type of plot above. For such companies, I would expect the buyer’s graph to show that buyer has been changing suppliers very frequently.\nFor our group project we can consider creating drop down list of ‘buying companies’ for user to interact with and see each of the buyer’s interaction with their top n suppliers over the years. A coordinated view can be created with social network graph such that when a buyer / supplier node is selected on the network graph, its trading activity over time with top n suppliers/ buyers is automatically generated."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-5-out-degree-centrality-companies",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-graph-for-top-5-out-degree-centrality-companies",
    "title": "Take-home_Ex02",
    "section": "3.3 Interactive graph for top 5 OUT-DEGREE CENTRALITY companies",
    "text": "3.3 Interactive graph for top 5 OUT-DEGREE CENTRALITY companies\nLet us get the top 5 company names in terms of out-deg centrality scores.\n\n\nShow the code\nn <- 5\ntop_n_out_list <- seafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(out_deg_centrality)) %>% \n  top_n(n, wt = out_deg_centrality) %>%\n  pull(id)\n#kable(top_20_out_list, col.names=c('Top 20 out-deg companies'))\n\n\nFilter all records in the edge file involving the top 5 out-deg companies.\nNext calculate the sum of all shipments between each company and each of it buyercompany.\nTo further reduce the number of nodes, we will keep the records when the ‘total_shipped_weightkg’ is above 10,000,000 kg. This will retain only trading interactions involving a sizable weight (kg) of goods traded. This is an alternative to filtering using the ‘weight’ column which indicates the number of trading interactions. There are 10k edges after this step.\n\n\nShow the code\n# Subset the dataframe to keep only rows with valid values (top 5 out-deg) in 'from' column (15K rows)\nmc2_seafood_edges_agg_out <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$source %in% top_n_out_list, ]\n\n# Calculate the sum of all the shipment weight between each pair of companies in edge file.\nmc2_seafood_edges_agg_out <- mc2_seafood_edges_agg_out %>%\n  group_by(source, target) %>%\n  mutate(total_shipped_weightkg = sum(sum_goods_weightkg))\n\n# further removal of records where 'weight' column is less than  10\nmc2_seafood_edges_agg_out <- mc2_seafood_edges_agg_out %>% \n  filter(total_shipped_weightkg>=10000000)\n\n\nThe first code chunk below extracts the ‘nodes’ table from tbl_graph() object ‘seafood_graph’ created earlier. The reason for doing so is because we want the centrality values inside for tooltip later.\nNext, trim this node dataframe by including nodes found only in the ‘from’ and ‘target’ columns in ‘mc2_seafood_edges_agg_in’ edge file. There are 24 nodes after this step.\n\n\nShow the code\nnodes_seafood_out <- as.data.frame(seafood_graph %>% activate(nodes))\n\nnodes_seafood_out <- nodes_seafood_out %>%\n  filter (id %in% c(mc2_seafood_edges_agg_out$source, mc2_seafood_edges_agg_out$target))\n\n\nRename the first two columns of the edge file to from and to for visNetwork to be able to recognise them. Create a title column for visNetwork to display the tooltip when we hover our mouse over the edges later.\n\n\nShow the code\nmc2_seafood_edges_agg_vis_out <- mc2_seafood_edges_agg_out %>% \n  rename(from = source) %>% \n  rename(to = target) %>% \n  mutate(title = paste('Total shipment weight = '\n                       ,total_shipped_weightkg,\n                       \"\\n HSCODE =\",\n                       hscode))\n\n\nSimilarly, in the nodes file we add a column title We will be displaying the ‘in-deg’, ‘betweenness’ and ‘out-deg’ scores in the tooltip.\n\n\nShow the code\nnodes_seafood_vis_out <- nodes_seafood_out %>% \n  #rename(group= rcvcountry)  %>% \n  mutate(pagerank = round(pagerank, 5)) %>% \n  mutate(title = paste('shpcountry =', shpcountry, ',',\n                       'rcvcountry =', rcvcountry, ',',\n                       '\\n In-deg = ',in_deg_centrality, ',',\n                       \"\\n Betweenness =\", betweenness_centrality, ',',\n                       \"\\n Out-deg =\", out_deg_centrality))\n\n\nAs we want the nodes to be colour-coded by the out-deg centrality scores, we have first have to bin the out-deg scores into intervals using the cut() function. Next, rename the out_deg_grp column to group column for VisNetwork to colour nodes by out-deg score intervals.\n\n\nShow the code\nout_deg_brks <- c(0,10000, 20000, 30000)\ngrps <- c('10,000 & Below','10,001-20,001', '20,001-27,570')\n\nnodes_seafood_vis_out$out_deg_grp <- cut(nodes_seafood_vis_out$out_deg_centrality, breaks=out_deg_brks, labels = grps,include.lowest = TRUE)\n\n\nnodes_seafood_vis_out <- nodes_seafood_vis_out %>% \n  rename(group = out_deg_grp)\n\n\nThe code chunk below plots the interactive graph.\n\n\nShow the code\nvisNetwork(nodes_seafood_vis_out,\n           mc2_seafood_edges_agg_vis_out,\n           main = \"Ego Network of top 5 OUT-DEG companies\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nThe chart highlights actors with high out-deg centrality scores over 2028 to 2034. In this default view, we can quickly identify the 2 coloured nodes as top 2 suppliers. Both top out-deg companies supplies to 3 common target companies; they are “Coasta de la Felicidad’, ‘Niger Bend Limited Liability..’ and”Pao gan SE Seal’.\nThe table below shows the names of the top 5 out-deg companies\n\n\nShow the code\nseafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(out_deg_centrality)) %>% \n  select(id,out_deg_centrality,in_deg_centrality,pagerank) %>% \n  head(n=5) %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nid\nout_deg_centrality\nin_deg_centrality\npagerank\n\n\n\n\nnián yú Ltd. Corporation\n27570\n2\n8.55e-05\n\n\nSea Breezes S.A. de C.V. Freight\n24283\n10\n5.29e-05\n\n\nEstrella de la Costa SRL\n8862\n7\n8.12e-05\n\n\nBlue Horizon Family &\n7670\n0\n4.16e-05\n\n\nMadhya Pradesh Market LLC\n6789\n1\n4.27e-05\n\n\n\n\n\n\n3.3.1 Temporal analysis of top 20 ‘out-deg’ centrality companies in trading occurrence over the years\nFirst, obtain the list of company ids in the top 20 in-deg ranking.\n\n\nShow the code\nn <- 20\ntop_n_out_list <- seafood_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(out_deg_centrality)) %>% \n  top_n(n, wt = out_deg_centrality) %>%\n  pull(id)\n\n\nNext, filter the edge file to keep only the records involving the top 20 out-deg companies. There are 43K records after this step\n\n\nShow the code\n# Subset the dataframe to keep only rows with valid values (top 20 out-deg) in 'target' column\nmc2_seafood_edges_agg_out_hm <- mc2_seafood_edges_agg[mc2_seafood_edges_agg$source %in% top_n_out_list, ]\n\n\nUse the first code chunk below to compute the sum of monthly weights (interactions) between each unique pair of company in our filtered data. The second code chunk is to compute the year-month in which a source company has the max number of shipping interaction throughout the years.\n\n\nShow the code\nhm <- mc2_seafood_edges_agg_out_hm %>%\n  mutate(month = floor_date(arrivaldate, unit = \"month\")) %>%\n  group_by(source, month) %>%\n  summarise(total_weight_by_month = sum(weight)) %>% \n  ungroup() \n\nhm <- hm  %>%  group_by(source) %>%\n  mutate(maxweightmonth = as.Date(month[which.max(total_weight_by_month)])) %>%\n  ungroup()\n\n\nWe will now plot the heatmap of top 20 out-deg companies to show their shipping interactions over the years.\n\n\nShow the code\nplotfrom <- \"2028-01-02\"\nplotto <- '2034-01-02'\n\nggplot(hm, aes(x = month, y = fct_reorder(source, maxweightmonth), fill = total_weight_by_month)) +\n  geom_tile(colour=\"White\", show.legend=FALSE) +\n  scale_fill_distiller(palette=\"Spectral\") +\n  scale_y_discrete(name=\"\", expand=c(0,0))+\n  scale_x_date(name=\"Arrival Date\", \n               limits=as.Date(c(plotfrom, plotto)), \n               expand=c(0,0),date_breaks = \"1 year\", \n               date_labels = \"%Y\") +\n  labs(title=\"Heatmap of shipping interactions\",\n       subtitle=paste0(\"Top supplier companies from \", \n                       plotfrom, ' to ', plotto)) +\n  theme_classic() +\n  \n  theme(axis.line.y=element_blank(), plot.subtitle=element_text(size=rel(0.78)),\n        plot.title.position=\"plot\",\n        axis.text.y=element_text(colour=\"Black\",size=5), \n        plot.title=element_text(size=rel(2.3)))\n\n\n\n\n\nAnalysis of the plot above:\nAt a quick glance, three companies are hot, they are ‘nian yu Ltd Corporation’ ,‘Playa del Tesoro OJSC’. and ‘Sea Breezes SA’.\n‘nian yu Ltd Corporation’ had several instances of very high shipping activities in 2031 and 2032, which is worth looking into.\n‘Madhya Pradesh Market LLC’ had interactions only from 2029 to 2032 first quarter. What happened to it?\n\n\n3.3.2 Treemap of business relationship between shipping and receiving companies\nFinally, lets build a treemap to get a high level view of the companies that these main out-deg companies ship the goods to.\nThe code chunk below groups the data by from and to columns and aggregating the TotalInteractions and MedianCargoWeight_daily between each pair shipping and receiving company by hscode.\n\n\nShow the code\nseafood_tree <-mc2_seafood_edges_agg_out_hm %>% \n  group_by(source,target) %>% \n  summarise(TotalInteractions=sum(weight),\n            Totalshipment= sum(sum_goods_weightkg)) %>% \n  ungroup() %>% \n  arrange(desc(TotalInteractions))\n\n\nThe code chunk below plots the treemap using the treemap library.\n\n\nShow the code\ntm<- treemap(seafood_tree,\n        index=c(\"source\", \"target\"),\n        vSize=\"TotalInteractions\",\n        vColor=\"Totalshipment\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        algorithm = \"squarified\",\n        title='Shipping and receiving companies interaction pattern',\n        title.legend = \"Total shipment weight\"\n        )\n\n\n\n\n\nThe most outer layer refers to shipping companies while the tiles within represents the companies that they shipped their goods to. The bigger the tile size, the more the interaction. The darker the colour, the greater the total shipment weight accumulated across the years.\nThe largest player here is ‘nian yu Ltd Corporation’ .\n‘Playa del Tesoro OJSC’ is the ranked fifth in terms of out-deg centrailty and it ships in high frequency and large volume to its partners over the years 2028 to 2034.\nThe interactive version created with d3treeR library. But since this is not available in CRAN, we might not be able to use this in Shiny.\n\n\nShow the code\nlibrary(d3treeR)\n\n\n\n\nShow the code\nd3tree(tm,rootname = \"Supplier Companies\" )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#correlationship-between-partnership-intervals-in-days-and-total-number-of-interactions-between-company-pairs",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#correlationship-between-partnership-intervals-in-days-and-total-number-of-interactions-between-company-pairs",
    "title": "Take-home_Ex02",
    "section": "3.4 Correlationship between partnership intervals (in days) and total number of interactions between company pairs",
    "text": "3.4 Correlationship between partnership intervals (in days) and total number of interactions between company pairs\nIn this section, lets examine whether there is a correlationship between the interval of interaction within a year and the number of interactions between each pairs of companies. Will there be companies with short partnership duration and high number of interactions (e.g. shipping in high frequency for only 1 week within a year) ?\nWe will prepare the dataframe needed for the plot.\nFirst, for each group of from, to, year:\n1) partnership_days : the number of days within the year that each pair of companies had interactions\n2) total_interaction : sum of all the Weights between each pair of companies\n\n\nShow the code\ncor <- mc2_seafood_edges_agg_vis_in %>% \n  group_by(from, to,year) %>% \n  summarise(partnership_days=as.integer(max(arrivaldate)-min(arrivaldate)+1),\n            total_interaction = sum(weight),\n            median_interaction = median(weight)) %>% \n  ungroup() %>% \n  arrange(partnership_days)\n\n\n\n3.4.1 Checking for statistical significance in correlationship\n\n\nShow the code\nlibrary(scales)\ncorrel_2028 <- ggscatterstats(data = cor %>% filter(year==2028), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2028\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\ncorrel_2029 <- ggscatterstats(data = cor %>% filter(year==2029), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2029\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n\ncorrel_2030 <- ggscatterstats(data = cor %>% filter(year==2030), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2030\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n\ncorrel_2031 <- ggscatterstats(data = cor %>% filter(year==2031), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2031\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n\ncorrel_2032 <- ggscatterstats(data = cor %>% filter(year==2032), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2032\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n\ncorrel_2033 <- ggscatterstats(data = cor %>% filter(year==2033), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2033\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n\n\ncorrel_2034 <- ggscatterstats(data = cor %>% filter(year==2034), \n                           x = partnership_days, y = total_interaction,\n                           type = \"nonparametric\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  scale_x_continuous(limits = c(0, 365), \n                     breaks=seq(0, 365, 30), \n                     labels= comma) + \n  scale_y_continuous(limits = c(0, 1250), \n                     breaks=seq(0, 1250, 250), \n                     labels= comma) +\n  \n  labs(title = \"Number of interactions and partnership duration in 2034\", \n       x = \"Partnership days\", y = \"Total interactions\") \n\n# combining plots using patchwork\np_correl <- (correl_2034 + correl_2033) / (correl_2032 + correl_2031) / (correl_2030 + correl_2029) # + plot_spacer() + plot_spacer()\np_correl + plot_annotation(title = \"Correlation between Number of interactions and partnership duration (days)\", \n                           theme = theme(plot.title = element_text(size = 18),\n                                         plot.subtitle = element_text(size = 12))) + plot_layout(ncol = 1, nrow = 3,\n                                                                                                 heights = c(2,2))\n\n\n\n\n\nThe plots (non-parametric) above has p-values less than 0.05 and it suggests that there is a correlationship between the rank-transformed data of total interactions and partnership duration between companies . The upper outliers pairs of companies could be worth investigating because they have high number of interactions for a particular partnership duration in a year. For example, if a company A had interaction with company B for only 3 months but with exceptionally high number of trading interactions, should both companies be worth investigating?\n\n\n3.4.2 Coordinated and interactive scatterplot\nFor usability , lets us plot an interactive and coordinate equivalent plot of the above scatterplot chart for the most recent four years 2034, 2033, 2032 and 2031.\nFirst we will prepare the tooltip to be shown.\n\n\nShow the code\ncor <- cor %>%\n  mutate(label1 = group_indices(., from, to)) %>% \n  mutate(fromto = paste('From: ', from,\n                        'To: ', to)) %>% \n  mutate(tooltip = paste(fromto,\n                         '\\nTotal interaction: ', total_interaction,\n                         '\\nPartnership duration: ', partnership_days, 'days'))\n\n\nNext, we will filter and use geom_point_interactive() of ggiraph library to plot.\n\n\nShow the code\nscatter_2034 <- ggplot(data=cor%>% filter(year==2034),\n       aes(x=partnership_days, y=total_interaction)) +\n  geom_point_interactive(aes(data_id = label1,\n                             tooltip= tooltip),\n                         size = 0.8) +\n  geom_smooth(method='lm',\n              size = 0.5) +\n  scale_x_continuous(limits = c(0, 365), breaks = seq(0, 365, 30)) +\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 200)) +\n\n  labs(title = \"Scatterplot of # of interactions and Partnership duration in 2034\",\n       y = \"Total interactions\",\n       x = \"Partnership duration\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face ='bold'),\n        axis.title = element_text(size = 8, face= 'bold'),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 8))\n\n\nscatter_2033 <- ggplot(data=cor%>% filter(year==2033),\n       aes(x=partnership_days, y=total_interaction)) +\n  geom_point_interactive(aes(data_id = label1,\n                             tooltip= tooltip),\n                         size = 0.8) +\n  geom_smooth(method='lm',\n              size = 0.5) +\n  scale_x_continuous(limits = c(0, 365), breaks = seq(0, 365, 30)) +\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 200)) +\n\n  labs(title = \"Scatterplot of # of interactions and Partnership duration in 2033\",\n       y = \"Total interactions\",\n       x = \"Partnership duration\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12,face = 'bold'),\n        axis.title = element_text(size = 8, face= 'bold'),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 8))\n\nscatter_2032 <- ggplot(data=cor%>% filter(year==2032),\n       aes(x=partnership_days, y=total_interaction)) +\n  geom_point_interactive(aes(data_id = label1,\n                             tooltip= tooltip),\n                         size = 0.8) +\n  geom_smooth(method='lm',\n              size = 0.5) +\n  scale_x_continuous(limits = c(0, 365), breaks = seq(0, 365, 30)) +\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 200)) +\n\n  labs(title = \"Scatterplot of # of interactions and Partnership duration in 2032\",\n       y = \"Total interactions\",\n       x = \"Partnership duration\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12,face = 'bold'),\n        axis.title = element_text(size = 8, face= 'bold'),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 8))\n\n\nscatter_2031 <- ggplot(data=cor%>% filter(year==2031),\n       aes(x=partnership_days, y=total_interaction)) +\n  geom_point_interactive(aes(data_id = label1,\n                             tooltip= tooltip),\n                         size = 0.8) +\n  geom_smooth(method='lm',\n              size = 0.5) +\n  scale_x_continuous(limits = c(0, 365), breaks = seq(0, 365, 30)) +\n  scale_y_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 200)) +\n\n  labs(title = \"Scatterplot of # of interactions and Partnership duration in 2031\",\n       y = \"Total interactions\",\n       x = \"Partnership duration\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12,face = 'bold'),\n        axis.title = element_text(size = 8, face= 'bold'),\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 8))\n\n\n\n\ngirafe(code = print(scatter_2034 / scatter_2033 /scatter_2032 /scatter_2031),\n       width_svg = 6,\n       height_svg = 7,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeatures of the chart above\n\nEach point in chart represents a pair of companies that ships and receives, hover mouse over to see ‘company pair names’, ‘total interaction’ and ‘partnership days’ inside tooltip\nIf a particular company pair exists in more than one chart , its corresponding point will also be highlighted in the other charts.\n\n\n\nIf two companies consistently have similar number of interactions within similar partnership days across the years, then there is no issue. Explore the interactive chart to spot pairs of companies with ‘solo’ occurrence throughout the years and is also one of the outlier pairs. Those pair of companies could be worth investigating."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#comparison-of-centrality-values-across-shipping-countries",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#comparison-of-centrality-values-across-shipping-countries",
    "title": "Take-home_Ex02",
    "section": "3.5 Comparison of centrality values across shipping countries",
    "text": "3.5 Comparison of centrality values across shipping countries\nIn this section, we will compare the centrality values of companies from different countries.\n\n3.5.1 Comparison of out-deg centrality between top shipping countries\nLet us compute top 5 shipping countries in terms of out-deg centrality scores.\n\n\nShow the code\nnodes_seafood_vis_out %>% \n  group_by(shpcountry) %>% \n  summarise(sum_out_deg = sum(out_deg_centrality)) %>% \n  arrange(desc(sum_out_deg)) %>% \n  head(n=5) %>% kable()\n\n\n\n\n\nshpcountry\nsum_out_deg\n\n\n\n\nMerigrad\n34396\n\n\nIsliandor\n24283\n\n\nMawazam\n8891\n\n\nOsterivaria\n7670\n\n\nMarebak\n22\n\n\n\n\n\nThe code chunk below filters the top 5 shipping countries and non parametric one way anova test is performed to compare for significance in difference in median of number of interactions between them.\n\n\nShow the code\nggbetweenstats(data = nodes_seafood_vis_out %>% \n                 filter(shpcountry %in% c(\"Isliandor\", \"Osterivaria\", \"Vesperanda\", \"Mawazam\", \"Merigrad\")),\n               x = shpcountry, \n               y = out_deg_centrality,\n               xlab = \"Shipping Country\", ylab = \"Out-Deg centrality\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median Out-Deg centrality across shipping Countries\") +\n  scale_y_continuous(limits = c(0, 4000)) +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\nThe P - value is above 0.05, so there is not enough statistical evidence to reject the null hypothesis that the median out-deg centrality scores between top 5 shipping countries are different.\n\n\n3.5.2 Comparison of in-deg centrality between top receiving countries\nLet us compute top 5 receiving countries in terms of in-deg centrality scores.\n\n\nShow the code\nnodes_seafood_vis_in %>% \n  group_by(rcvcountry) %>% \n  summarise(sum_in_deg = sum(in_deg_centrality)) %>% \n  arrange(desc(sum_in_deg)) %>% \n  head(n=5)\n\n\n# A tibble: 5 × 2\n  rcvcountry  sum_in_deg\n  <chr>            <dbl>\n1 Oceanus         240470\n2 Coralmarica         33\n3 Marebak              4\n4 Merigrad             1\n5 Faraluna             0\n\n\nThere are only 3 major receiving countries in our filtered dataset .\n\n\nShow the code\nggbetweenstats(data = nodes_seafood_vis_in %>% filter(rcvcountry %in% c(\"Jiraputra\", \"Oceanus\",'Coralmarica','Marebak','Mawazam')), x = rcvcountry, y = in_deg_centrality,\n               xlab = \"Shipping Country\", ylab = \"In-Deg centrality\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Median In-Deg centrality across receiving Countries\") +\n  scale_y_continuous(limits = c(0, 150)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\nThe P - value is above 0.05, so there is not enough statistical evidence to reject the null hypothesis that the median in-deg centrality scores between top 5 receiving countries are different."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "title": "Take-home_Ex02",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nIn this exercise, we have identified the key centrality actors in terms of betweenness, in-degree and out-degree. My key findings are such:\n\nCompanies high in betweenness scores ‘buy’ and ‘sell’, although they tend to focus on one of the two activities. However, this companies usually do not have the highest in and out degree scores. We can derive simple business relationships as seen in section 3.1 and to get more relationship, we can simply extend the network graph.\nCompanies with high in-degree scores tend to focus heavily on buying only and companies with highh out-degree scores tend to focus on selling only. Companies with high in/out centrality scores can share similar suppliers/ buyers. A treemap (section 3.3.3) will be able to help us see such relationship.\nHeatmaps allows us to see shipping activities for many companies at one go. We can look out for companies who have spikes in their selling and buying patterns to fish out illegal activities.\nMultiple line plots (section 3.2.2) can show us the shipping patterns between a target company and some of its source companies over time. From target companies who keeps changing source companies, we could try to investigate them to check if they are dealing with companies who kept de-registering after being caught for IUU and re-registering to continue the business.\n\nMy key takeaway from this exercise:\n\nI filtered records too early, even though some edge weights are small, but they could provide the link to finding high betweenness nodes. By chopping away these edges to early, I was unable to get any betweenness scores for the remaining network in my filtered dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home_Ex03",
    "section": "",
    "text": "Use visual analytics to understand patterns of groups in the knowledge graph and highlight anomalous groups.\n\nUse visual analytics to identify anomalies in the business groups present in the knowledge graph. Limit your response to 400 words and 5 images.\nDevelop a visual analytics process to find similar businesses and group them. This analysis should focus on a business’s most important features and present those features clearly to the user. Limit your response to 400 words and 5 images.\nMeasure similarity of businesses that you group in the previous question. Express confidence in your groupings visually. Limit your response to 400 words and 4 images.\nBased on your visualizations, provide evidence for or against the case that anomalous companies are involved in illegal fishing. Which business groups should FishEye investigate further? Limit your response to 600 words and 6 images.\nReflection: What was the most difficult aspect of working with this knowledge graph? Did you have the tools and resources you needed to complete the challenge? What additional resources would have helped you? Limit your response to 300 words"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-dictionary",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-dictionary",
    "title": "Take-home_Ex03",
    "section": "1.1 Data dictionary",
    "text": "1.1 Data dictionary\nNode Attributes:\n• type – Type of node as defined above.\n• country – Country associated with the entity. This can be a full country or a two-letter country code.\n• product_services – Description of product services that the “id” node does.\n• revenue_omu – Operating revenue of the “id” node in Oceanus Monetary Units.\n• id – Identifier of the node is also the name of the entry.\n• role – The subset of the “type” node, not in every node attribute.\n• dataset – Always “MC3”.\nEdge Attributes:\n• type – Type of the edge as defined above.\n• source – ID of the source node.\n• target – ID of the target node.\n• dataset – Always “MC3”.\n• role - The subset of the “type” node, not in every edge attribute."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-the-datasets",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-the-datasets",
    "title": "Take-home_Ex03",
    "section": "1.2 Importing the datasets",
    "text": "1.2 Importing the datasets\nImport libraries\nThe new libraries used today are :\n\njsonlite to import json file\ntidytext is to do basic text mining in R\n\n\n\nShow the code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, DT, visNetwork, lubridate, clock, tidyverse, graphlayouts,knitr,plotly, ggiraph, ggstatsplot,\nggHoriPlot, ggthemes,hrbrthemes,treemap,patchwork, ggiraph,proxy, tidytext, skimr,\nGGally, parallelPlot, tidyverse)\n\n\nLoad the MC3 dataset\n\n\nShow the code\nmc3_data <- fromJSON(\"C:/yixin-neo/ISSS608-VAA/Project/data/mc3.json\")\n\n\n\nExtracting edges\nThe code chunk below will be used to extract the links data.frame of mc3_data and save it as a tibble data.frame called mc3_edges.\n\n\nShow the code\nmc3_edges <- as_tibble(mc3_data$links) %>% \n  distinct() %>%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %>%\n  group_by(source, target, type) %>%\n    summarise(weights = n()) %>%\n  filter(source!=target) %>%\n  ungroup()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ndistinct() is used to ensure that there will be no duplicated records.\nmutate() and as.character() are used to convert the field data type from list to character.\ngroup_by() and summarise() are used to count the number of unique links.\nthe filter(source!=target) is to ensure that no record with similar source and target.\n\n\n\nThere are no duplicates in the mc3_edges dataframe.\n\n\nShow the code\nsum(duplicated(mc3_edges))\n\n\n[1] 0\n\n\n\n\nExtracting nodes\nThe code chunk below will be used to extract the nodes data.frame of mc3_data and save it as a tibble data.frame called mc3_nodes.\n\n\nShow the code\nmc3_nodes <- as_tibble(mc3_data$nodes) %>%\n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %>%\n  select(id, country, type, revenue_omu, product_services)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nmutate() and as.character() are used to convert the field data type from list to character.\nTo convert revenue_omu from list data type to numeric data type, we need to convert the values into character first by using as.character(). Then, as.numeric() will be used to convert them into numeric data type.\nselect() is used to re-organise the order of the fields.\n\n\n\nCheck for duplicates in mc3_nodes dataframe across all columns and remove them.\n\n\nShow the code\n#sum(duplicated(mc3_nodes))\n#mc3_nodes[duplicated(mc3_nodes), ]\n\n# Remove duplicates based on all columns\nmc3_nodes <- mc3_nodes[!duplicated(mc3_nodes), ]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-for-null-values",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-for-null-values",
    "title": "Take-home_Ex03",
    "section": "2.1 Check for null values",
    "text": "2.1 Check for null values"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html",
    "href": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html",
    "title": "Hands-on Exercise 16 (Week 8: Choropleth Map)",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#getting-started",
    "title": "Hands-on Exercise 16 (Week 8: Choropleth Map)",
    "section": "18.2 Getting Started",
    "text": "18.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\nShow the code\npacman::p_load(sf,tmap,tidyverse, knitr)\n\n\nWe only have to install tidyverse instead of readr, tidyr and dplyr individually.\n18.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data file. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it's PA and SZ (FK) fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile (SUBZONE_N as join key). Aim to get geometry column from shapefile.\n\n\n18.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nShow the code\nmpsz <- st_read(dsn= 'data/geospatial',\n                layer = 'MP14_SUBZONE_WEB_PL')\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\yixin-neo\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex16\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nShow the code\nclass(mpsz)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nTo examine the content of mpsz,\nunder geometry type there could be :\n\nmutlipolygon\nmultistring\npoint\n\n\n\nShow the code\nhead(mpsz,5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n18.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex. <-- aspatial file\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nShow the code\npopdata <- read_csv('data/aspatial/respopagesextod2011to2020.csv')\n\n\n\n\nShow the code\n#summary(popdata)\nglimpse(popdata)\n\n\nRows: 984,656\nColumns: 7\n$ PA   <chr> \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   <chr> \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ AG   <chr> \"0_to_4\", \"0_to_4\", \"0_to_4\", \"0_to_4\", \"0_to_4\", \"0_to_4\", \"0_to…\n$ Sex  <chr> \"Males\", \"Males\", \"Males\", \"Males\", \"Males\", \"Males\", \"Males\", \"M…\n$ TOD  <chr> \"HDB 1- and 2-Room Flats\", \"HDB 3-Room Flats\", \"HDB 4-Room Flats\"…\n$ Pop  <dbl> 0, 10, 30, 50, 0, 0, 40, 0, 0, 10, 30, 60, 0, 0, 40, 0, 0, 10, 30…\n$ Time <dbl> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,…\n\n\n\n\n18.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n18.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nShow the code\nunique(popdata$AG)\n\n\n [1] \"0_to_4\"      \"5_to_9\"      \"10_to_14\"    \"15_to_19\"    \"20_to_24\"   \n [6] \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"    \"45_to_49\"   \n[11] \"50_to_54\"    \"55_to_59\"    \"60_to_64\"    \"65_to_69\"    \"70_to_74\"   \n[16] \"75_to_79\"    \"80_to_84\"    \"85_to_89\"    \"90_and_over\"\n\n\n\n\nShow the code\npopdata2020 <- popdata %>% \n  filter(Time == 2020) %>% \n  group_by(PA,SZ,AG) %>%   #<<< to calculate sum of population of each combinatin of PA, SZ and AG \n  summarise (POP = sum(Pop))\n\nkable(head(popdata2020,5))\n\n\n\n\n\nPA\nSZ\nAG\nPOP\n\n\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n0_to_4\n170\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n10_to_14\n280\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n15_to_19\n340\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n20_to_24\n270\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n25_to_29\n260\n\n\n\n\n\n\n\nShow the code\npopdata2020 <- popdata %>% \n  filter(Time == 2020) %>% \n  group_by(PA,SZ,AG ) %>%   #<<< to calculate sum of population of each combinatin of PA, SZ and AG \n  summarise (POP = sum(Pop))  %>%  #<< at this point, we only have four columns (PA, SZ, AG, POP)\n  ungroup() %>%  #<<< ungroup to release the grouping and continue with other wrangling\n  pivot_wider(names_from = AG,\n              values_from = POP)\nkable(head(popdata2020,5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPA\nSZ\n0_to_4\n10_to_14\n15_to_19\n20_to_24\n25_to_29\n30_to_34\n35_to_39\n40_to_44\n45_to_49\n50_to_54\n55_to_59\n5_to_9\n60_to_64\n65_to_69\n70_to_74\n75_to_79\n80_to_84\n85_to_89\n90_and_over\n\n\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n170\n280\n340\n270\n260\n310\n330\n400\n480\n380\n310\n230\n290\n250\n240\n130\n100\n30\n10\n\n\nAng Mo Kio\nCheng San\n1060\n1040\n1160\n1330\n1720\n2020\n2150\n2080\n2200\n2050\n2130\n1050\n2110\n2180\n1750\n960\n650\n340\n170\n\n\nAng Mo Kio\nChong Boon\n850\n1020\n1070\n1310\n1610\n1890\n1720\n1810\n1820\n1900\n2100\n850\n2150\n2100\n1800\n1120\n800\n430\n220\n\n\nAng Mo Kio\nKebun Bahru\n680\n960\n1010\n1170\n1410\n1420\n1440\n1630\n1810\n1720\n1800\n800\n1780\n1710\n1450\n830\n630\n350\n150\n\n\nAng Mo Kio\nSembawang Hills\n210\n400\n450\n500\n500\n340\n300\n370\n550\n540\n550\n320\n480\n410\n360\n230\n150\n100\n60\n\n\n\n\n\n\n\nShow the code\npopdata2020 <- popdata %>% \n  filter(Time == 2020) %>% \n  group_by(PA,SZ,AG ) %>%   #<<< to calculate sum of population of each combinatin of PA, SZ and AG \n  summarise (POP = sum(Pop))  %>%  #<< at this point, we only have four columns (PA, SZ, AG, POP)\n  ungroup() %>%  #<<< ungroup to release the grouping and continue with other wrangling\n  pivot_wider(names_from = AG,\n              values_from = POP) %>% \n  mutate(YOUNG = rowSums(.[3:5]) + rowSums(.[6])) %>% \n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:15])) %>% \n  mutate(AGED = rowSums(.[16:21])) %>% \n  mutate(TOTAL = rowSums(.[3:21])) %>% \n  mutate(DEPENDENCY = (YOUNG + AGED) / `ECONOMY ACTIVE`) %>% \n  select(PA, SZ, YOUNG, \n         `ECONOMY ACTIVE`, AGED,\n         TOTAL, DEPENDENCY)\n kable(head(popdata2020,5)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPA\nSZ\nYOUNG\nECONOMY ACTIVE\nAGED\nTOTAL\nDEPENDENCY\n\n\n\n\nAng Mo Kio\nAng Mo Kio Town Centre\n1060\n2990\n760\n4810\n0.6086957\n\n\nAng Mo Kio\nCheng San\n4590\n17510\n6050\n28150\n0.6076528\n\n\nAng Mo Kio\nChong Boon\n4250\n15850\n6470\n26570\n0.6763407\n\n\nAng Mo Kio\nKebun Bahru\n3820\n13810\n5120\n22750\n0.6473570\n\n\nAng Mo Kio\nSembawang Hills\n1560\n3950\n1310\n6820\n0.7265823\n\n\n\n\n\n\n\n\n18.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow the code\npopdata2020 <- popdata2020 %>% \n  mutate_at(.var = vars(PA, SZ),\n            .funs = funs(toupper)) %>% \n  filter(`ECONOMY ACTIVE` >0)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nmutate_at function: This function is used to modify multiple columns in a dataframe simultaneously. It allows you to specify the columns to be transformed and the transformation function to be applied.\n.var argument: It specifies the columns to be transformed. In this case, the columns are “PA” and “SZ” from the “popdata2020” dataframe.\n.funs argument: It specifies the transformation function(s) to be applied to the selected columns. In this case, the function toupper is used, which converts the text to uppercase.\n\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N (mpsz) and SZ (popdata2020) as the common identifier.\nThe geometry column from mpsz sf file is shifted all the way to the back after left join. The output table is a sf file because mpsz (sf obj) is the left table.\n\n\nShow the code\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c('SUBZONE_N' ='SZ'))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nWrite the mpsz_pop2020 as a rds file\n\n\nShow the code\nwrite_rds(mpsz_pop2020, 'data/rds/mpszpop2020.rds')\n\n\n\n\nShow the code\nplot(mpsz_pop2020)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 16 (Week 8: Choropleth Map)",
    "section": "18.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "18.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements. (More fine controls)\n\n\n18.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nShow the code\ntmap_mode('plot')\n#tmap_mode('view')\n#tmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020,\n    fill = 'DEPENDENCY')\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ntmap_mode() with \"plot\" option is used to produce a static map. For interactive mode, \"view\" option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\n\n18.4.2 Creating a choropleth map by using tmap's elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap's drawing elements should be used.\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill('DEPENDENCY',\n          style = 'quantile', #<< affect how values of DEPENDENCY is binned\n          palette = 'Blues',\n          title = \"Dependency ratio\") +\n tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n           main.title.position = \"center\",\n           main.title.size = 1.2,\n           legend.height = 0.45, \n           legend.width = 0.35,\n           frame = TRUE) +\n  \n  tm_legend(position = c('right','bottom'),\n            frame= TRUE) +\n\n  tm_borders(alpha = 0.5) + #<< the outline of each SZ\n  tm_compass(type=\"8star\", size = 2) + #<< compass\n  tm_scale_bar() + #<< the BW scale bar\n  tm_grid(alpha =0.2) + #<< the plot grid lines\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nShow the code\nquantile(mpsz_pop2020$DEPENDENCY, \n         probs = seq(0,1,0.2),\n         na.rm=TRUE)\n\n\n        0%        20%        40%        60%        80%       100% \n 0.0000000  0.5000000  0.5514706  0.5888009  0.6576000 19.0000000 \n\n\n\n\n\n\n18.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n18.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\") + \n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default interval binning used to draw the choropleth map is called \"pretty\". A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n21.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1, #<< linewidth \n             alpha = 1,\n             lty= 'dashed') #<< transparency\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is \"solid\".\n\n\n\n\n18.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n21.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.5240  0.5691  0.6495  0.6451 19.0000      92 \n\n\n\n\nShow the code\nquantile(mpsz_pop2020$DEPENDENCY, \n         probs = seq(0,1,0.25),\n         na.rm=TRUE)\n\n\n        0%        25%        50%        75%       100% \n 0.0000000  0.5240379  0.5690842  0.6450497 19.0000000 \n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,   # <<< 4 quantiles\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nWarning: Maps Lie!\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nFixed intervals\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          #n = 5,\n          style = \"fixed\",\n          breaks = c(0,15,19) ) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          #n = 5,\n          style = \"fixed\",\n          breaks = c(0,0.5,1,6,8,10,12,14,16,18,20) ) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nKmeans\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n21.4.3.2 Plotting choropleth map with custom break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.5240  0.5691  0.6495  0.6451 19.0000      92 \n\n\n\n\nShow the code\nquantile(mpsz_pop2020$DEPENDENCY, \n         probs = seq(0,1,0.25),\n         na.rm=TRUE)\n\n\n        0%        25%        50%        75%       100% \n 0.0000000  0.5240379  0.5690842  0.6450497 19.0000000 \n\n\n---- TBC on break points -\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n----------------------------------"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex16/Hands-on_Ex16.html#reference",
    "title": "Hands-on Exercise 16 (Week 8: Choropleth Map)",
    "section": "18.5 Reference",
    "text": "18.5 Reference\n\n18.5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n18.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n18.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with 'spread()' and 'gather()' Functions"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-edges-data-frame",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-edges-data-frame",
    "title": "Take-home_Ex03",
    "section": "2.1 Exploring the edges data frame",
    "text": "2.1 Exploring the edges data frame\nIn the code chunk below, skim() of skimr package is used to display the summary statistics of mc3_edges tibble data frame.\n\n\nShow the code\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n24036\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n700\n0\n12856\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nweights\n0\n1\n1\n0\n1\n1\n1\n1\n1\n▁▁▇▁▁\n\n\n\n\n\nThe report above reveals that there are no missing values in all fields.\nWhy are there source columns with maximum character length of 700?\nChecking for the longest length value in source column of edge file.\n\n\nShow the code\n# Find the index of the value with the maximum length\nmax_index <- which.max(nchar(mc3_edges$source))\n\n# Extract the value with the maximum length in `source` column\nmax_value <- mc3_edges$source[max_index]\nmax_value\n\n\n[1] \"c(\\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\n\\\"Baltic Sprat Incorporated Investment\\\", \\\"Baltic Sprat Incorporated Investment\\\", \\\"Water World Limited Liability Company Freight\\\", \\\"Water World Limited Liability Company Freight\\\")\"\n\n\nIt seems like we found more issue with the mc3_edge dataframe. Some of the source column values are still in a list format. We need to unlist the source companies that are hidden.\n\n\n\n\n2.1.1 Cleaning the edges data frame\nNow, I would like to split into mc3_edges dataframe into two dataframes, where\ndf1 : containing rows where the source does NOT contain “c(” <- actual source value, no further processing needed\ndf2 : containing rows where the source contains values starting with “c(” . As the source column still contains inner list of source entities , it still needs to be unlisted further.\n\n\nShow the code\n# Creating an empty data frame for the two subsets\ndf1 <- data.frame()\ndf2 <- data.frame()\n\n# Looping through each row of the original data frame\nfor (i in 1:nrow(mc3_edges)) {\n  if (!grepl(\"c\\\\(\", mc3_edges$source[i])) {\n    # Append the row to df1 if source does not contain \"c(\"\n    df1 <- rbind(df1, mc3_edges[i, ])\n  } else {\n    # Append the row to df2 if source contains \"c(\"\n    df2 <- rbind(df2, mc3_edges[i, ])\n  }\n}\n\n\ndf1 dataframe\n\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\n1 AS Marine sanctuary\nChristina Taylor\nCompany Contacts\n1\n\n\n1 AS Marine sanctuary\nDebbie Sanders\nBeneficial Owner\n1\n\n\n1 Ltd. Liability Co Cargo\nAngela Smith\nBeneficial Owner\n1\n\n\n1 S.A. de C.V.\nCatherine Cox\nCompany Contacts\n1\n\n\n1 and Sagl Forwading\nAngela Mendoza\nCompany Contacts\n1\n\n\n\n\n\ndf2 dataframe\n\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\nc(“1 Ltd. Liability Co”, “1 Ltd. Liability Co”)\nYesenia Oliver\nCompany Contacts\n1\n\n\nc(“1 Swordfish Ltd Solutions”, “1 Swordfish Ltd Solutions”, “Saharan Coast BV Marine”, “Olas del Sur Estuary”)\nDaniel Reese\nCompany Contacts\n1\n\n\nc(“5 Limited Liability Company”, “Bahía de Coral Kga”)\nBrittany Jones\nBeneficial Owner\n1\n\n\nc(“5 Limited Liability Company”, “Bahía de Coral Kga”)\nElizabeth Torres\nBeneficial Owner\n1\n\n\nc(“5 Limited Liability Company”, “Bahía de Coral Kga”)\nSandra Roberts\nCompany Contacts\n1\n\n\n\n\n\nWe would need to clean up the df2 dataframe’s source column.\nThe code chunk below uses the\n\ngsub function to remove the unwanted characters from the source column. It replaces any occurrence of \" or c( or ) with an empty string, ’’\ntrimws function to remove any leading or trailing whitespace from the cleaned source column,\n\\\\: The backslash is an escape character in regular expressions. In this case, it is used to escape the closing parenthesis character, so it is treated as a literal character in the pattern.\n\n\n\nShow the code\ndf2$source <- gsub('[\"c(\\\\)]', '', df2$source)\ndf2$source <- trimws(df2$source)\nkable(head(df2,5))\n\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\n1 Ltd. Liability Co, 1 Ltd. Liability Co\nYesenia Oliver\nCompany Contacts\n1\n\n\n1 Swordfish Ltd Solutions, 1 Swordfish Ltd Solutions, Saharan Coast BV Marine, Olas del Sur Estuary\nDaniel Reese\nCompany Contacts\n1\n\n\n5 Limited Liability Company, Bahía de Coral Kga\nBrittany Jones\nBeneficial Owner\n1\n\n\n5 Limited Liability Company, Bahía de Coral Kga\nElizabeth Torres\nBeneficial Owner\n1\n\n\n5 Limited Liability Company, Bahía de Coral Kga\nSandra Roberts\nCompany Contacts\n1\n\n\n\n\n\nFor each row, extract each entity in Source column and insert this entity as a new row.\n\n\nShow the code\n# Create a new data frame for the modified rows\ndf2_modified <- data.frame()\n\n# Loop through each row of df2\nfor (i in 1:nrow(df2)) {\n  # Split the source value by comma\n  source_values <- unlist(strsplit(df2$source[i], \", \"))\n  \n  # Create a new row for each source value\n  for (value in source_values) {\n    # Create a new row with the same \"target\", \"type\", and \"weights\" values\n    new_row <- data.frame(\n      source = value,\n      target = df2$target[i],\n      type = df2$type[i],\n      weights = df2$weights[i]\n    )\n    \n    # Append the new row to df2_modified\n    df2_modified <- rbind(df2_modified, new_row)\n  }\n}\n\n# Print the modified data frame\n#cat(\"df2_modified:\\n\")\nkable(head(df2_modified,10))\n\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\n1 Ltd. Liability Co\nYesenia Oliver\nCompany Contacts\n1\n\n\n1 Ltd. Liability Co\nYesenia Oliver\nCompany Contacts\n1\n\n\n1 Swordfish Ltd Solutions\nDaniel Reese\nCompany Contacts\n1\n\n\n1 Swordfish Ltd Solutions\nDaniel Reese\nCompany Contacts\n1\n\n\nSaharan Coast BV Marine\nDaniel Reese\nCompany Contacts\n1\n\n\nOlas del Sur Estuary\nDaniel Reese\nCompany Contacts\n1\n\n\n5 Limited Liability Company\nBrittany Jones\nBeneficial Owner\n1\n\n\nBahía de Coral Kga\nBrittany Jones\nBeneficial Owner\n1\n\n\n5 Limited Liability Company\nElizabeth Torres\nBeneficial Owner\n1\n\n\nBahía de Coral Kga\nElizabeth Torres\nBeneficial Owner\n1\n\n\n\n\n\nQuestion to think about: ‘Do we aggregate the weights for each ’source’ -‘target’ pair or treat them as duplicates?\nFor now, let us aggregate and store the number of occurrence in ‘weights’ column. Now we have df2_modified dataframe where each ‘source-target’ pair is unique and have no duplicates.\n\n\nShow the code\ndf2_modified <- df2_modified %>% \n  group_by(source, target, type) %>% \n  summarise(weights = sum(weights)) %>% \n  ungroup() %>% \n  arrange(desc(weights))\n\nkable(head(df2_modified))\n\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\nNiger River Delta S.p.A.\nCole Allen\nCompany Contacts\n20\n\n\nNiger River Delta S.p.A.\nShawn Myers\nCompany Contacts\n20\n\n\nBalti Sprat Inorporated Investment\nJose Ramirez\nCompany Contacts\n14\n\n\nCape Verde Islands Pl Otter\nDuane Edwards\nCompany Contacts\n14\n\n\nCape Verde Islands Pl Otter\nJill Newman\nBeneficial Owner\n14\n\n\nGreek Makerel Ltd. Corporation Express\nMegan Wyatt\nCompany Contacts\n13\n\n\n\n\n\nFinally, we will appended df1 below df2_modified to get our edges_combined data frame, now with 24,935 rows after unlisting all source entities.\nIn the code chunk below, datatable() of DT package is used to display mc3_edges tibble data frame as an interactive table on the html document.\n\n\nShow the code\n# Append df1 to df2_modified\nedges_combined <- rbind(df2_modified, df1)\n\n# Print the combined data frame\nDT::datatable(edges_combined)\n\n\n\n\n\n\n\nLet us use bar chart to visualise the type column. (Improve on this bar chart later)\n\n\nShow the code\nggplot(data = edges_combined,\n       aes(x = type)) +\n  geom_bar()\n\n\n\n\n\nThe edges_combined df contains relationship between individuals and companies, and describe whether an individual is a Beneficial Owner or a Company Contact of a company.\n\n\n2.1.2 Build a node dataframe using new edge dataframe\nRebuild a brand new mc3_nodes1 dataframe (enhanced version of mc3_nodes dataframe), by appending unique target and source entities from the edges_combined dataframe to get a column id.\nThis newly created dataframe only has 1 column id. We can bring in nodes attributes like country, type, revenue_omu, product_services from the original mc3_nodes dataframe into the mc3_nodes1 dataframe.\n\n\nShow the code\nid1 <- edges_combined %>%\n  select(source) %>%\n  rename(id = source)\nid2 <- edges_combined %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes1 <- rbind(id1, id2) %>%\n  distinct() %>%                  #<< ensures no duplicated in `id` column\n  left_join(mc3_nodes, by='id',\n            unmatched = \"drop\")  #<<< bring in 4 node attributes from `mc3_nodes` df\n\n\nAfter the left join, mc3_nodes1 df increased in number of rows from 34,443 to 35,767 rows because entities like Adams LLC has both company contacts and beneficial owner types in mc3_nodes df.\nLet us take a sneak peak at the mc3_nodes1 df now. We notice the following:\n\n‘Andrew Yu’ does not have value in the type column. However, its type value can be retrieved from the edges_combined df.\n‘Andrews PLC’ as a single entity has two types, namely ‘Company Contacts’ and ‘Beneficial Owners’.\n\n\n\nShow the code\nmc3_nodes1 %>% \n  arrange(id) %>% \n  slice(1563:1570) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry\ntype\nrevenue_omu\nproduct_services\n\n\n\n\nAndrew Woodward\nNA\nNA\nNA\nNA\n\n\nAndrew Yu\nNA\nNA\nNA\nNA\n\n\nAndrews LLC\nZH\nCompany\n72609.01\nFood preparations and kindred products\n\n\nAndrews Ltd\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\nAndrews PLC\nZH\nBeneficial Owner\nNA\ncharacter(0)\n\n\nAndrews PLC\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\nAndrews and Sons\nZH\nCompany\nNA\nUnknown\n\n\nAndrews and Sons\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\n\n\n\nLooking into the edges_combined df where the type attribute of ‘Andrew Yu’ is stored.\nHe is a beneficial owner of ‘Mar del Paraíso GmbH’.\n\n\nShow the code\nedges_combined %>% \n  filter(grepl('Andrew Yu', target)) %>% \n  kable()\n\n\n\n\n\nsource\ntarget\ntype\nweights\n\n\n\n\nMar del Paraíso GmbH\nAndrew Yu\nBeneficial Owner\n1\n\n\n\n\n\nWe would like to ingest his relationship type with ‘Mar del Paraíso GmbH’ into the mc3_nodes1 dataframe.\nWe can further enrich each id’s type attribute of the mc3_nodes1 dataframe by extracting the type values of the target entity from the edges_combined dataframe. The number of records further increased from 35,767 to 39,437 because each target entity in edges_combined df can have more than 1 type. An example is ‘Aaron Garcia’ - who is both a company contacts and a beneficial owner type to a company.\n\n\nShow the code\n# Perform a left join to bring in the \"type\" values from 'edges_combined` df to 'mc3_nodes1` df\n\nmerged_df <- left_join(mc3_nodes1, edges_combined, by = c(\"id\" = \"target\"))\n\n# Replace NA values in the \"type.x\" column with non-NA values from the \"type.y\" column\nmerged_df$type.x[is.na(merged_df$type.x)] <- merged_df$type.y[is.na(merged_df$type.x)]\n\n# Select the relevant columns and rename \"type.x\" to \"type\"\nmerged_df <- merged_df %>%\n  select(id, country, type = type.x, revenue_omu, product_services)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk above is used to replace the NA values in the “type.x” column of the merged_df dataframe with non-NA values from the “type.y” column.\n\nmerged_df$type.x refers to the “type.x” column of the merged_df dataframe. This is the column where we want to replace the NA values.\nis.na(merged_df$type.x) creates a logical vector with TRUE for NA values and FALSE for non-NA values in the “type.x” column.\nmerged_df$type.y[is.na(merged_df$type.x)] uses the logical vector as an index to select only the non-NA values from the “type.y” column corresponding to the NA values in the “type.x” column.\n\n\n\nNote that the type attribute of ‘Andrew Yu’ and similar others have been added to this df.\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry\ntype\nrevenue_omu\nproduct_services\n\n\n\n\nAndrew Woodward\nNA\nBeneficial Owner\nNA\nNA\n\n\nAndrew Yu\nNA\nBeneficial Owner\nNA\nNA\n\n\nAndrews LLC\nZH\nCompany\n72609.01\nFood preparations and kindred products\n\n\nAndrews Ltd\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\nAndrews PLC\nZH\nBeneficial Owner\nNA\ncharacter(0)\n\n\nAndrews PLC\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\nAndrews and Sons\nZH\nCompany\nNA\nUnknown\n\n\nAndrews and Sons\nZH\nCompany Contacts\nNA\ncharacter(0)\n\n\n\n\n\nCheck for duplicates. There are once again duplicates after mc3_nodes1 left outer join with edges_combined df because in edges_combined df, there are many relationship types for each individual and its associated company. (E.g check Cole Allen).\n\n\n\nWe will drop the duplicates in merged_df df, leaving us with 36,731 rows. merged_df is now an enhanced version of my main nodes file. It contains nodes (seafood and non-seafood related) that are present in the edges_combined df. In addition, node attribute type column was also enhanced into this df from edges_combined df. Using datatable() of DT pacakge, let us take a look at merged_df as an interactive table on the html document using.\n\n\nShow the code\nmerged_df <- merged_df[!duplicated(merged_df), ]  # 36,731 rows\nDT::datatable(merged_df)\n\n\n\n\n\n\n\nFinally, the nodes and edges dataframes are ready! We will use the merged_df as our main nodes file and edges_combined as main edges file from now. Before further cleaning, with what we have now, lets build an initial visualisation of the network.\n\n\n2.1.3 Building network model with tidygraph\nFirst, create a graph object using tbl_graph() function. Then calculate betweenness and closeness centrality scores.\n\n\nShow the code\nmc3_graph <- tbl_graph(nodes = merged_df,\n                       edges = edges_combined,\n                       directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness())\n\nmc3_graph <- mc3_graph %>% \n  mutate(membership = components(mc3_graph)$membership)\n\n\n\n\n\n\n\n2.1.4 Visualising network graph\nWe will filter nodes with high betweenness centrality scores (>2,000,000) and visualise them to see the relationships that they have.\n\n\nShow the code\nset.seed(1234)\nmc3_graph %>%\n  filter(betweenness_centrality >= 2000000) %>%\nggraph(layout = \"fr\") +\n  geom_edge_link(aes(#width= weights,\n                     alpha=0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    #color = type,\n    alpha = 0.3)) +\n  geom_node_label(aes(label = id),repel=TRUE, size=2.5, alpha = 0.8) +\n  scale_size_continuous(range=c(1,10)) +\n  theme_graph() +\n  labs(title = 'Initial network visualisation \\n(Seafood and non-seafood)',\n       subtitle = 'Entities with betweenness scores > 2,000,000')\n\n\n\n\n\nBelow is a dataframe showing us the top 10 entities with the highest betweenness scores.\n\n\nShow the code\nmc3_graph %>%  activate(nodes) %>%  as_tibble() %>% arrange(desc(betweenness_centrality)) %>% slice(1:10) %>% kable() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry\ntype\nrevenue_omu\nproduct_services\nbetweenness_centrality\ncloseness_centrality\nmembership\n\n\n\n\nWave Warriors S.A. de C.V. Express\nLumindoria\nCompany\n1761580.75\nBeer, ale, and malt liquors, as well as nonalcoholic beer and other related products\n3981896\n1.31e-05\n6\n\n\nDutch Oyster Sagl Cruise ship\nMarebak\nCompany\n62913.42\nImport and export of textiles, knitwear and raw materials\n3878339\n1.34e-05\n6\n\n\nSenegal Coast Ltd. Liability Co\nOceanus\nCompany\nNA\nUnknown\n3725057\n1.34e-05\n6\n\n\nLimpopo River Ltd. Liability Co\nMarebak\nCompany\n12065.26\nMeat and processed meat products\n3632403\n1.30e-05\n6\n\n\nOcean Observers Marine mist\nPuerto Sol\nCompany\n39678.54\nTransportation and other related services\n3559968\n1.24e-05\n6\n\n\nMatthew Reynolds\nNA\nCompany Contacts\nNA\nNA\n3484895\n1.31e-05\n6\n\n\nNiger Bend AS Express\nPuerto Sol\nCompany\n613590.73\nCruise ship holidays\n3479011\n1.32e-05\n6\n\n\nLuangwa River Limited Liability Company Holdings\nSol y Oceana\nCompany\nNA\nChemicals and allied products, such as acids, industrial and heavy chemicals, dyestuffs, industrial salts, rosin, and turpentine\n3305332\n1.23e-05\n6\n\n\nJennifer Smith\nNA\nBeneficial Owner\nNA\nNA\n3142993\n1.25e-05\n6\n\n\nCoral del Mar SE United\nOceanus\nCompany\nNA\nUnknown\n3133167\n1.34e-05\n6\n\n\n\n\n\nThe top 10 betweenness entities above are not dealing with seafood related industries directly. In the next section, we will filter entities from the merged_df dataframe for only seafood related entities. We may revisit the non-seafood entities later when we have specific targets/companies to investigate.\nHIVE plot (Later )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-with-tidytext",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-with-tidytext",
    "title": "Take-home_Ex03",
    "section": "3.1 Text Sensing with tidytext",
    "text": "3.1 Text Sensing with tidytext\nIn section 2.1.2, we saw in merged_df dataframe that the product_services column contains raw text data on products that each entity provides. We would like to give each company a meaningful label based on its product_services.\nHence in this section, we will perform basic text sensing using appropriate functions of tidytext package.\n\n3.1.1 Simple word count\nThe code chunk below calculates number of times the word fish appeared in the field product_services.\n\n\nShow the code\nmerged_df %>% \n    mutate(n_fish = str_count(product_services, \"fish\")) %>% arrange(desc(n_fish)) %>% head()\n\n\n# A tibble: 6 × 7\n  id                     country type  revenue_omu product_services type1 n_fish\n  <chr>                  <chr>   <chr>       <dbl> <chr>            <fct>  <int>\n1 Gvardeysk Sextant ОАО… Uziland Comp…      73027. Fish salads (It… Comp…     11\n2 Taylor LLC             ZH      Comp…     138982. Fish (anchovy, … Comp…     11\n3 SeaSelect Foods Salt … Marebak Comp…      41902. European whole … Comp…      7\n4 Samaka Chart ОАО Deli… Nalako… Comp…      16207. Live crayfish, … Comp…      6\n5 suō yú Ltd. Liability… Coralm… Comp…      31567. Offers a wide r… Comp…      6\n6 Arunachal Pradesh s S… Marebak Comp…      60346. Offers a wide r… Comp…      6\n\n\n\n\n3.1.2 Tokenisation\nThe word tokenisation have different meaning in different scientific domains. In text sensing, tokenisation is the process of breaking up a given text into units called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenisation, some characters like punctuation marks may be discarded. The tokens usually become the input for the processes like parsing and text mining.\nIn the code chunk below, unnest_token() of tidytext is used to split text in product_services columninto words.\n\n\nShow the code\ntoken_nodes <- merged_df %>%\n  mutate(product_services = ifelse(is.na(product_services), 'unknown', product_services)) %>% \n  unnest_tokens(word, \n                product_services)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe two basic arguments to unnest_tokens() used here are column names.\nFirst we have the output column name that will be created as the text is unnested into it (word, in this case), and then the input column that the text comes from (product_services, in this case).\nBefore tokenising, the NA values under product_services have been replaced by ‘unknown’ string.\nBy default, punctuation has been stripped.\nBy default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).\n\n\n\n\n\n\n\n\n3.1.3 Removing stopwords\nWe will use the tidytext package’s function called stop_words that will help us clean up stop words. In addition, we can add in additional stopwords in the stopwords list.\n\n\nShow the code\n# Create a new dataframe with customised stopwords\nnew_stopwords <- data.frame(word = c(\"unknown\", \"character\", \"0\", \"products\",\"range\", \"offers\",\"including\"))\n\n# Add the new stopwords to the existing stop_words dataframe\nstop_words <- bind_rows(stop_words, new_stopwords)\n\n# remove stopwords from `token_nodes`\nstopwords_removed <- token_nodes %>% \n  anti_join(stop_words)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two processes:\n\nLoad the stop_words data included with tidytext. This data is simply a list of words that you may want to remove in a natural language analysis.\nThen anti_join() of dplyr package is used to remove all stop words from the analysis.\n\n\n\nNow we can visualise the words extracted by using the code chunk below.\n\n\nShow the code\nstopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  top_n(50) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Count\",\n      y = \"Unique words\",\n      title = \"Count of unique words found in product_services field\") +\n   theme(axis.text.y = element_text(size = 7.5, hjust = 0))\n\n\n\n\n\nThe diagram above shows that some words like ‘fish’, ‘seafood’, ‘salmon’ are related to the seafood industry and we could tag companies containing these words to the label ‘seafood’. The non-seafood related industry companies could be tag to the label ‘others’ while missing product services could be tag to ‘unknown’."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#preparing-a-master-node-dataframe-with-unique-id-per-row",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#preparing-a-master-node-dataframe-with-unique-id-per-row",
    "title": "Take-home_Ex03",
    "section": "3.2 Preparing a master node dataframe with unique id per row",
    "text": "3.2 Preparing a master node dataframe with unique id per row\n\n3.2.1 Adding a label column using product_services column\nCreating nodes_all_notagg dataframe where this df contains a new column called label that groups all the fishing related companies together and non-seafood related companies together. There are three categories in label column namely ‘seafood’, ’ others’, ‘unknown’. nodes_all_notagg dataframe is created from merged_df.\nNote that in nodes_all_notagg dataframe , there can be duplicated ids bacause an entity can be associated to more than 1 country, 1 relationship type and 1 product labels group.\n(Refer to ‘Manipur Market Ltd. Liability Co’ if needed)\n\n\nShow the code\n#library(stringr)\n\n# Define the seafood keywords\nseafood_keywords <- c(\"sea food\", \"seafood\", \"fish\", \"prawn\", \"shrimp\", \"shell\", \"crab\", \"lobster\", \"mussel\", \"cavier\", \"oyster\", \"octopus\", \"squid\", \"aquatic\", \"crayfish\", \"tuna\", \"salmon\", \"scallop\", \"mackerel\", \"trout\", \"sardine\", \"winkle\", \"Barramundi\", \"tilapia\")\n\n# Add the 'label' column based on the 'product_services' column\nnodes_all_notagg <- merged_df %>%\n  mutate(product_services = ifelse(is.na(product_services), 'Unknown', product_services)) %>% \n  mutate(label = ifelse(product_services == 'Unknown' | product_services == 'character(0)', 'unknown',\n                        ifelse(str_detect(product_services, regex(paste(seafood_keywords, collapse = \"|\"), ignore_case = TRUE)), 'seafood', 'others')))\n\n\nIn the code chunk above, we first check if product_services is equal to either “Unknown” or “character(0), if true, ‘label’ is set to ‘unknown’.\nNext, if product_services contains seafood_keywords, then ‘label’ is set to ‘seafood’.\nIf none of the previous conditions are met, ‘label’ is set to ‘others’.\n\n\n\n\n\n\nNote\n\n\n\nnodes_all_notagg df can be used with edges_combined edge file\n\n\nVisualising the newly created label column using bar charts. The results are as expected because most of the Beneficial Owners and Company Contacts do not have values under the product_services column.\n\n\nShow the code\nggplot(data = nodes_all_notagg,\n       aes(x = reorder(label,label,\n                       function(x) + length(x)))) +\n  geom_bar(fill='lightblue') +\n  ylim(0,50000) +\n  coord_flip() +\n  geom_text(stat=\"count\", \n            aes(label=paste0(..count.., \", \", \n                             round(..count../sum(..count..)*100, 1), \"%\")),\n            hjust= -0.1,\n            size = 5) +\n\n  theme_minimal() +\n  \n  labs(x = 'Product services labels',\n       title = 'Distribution of labels') +\n  theme(plot.title = element_text(size = 22,\n                                  face='bold', \n                                  hjust = 0.5),\n        axis.title.x=element_text(size= 20,\n                                  hjust = 0.5),\n        axis.title.y=element_text(size= 20),\n        axis.text.x = element_text(size = 15, \n                                  color = \"black\", \n                                  angle = 0, \n                                  hjust = 0.5, \n                                  vjust = 0.5),\n        axis.text.y = element_text(size = 15, \n                                  color = \"black\", \n                                  angle = 0, \n                                  hjust = 0.5, \n                                  vjust = 0.5),\n        panel.grid.major.y = element_blank() )\n\n\n\n\n\n\n\n3.2.2 Aggregation to ensure no duplicates of id in master node df\nAs explained earlier , the entity names in the id column of the nodes_all_notagg dataframe is not unique.\nThus from nodes_all_notagg dataframe, we will create an aggregated version of it called nodes_all_agg dataframe (our master node df) , where there is only 1 instance of each id. It would mean aggregation is needed in order not to lose any information.\nFirst, group by id and create new columns :\ncountry_qty : number of countries that an entity is associated with\ntype_qty : number of business relationship types an entity has\nrevenue_sum: total revenue of entity\nlabel_qty: number of categories of products associated with entity\ncountry_concat : list of all countries associated with entity\ntype_concat : list of all business relationship types associated with entity\nlabel_concat : list of all product labels associated with entity\n\n\nShow the code\n nodes_all_agg <- nodes_all_notagg %>%\n  group_by(id) %>%\n  summarise(country_qty = ifelse(all(is.na(country)), 0, n_distinct(country)),\n            type_qty = ifelse(all(is.na(type)), 0, n_distinct(type)),\n            label_qty = n_distinct(label),\n            revenue_sum = sum(revenue_omu, na.rm=TRUE),\n            country_concat = ifelse(country_qty > 1, paste(unique(country), collapse = ', '), country[1]),\n            type_concat = ifelse(type_qty > 1, paste(unique(type), collapse = ', '), type[1]),\n            label_concat = ifelse(label_qty > 1, paste(unique(label), collapse = ', '), label[1]),\n            product_services_concat = paste(product_services, collapse = \"| \")\n            )\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe codes above checks for each entity\n\ncountry values contains all null. If so, country_qyt column is 0. Otherwise, the number of distinct countries is computed using n_distinct().\nthe number of distinct countries (country_qty) is greater than 1. If it is, it uses paste() with unique(country) to concatenate only the unique country values, separated by a comma and a space. Otherwise, it simply uses the first country value in the group (country[1]). This ensures that the country values are concatenated only if they are different within each group.\n\n\n\nTake a peek at a few rows in `nodes_all_agg’. It contains information of seafood and non-seafood related entities and has 34,442 rows in total.\n\n\nShow the code\nnodes_all_agg %>% arrange(desc(country_qty)) %>% slice(100:105) %>%  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry_qty\ntype_qty\nlabel_qty\nrevenue_sum\ncountry_concat\ntype_concat\nlabel_concat\nproduct_services_concat\n\n\n\n\nOceanfront Oasis GmbH & Co. KG Carriers\n2\n1\n2\n138105.75\nBrindivaria, Oceanus\nCompany\nothers, unknown\nLotions and other skin care products, cosmetics, perfumes and toilet preparations, and other personal hygiene products| Unknown\n\n\nOcéano del Este ОАО\n2\n1\n2\n136636.13\nSolovarossa, Oceanus\nCompany\nseafood, unknown\nOffers a wide range of products such as salmon and salmon eggs, parr and smolt, and various other species, including cobia and cod juveniles| Unknown\n\n\nOdisha Ltd. Liability Co\n2\n1\n2\n93427.09\nOceanus, Marebak\nCompany\nunknown, seafood\nUnknown| A range of fish and other related seafood products| Unknown\n\n\nOka Ltd. Corporation Transport\n2\n1\n1\n40152.33\nKorvelonia, Oceanus\nCompany\nseafood\nFish and fish products| Pink and chum salmon\n\n\nOla Azul Ges.m.b.H. Services\n2\n1\n2\n59994.96\nNalakond, Zawalinda\nCompany\nseafood, unknown\nSeafood product preparation and packaging| Unknown\n\n\nOla del Mar SRL\n2\n1\n2\n33526.70\nZawalinda, Puerto del Mar\nCompany\nothers, unknown\nPrimarily involved in providing air, surface, or combined courier delivery services of parcels generally between metropolitan areas or urban centers| Unknown\n\n\n\n\n\n\n\n3.2.3 seafood_entities df\nIn this section, we will subset the nodes_all_agg dataframe above by filtering label == ‘seafood’ to extract only seafood-related entities information.\nFilter for ids with label_concat column containing seafood\n\n\n\nTake a peek at a few rows in seafood_entities. There are 651 seafood related entities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry_qty\ntype_qty\nlabel_qty\nrevenue_sum\ncountry_concat\ntype_concat\nlabel_concat\nproduct_services_concat\n\n\n\n\nBahía de Plata Trout\n1\n1\n1\n16349.68\nRio Isla\nCompany\nseafood\nGrocery products (Canned and frozen foods, milk, fresh fruits and vegetables, fresh and prepared meats, fish, poultry and poultry products); Other household products (alcohol, household cleaning products, medicine, and clothes)\n\n\nBahía del Sol Deep-sea\n1\n1\n1\n36095.44\nNalakond\nCompany\nseafood\nFish and seafood products (tuna, salmon, herring, shellfish, and groundfish products; and flounder fillets, cornmeal pollock strips, burger, tuna steak, frozen halibut steaks, as well as canned sockeye salmon and frozen sockeye, and crabs)\n\n\nBahía del Sol Kga Consulting\n1\n1\n1\n14016.58\nVesperanda\nCompany\nseafood\nSeafood products\n\n\nBaker LLC\n1\n3\n2\n11016.82\nZH\nCompany, Beneficial Owner, Company Contacts\nseafood, unknown\nCanned fish and seafood products| character(0)| character(0)\n\n\nBaker and Sons\n1\n2\n2\n104095830.23\nZH\nCompany, Beneficial Owner\nseafood, unknown\nFish; fresh or chilled, mackerel (Scomber scombrus, Scomber australasicus, Scomber japonicus), excluding fillets, fish meat of 0304, and edible fish offal of subheadings 0302.91 to 0302.99| character(0)\n\n\nBalkan GmbH & Co. KG Cargo\n1\n1\n1\n29457.13\nNalakond\nCompany\nseafood\nSeafood and related products\n\n\n\n\n\n\n\n3.2.4 seafood_edges df\nNext, we need to filter only the relevant relationship from the edges_combined df according to seafood_entities df .\nFilter the relevant rows from edges_combined df where either of its target or source values are found in id column of seafood_entities dataframe. This would filter out all relationship related to seafood related entities. There are no duplicates in seafood_edges df.\n\n\nShow the code\n# Filter rows in edges_combined df based on matching values of id column in the'seafood_entities' dataframe\nseafood_edges <- edges_combined[edges_combined$source %in% seafood_entities$id | edges_combined$target %in% seafood_entities$id, ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 seafood_nodes df\nUsing seafood_edges df, create a node file called seafood_nodes for network visualisation later.\nA left join with nodes_all_agg is required to ingest all the nodes attributes.\n\n\nShow the code\n# Extract unique ids from 'source' and 'target' columns of 'seafood_edges' dataframe\nunique_entities <- unique(c(seafood_edges$source, seafood_edges$target))\n\n# Create a new 'seafood_nodes' dataframe as a tibble\nseafood_nodes <- as_tibble(data.frame(id = unique_entities))\n\n# Perform left outer join with my nodes master table 'nodes_all_agg' \nseafood_nodes <- left_join(seafood_nodes, nodes_all_agg, by = \"id\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#seafood_graph-tbl-object",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#seafood_graph-tbl-object",
    "title": "Take-home_Ex03",
    "section": "Seafood_graph tbl object",
    "text": "Seafood_graph tbl object\nThe seafood_nodes and seafood_edges dfs will be the main files used to create a tbl graph object. The tbl_graph function from tidygraph library will be used.\nThe seafood_graph object has 3114 nodes and 2505 edges. There are 609 subgraphs inside.\nIn addition, we will calculate various centrality scores and also add a column called membership for us to recognise which subgraph each id belongs to.\n\n\nShow the code\nseafood_graph<- tbl_graph(nodes=seafood_nodes,\n                          edges = seafood_edges,\n                          directed = FALSE)\n\nseafood_graph <- seafood_graph %>% \n  activate(nodes) %>% \n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         eigenvector_centrality = centrality_eigen(),\n         membership = components(seafood_graph)$membership)\n\n\nseafood_graph nodes interactive datatable below to show all the additional centrality and membership attributes calculated using wrapper functions of the tidygraph package. You could sort the various columns inside.\n\n\n\n\n\n\n\n\n\n\nWe will build a network graph that contains only seafood_related business relationships. The graph will show only entities with high betweenness scores.\nPrepare edge file for visNetwork library. Rename source to from and target to to.\n\n\nShow the code\nseafood_edges_vis <- seafood_edges %>% \n  rename(from = source) %>% \n  rename(to = target) %>% \n  mutate(title = type)\n\n\nPrepare node file for visNetwork library. By renaming type_concat to group, visnetwork could help us to colour the nodes by business relationship types.\n\n\nShow the code\nseafood_nodes_vis <- seafood_graph %>% \n  activate(nodes) %>% \n  as_tibble() %>% \n  mutate(group = type_concat) %>% \n  mutate(title = paste('id = ',id, \"\\n ,Betweenness =\", betweenness_centrality,\"\\n ,Revenue =\", revenue_sum, \"\\n ,Country =\", country_concat))\n\n\nLet us visualise the nodes with betweenness score 15 and above.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(seafood_nodes_vis %>% filter(betweenness_centrality >= 15),\n           seafood_edges_vis,\n           main = \"Interactive Network graph of top betweenness entities\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nAll the entities shown above have relatively higher betweenness scores than other entities in the entire network. Hovering the mouse over each node and edge will reveal attributes like betweenness scores, country associated each node with and its revenue.\nAt a glance, there are two subgraphs (bottom) with 5 high betweenness nodes connected to one another, forming a larger than normal network. We are interested in subgraphs with longer network diameter like these because they represent more complex business relationship. For instance, in such complex subgraphs, entities involved are ‘company’, ‘business owners’ and ‘company contacts’ and there are also more than 1 company in the network. From literature review, having entities associated to two or more companies (conflict of interests could occur) could suggests transshipment activities. This could allows us to investigate the entities in the complex subgraphs to check for any IUU crime.\nInstead of having to ‘eye-ball’ each of 609 the seafood network subgraphs to identify those with longer network diameter, we can extract edge data of each subgraph (using membership column) and compile the network diameter of each subgraph into a dataframe."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#calculate-network-diameter",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#calculate-network-diameter",
    "title": "Take-home_Ex03",
    "section": "Calculate network diameter",
    "text": "Calculate network diameter\n\n\nShow the code\n# Get unique membership values\nunique_memberships <- unique(seafood_nodes_vis$membership)\n\n# Initialize empty list to store results\nresults <- list()\n\n# Iterate over each membership value\nfor (x in unique_memberships) {\n  # Filter nodes based on membership\n  nodes <- seafood_nodes_vis %>%\n    filter(membership == x)\n  \n  # Filter edges based on nodes\n  edges <- seafood_edges_vis %>%\n    filter(from %in% nodes$id | to %in% nodes$id)\n  \n  # Create subgraph\n  subgraph <- as_tbl_graph(edges, directed = FALSE)\n  \n  # Calculate network diameter\n  diameter <- with_graph(subgraph, graph_diameter())\n  \n  # Store results in list\n  results[[as.character(x)]] <- diameter\n}\n# Create DataFrame with membership and network diameter columns\ndiameter_df <- tibble(\n  membership = unique_memberships,\n  network_diameter = unlist(results)\n)\n\n\nLet us visualise the distribution of network_diameter of all the seafood subgraphs.\nFirst, let us fix the order of the network diameter field by descending counts of network diameter.\n\n\nShow the code\ndiameter_counts <- count(diameter_df, network_diameter)\n\n# Reorder the factor levels of network_diameter based on count\ndiameter_df$network_diameter1 <- factor(\n  diameter_df$network_diameter,\n  levels = diameter_counts$network_diameter[order(diameter_counts$n, decreasing = FALSE)]\n)\n\n\n\n\n\n\n\nShow the code\nd <- highlight_key(diameter_df %>% arrange(desc(network_diameter1))) \n\n\np<-ggplot(data=diameter_df, \n       aes(x=as.factor(network_diameter1))) +\n  geom_bar(fill='lightblue') +\n  coord_flip() +\n  theme_minimal() + \n    labs(title = 'Distribution of Network Diameter',\n         x= 'Network diameter') +\n  theme(plot.title = element_text(face='bold'))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,\n                  DT::datatable(d,options = list(iDisplayLength = 5)),\n                  widths=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are about 32 subgraphs with network diameter of 4 and above. We can investigate these subgraphs closer."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#subgraphs-with-high-network-risk",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#subgraphs-with-high-network-risk",
    "title": "Take-home_Ex03",
    "section": "4.1 Subgraphs with high NETWORK RISK",
    "text": "4.1 Subgraphs with high NETWORK RISK\nFirst, let us ingest the network diameter of each subgraph into the main seafood_nodes_vis dataframe by performing a left join with diameter_df using membership columns in both df as join key.\n\n\nShow the code\nseafood_nodes_vis<- seafood_nodes_vis %>% \n  left_join(diameter_df, by='membership',\n            unmatched = \"drop\") %>% \n  arrange(desc(network_diameter), desc(betweenness_centrality)) %>% \n  select(id,membership,network_diameter,betweenness_centrality,closeness_centrality,eigenvector_centrality,revenue_sum,country_qty,type_qty,label_qty,country_concat,type_concat,label_concat,product_services_concat,group,title)\n\n\nIs there a difference in the betweenness scores across network diameter?\n\n\nShow the code\nggbetweenstats(data = seafood_nodes_vis, x = network_diameter, y = betweenness_centrality,\n               xlab = \"network_diameter\", ylab = \"betweenness_centrality\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               sort = \"descending\",\n               sort.fun = median,\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Betweenness centrality across different network diameters\") +\n  scale_y_continuous(limits = c(0, 2500)) +\n   theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9))\n\n\n\n\n\nFrom the plot above, p value< 0.05 and we have evidence to conclude that the betweenness scores across network of different diameter are different. There are many entities with very high betweenness scores in subgraphs with network_diameter of 2, 4 and 6. However, in this section we will only focus on subgraphs with network_diameter of 4 and above because of more complex business relationships within them.\nNext filter the subgraphs where network diameter is 4 and above.\n\n\nShow the code\nnetwork_risk_nodes_vis<- seafood_nodes_vis %>% \n  filter(network_diameter >= 4) %>% \n  arrange(desc(network_diameter), desc(betweenness_centrality))\n\n\nThere are only 2 subgraphs with network diameter of 6. They are subgraphs 112 and 227.\n\n\nShow the code\n#network_risk_nodes_vis %>% select(id,membership, network_diameter) %>%  head(10) %>% kable()\n\nDT::datatable(network_risk_nodes_vis ,options = list(iDisplayLength = 5))\n\n\n\n\n\n\n\nVisualisation of the two subgraphs with highest network diameter.\nFilter for all the nodes and edges in subgraph 112 from seafood_nodes_vis dataframe.\n\n\nShow the code\nm<- 112\nsub112_nodes_vis <- seafood_nodes_vis %>%  filter(membership==m)\nsub112_edges_vis <- seafood_edges_vis[seafood_edges_vis$from %in% sub112_nodes_vis$id | seafood_edges_vis$to %in% sub112_nodes_vis$id, ]\n\n\nVisualise the network graph 112.\n\n\nShow the code\nset.seed(1234)\nvisNetwork(sub112_nodes_vis, #%>% filter(betweenness_centrality >= 20),\n           sub112_edges_vis,\n           main = \"Network graph of subgraph 112 with diameter =6\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nIn this subgraph, we can see three companies (blue) linked together by two individuals, namely Andrew Reed and John Hernandez.\nAndrew Reed is a beneficial owner to both ‘Adair S.A. de C.V.’ and ‘Oka  Ltd. Corporation Transport’.\nJohn Hernandez is a Company contacts of ‘Danish Plaice Swordfish AB Shipping’ and beneficial owner of ‘Adair S.A. de C.V.’\nThe datatable containing details of the members in subgraph 112 allows us to filter them by betweenness scores and other attributes.\n\n\nShow the code\nDT::datatable(seafood_nodes_vis %>%  filter(membership==m), options = list(iDisplayLength = 5))\n\n\n\n\n\n\n\nFilter for all the nodes and edges in subgraph 227 from seafood_nodes_vis dataframe\n\n\nShow the code\nm<- 227\nsub227_nodes_vis <- seafood_nodes_vis %>%  filter(membership==m)\nsub227_edges_vis <- seafood_edges_vis[seafood_edges_vis$from %in% sub227_nodes_vis$id | seafood_edges_vis$to %in% sub227_nodes_vis$id, ]\n\n\nVisualising the subgraph 227\n\n\nShow the code\nset.seed(1234)\nvisNetwork(sub227_nodes_vis, #%>% filter(betweenness_centrality >= 20),\n           sub227_edges_vis,\n           main = \"Network graph of subgraph 227 with diameter =6\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nIn this subgraph, we can see three companies linked by two individuals, namely Christopher Rodrigues and Lisa Brown.\nLisa Brown is beneficial owner to both ‘Tide NV solutions’ and ‘Deep Blue Cargo ship’.\nChristopher Rodriguez is a beneficial owner of ‘Deep Blue Cargo ship’ and a company contacts of ‘Lewis PLC’. It seems to suggest that Deep Blue Cargo ship’ could have business dealings with ‘Lewis PLC’ via Christopher.\nThe datatable containing details of the members in subgraph 227 allows us to filter them by betweenness scores and other attributes.\n\n\n\n\n\n\n\n\nShow the code\nDT::datatable(seafood_nodes_vis %>%  filter(membership==m) , options = list(iDisplayLength = 5))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#high-financial-risk",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#high-financial-risk",
    "title": "Take-home_Ex03",
    "section": "4.2 High FINANCIAL RISK",
    "text": "4.2 High FINANCIAL RISK\nIn this section, we will look out for subgraphs with exceptionally high total revenue. As we expect bigger subgraphs to have higher revenue, thus the total revenue will be divided by the number of companies in the subgraph for fair comparison.\nThe code chunk below first group by membership and network_diameter columns to calculate sum of revenue called total_revenue_subgraph. It then sum up the number of times ‘Company’ appeared under the type_concat column to give us the number of distinct companies inside the subgraph. Finally, the total revenue for each subgraph is divided by the number of companies to give revenue_per_company field.\n\n\nShow the code\nfinancial_risk_nodes <- seafood_nodes_vis %>%\n  group_by(membership, network_diameter) %>%\n  summarize(\n    total_revenue_subgraph = as.numeric(sum(revenue_sum, na.rm=TRUE)),\n    no_of_companies_subgraph = sum(str_count(type_concat, \"\\\\bCompany\\\\b\")) - sum(str_count(type_concat, \"\\\\bCompany Contacts\\\\b\"))) %>% \n  ungroup() %>%\nmutate(\n    revenue_per_company = ifelse(no_of_companies_subgraph == 0, total_revenue_subgraph, round(total_revenue_subgraph / no_of_companies_subgraph))) %>% \n  arrange(desc(revenue_per_company))\n\n\nThe summary function shows signs of extreme outliers revenue_per_company value of $291,436,839. We might have to consider using log scale for box plots.\n\n\nShow the code\nsummary(financial_risk_nodes$revenue_per_company)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n        0     12822     32167   1265696     72367 291436839 \n\n\nVisualising the distribution of revenue per company across different network diameters.\n\n\nShow the code\nlibrary(scales)\n\nggplotly(ggplot(data = financial_risk_nodes %>% \n                  mutate(network_diameter=as.factor(network_diameter)), \n                aes(x = reorder(network_diameter, \n                                -revenue_per_company, \n                                median), \n                    y = revenue_per_company, \n                    fill=network_diameter)) +\n           \n   geom_boxplot(outlier.colour=\"blue\", \n                outlier.size=1) +\n     \n   geom_point(aes(label=membership),\n              position = 'jitter',\n              size=0.5) +\n     \n   stat_summary(fun.y=mean, \n                geom=\"point\", \n                shape=20, \n                size=2.5, \n                color=\"pink\", \n                fill=\"red\") +\n   xlab(\"Network diameter size\") +\n   ylab(\"Revenue per company\") +\n   ggtitle(\"Revenue per company by network diameter size\") +\n   #scale_fill_brewer(palette='Set2') +\n   theme(plot.title = element_text(face= 'bold',\n                                   hjust = 0.5),\n         legend.position = 'none') +\n     \n   scale_y_continuous(trans = log10_trans()))\n\n\n\n\n\n\nShow the code\n    #breaks = trans_breaks(\"log10\", function(x) 10^x),\n    #labels = trans_format(\"log10\", math_format(10^.x))))\n\n\nHovering the mouse above the points will reveal their subgraph membership numbers. Surprisely, there are more smaller network diameter groups with higher revenue per company.\nThe top 10 subgraphs with the highest total revenue per company are listed below. The network diameters are mainly 1 or 2.\n\n\n\n\n\n\n\n\n\n\n\nrevenue_per_company\nmembership\nnetwork_diameter\nno_of_companies_subgraph\n\n\n\n\n291436839\n396\n1\n1\n\n\n131450837\n577\n2\n0\n\n\n95809780\n316\n2\n1\n\n\n63153107\n173\n2\n1\n\n\n55376193\n193\n1\n0\n\n\n52053645\n162\n4\n2\n\n\n32183079\n581\n2\n1\n\n\n1507514\n498\n2\n1\n\n\n1216029\n553\n2\n1\n\n\n1205868\n48\n1\n1\n\n\n\n\n\nVisualising the top 2 subgraphs in terms of revenue per company.\nSubgraph 193\n\n\nShow the code\nm<- 396\nsub227_nodes_vis <- seafood_nodes_vis %>%  filter(membership==m)\nsub227_edges_vis <- seafood_edges_vis[seafood_edges_vis$from %in% sub227_nodes_vis$id | seafood_edges_vis$to %in% sub227_nodes_vis$id, ]\n\nset.seed(1234)\nvisNetwork(sub227_nodes_vis, #%>% filter(betweenness_centrality >= 20),\n           sub227_edges_vis,\n           main = \"Network graph of subgraph 396 with diameter = 1\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nIn this subgraph, Morgan Group commands a very high revenue of 291 million with only one business relationship with Jason Cole who is its Beneficial owner. Morgan Group sells fish (smoked or not smoked) products.\nInformation about the two members:\n\n\nShow the code\nDT::datatable(seafood_nodes_vis %>%  filter(membership==m))\n\n\n\n\n\n\n\nSubgraph 498\n\n\nShow the code\nm<- 577\nsub227_nodes_vis <- seafood_nodes_vis %>%  filter(membership==m)\nsub227_edges_vis <- seafood_edges_vis[seafood_edges_vis$from %in% sub227_nodes_vis$id | seafood_edges_vis$to %in% sub227_nodes_vis$id, ]\n\nset.seed(1234)\nvisNetwork(sub227_nodes_vis, #%>% filter(betweenness_centrality >= 20),\n           sub227_edges_vis,\n           main = \"Network graph of subgraph 577 with diameter = 2\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nIn this subgraph, the total revenue of the three individuals is 131 million recorded under the name of ‘WIlson LLC’. The product service offered by this company is ‘Fish, dried but not smoked’. ‘Wilson LLC’ seems like a company than company contacts or beneficial owner.\nInformation about the members:\n\n\nShow the code\nDT::datatable(seafood_nodes_vis %>%  filter(membership==m),options = list(iDisplayLength = 5))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#high-country-risk",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#high-country-risk",
    "title": "Take-home_Ex03",
    "section": "4.3 High COUNTRY RISK",
    "text": "4.3 High COUNTRY RISK\nIn this section, we will focus on subgraphs with business relationship involving many countries. This could usually indicate transshipment across carriers from different countries.\nAccording to literature reviews, a method of IUU is via “Flags of convenience” where vessels fly different country flags at different location to avoid inspections and intersections by local governing bodies.\nThe code chunk below is a continuation from section 4.3. It create a new column no_of_countries_subgraph where it computes the number of distinct values of countries present in each subgraph.\n\n\nShow the code\ncountry_risk_nodes <- seafood_nodes_vis %>%\n  group_by(membership, network_diameter) %>%\n  summarize(\n    total_revenue_subgraph = as.numeric(sum(revenue_sum, na.rm=TRUE)),\n    no_of_companies_subgraph = sum(str_count(type_concat, \"\\\\bCompany\\\\b\")) - sum(str_count(type_concat, \"\\\\bCompany Contacts\\\\b\")),\n  no_of_countries_subgraph = n_distinct(na.omit(unlist(strsplit(country_concat, \", \"))))) %>%\n  ungroup() %>%\nmutate(\n    revenue_per_company = ifelse(no_of_companies_subgraph == 0, total_revenue_subgraph, round(total_revenue_subgraph / no_of_companies_subgraph))) %>% \n  arrange(desc(revenue_per_company))\n\n\nWe will use a bar chart to visualise the count of countries in the business relationship.\n\n\nShow the code\nd <- highlight_key(country_risk_nodes %>% arrange(desc(no_of_countries_subgraph))) \n\n\np<-ggplot(data=country_risk_nodes, \n       aes(x=as.factor(no_of_countries_subgraph))) +\n  geom_bar(fill = 'lightblue') +\n  coord_flip() +\n  theme_minimal() + \n    labs(title= 'Distribution of \\nnumber of countries \\nin subgraphs',\n         x= 'Number of countries involved in a subgraph')\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,\n                  DT::datatable(d,options = list(iDisplayLength = 5)),\n                  widths=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgraph 6 is associated with the highest number of countries (10).\nSubgraph 85 is associated with 4 different countries.\nThere are 15 other subgraphs that are associated with 3 different countries.\nSubgraph 6 is very special because of the relationship between more than 10 countries so let us zoom in on its network graph.\n\n\nShow the code\nm<- 6\nsub227_nodes_vis <- seafood_nodes_vis %>%  filter(membership==m)\nsub227_edges_vis <- seafood_edges_vis[seafood_edges_vis$from %in% sub227_nodes_vis$id | seafood_edges_vis$to %in% sub227_nodes_vis$id, ]\n\nset.seed(1234)\nvisNetwork(sub227_nodes_vis, #%>% filter(betweenness_centrality >= 20),\n           sub227_edges_vis,\n           main = \"Network graph of subgraph 6 with diameter = 4\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enables = TRUE,\n                         type= 'straightCross'),\n           shadow = FALSE,\n           dash = FALSE) %>% \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>% \n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nIn the above subgraph, there are two companies ‘Aqua Aura SE Marine life’ registered in countries (Mawazam, Rio Isla, Icarnia, Oceanus, Nalakond, Coralmarica, Alverossia, Isliandor, Talandria) and ’ BlueWater Bistro GmbH Industrial’ company registered in (Marebak).\nInformation about its members:\n\n\nShow the code\nDT::datatable(seafood_nodes_vis %>%  filter(membership==m) , options = list(iDisplayLength = 5))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#finding-similar-subgraphs-groups",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#finding-similar-subgraphs-groups",
    "title": "Take-home_Ex03",
    "section": "4.4 Finding similar subgraphs (groups)",
    "text": "4.4 Finding similar subgraphs (groups)\nIn this section, we will use parallel coordinate plot to visualise and analyse multivariate, numerical data we have of each subgraphs. We will be comparing multiple variables\n\nno_of_companies_subgraph : the number of distinct companies in the subgraph\nno_of_beneficial_owners_subgraph: the number of beneficial owners in the subgraph\ntop_betweenness : the highest betweenness score of an entity in the subgraph\nrevenue_per_company: the total revenue of the subgraph divided by the total number of companies in subgraph\n\nPreparing the dataframe for the plot by creating two more new columns to calculate the number of beneficial owners and top betweenness score in each subgraph.\n\n\nShow the code\nprll <- seafood_nodes_vis %>%\n  group_by(membership, network_diameter) %>%\n  summarize(\n    total_revenue_subgraph = as.numeric(sum(revenue_sum, na.rm=TRUE)),\n    no_of_companies_subgraph = sum(str_count(type_concat, \"\\\\bCompany\\\\b\")) - sum(str_count(type_concat, \"\\\\bCompany Contacts\\\\b\")),\n  no_of_countries_subgraph = n_distinct(na.omit(unlist(strsplit(country_concat, \", \")))),\n  no_of_beneficial_owners_subgraph = sum(str_count(type_concat, \"Beneficial Owner|Beneficial Owner, Company Contacts\")),\n  top_betweenness = max(betweenness_centrality)) %>%\n  ungroup() %>%\nmutate(\n    revenue_per_company = ifelse(no_of_companies_subgraph == 0, total_revenue_subgraph, round(total_revenue_subgraph / no_of_companies_subgraph)),\n    membership = as.factor(membership)) %>% \n  arrange(desc(revenue_per_company))\n\n\nWe will filter subgraphs with network diameter 3 and above. The graph will also be faceted by the number of countries involvment in each subgraph. (Refer to the subtitle for the meaning of each box)\n\n\nShow the code\nset.seed(1234)\nggparcoord(prll %>% \n             filter(as.numeric(network_diameter) >=3) %>%\n             mutate(no_of_countries_subgraph=as.factor(no_of_countries_subgraph),\n                    network_diameter=as.factor(network_diameter)),\n           columns = c(4,6:8),\n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.3,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of subgraph variables\") +\n  theme(plot.title = element_text(face='bold'),\n        legend.position = \"right\",\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 10),\n        axis.text.x = element_text(angle = 35, hjust = 1),\n        axis.title.y = element_blank()) +\n  labs(subtitle='Box 1: 1 country, Box 2: 2 countries, Box 3: 3 countries, Box 4: 10 countries') +\n  \n  facet_wrap(~ `no_of_countries_subgraph`)\n\n\n\n\n\nBox 1 shows that there is one subgraph with network diameter =3 that has only 1 country in the relationship.\nIn box 2, the subgraphs with higher network diameter tend to have more beneficial owners in the relationship.\nIn box 3, there are subgraphs with more number of companies in the relationship. The number of beneficial owner, top betweenness score and revenue per company is also slightly higher than in box 2.\nBox 4 shows that there is one subgraph with network diameter =4 that has 10 countries in the relationship. The number of beneficial owners and top betweenness scores in te subgraph are also observed to be the highest amongst all the boxes."
  }
]